{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOFqE5r5Tn1F",
        "outputId": "197acea9-11b1-43a4-b43f-caac7bf819d4"
      },
      "outputs": [],
      "source": [
        "# ================= CELL 1: INSTALLATION =================\n",
        "!pip install torch torchvision torchaudio --upgrade\n",
        "!pip install transformers datasets scikit-learn\n",
        "!pip uninstall dgl -y -q\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/repo.html\n",
        "!pip install torchdata==0.7.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSKS4zwKTqpS",
        "outputId": "9fa89425-fbd1-4107-9018-000e789f1f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGL version: 2.1.0\n",
            "PyTorch version: 2.9.1+cu128\n",
            "✅ Hypergraph creation successful!\n"
          ]
        }
      ],
      "source": [
        " # ================= CELL 2: WORKING IMPORTS =================\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set DGL backend before importing\n",
        "os.environ['DGLBACKEND'] = 'pytorch'\n",
        "\n",
        "# Mock GraphBolt to prevent loading issues\n",
        "class MockModule:\n",
        "    def __getattr__(self, name):\n",
        "        return lambda *args, **kwargs: None\n",
        "\n",
        "sys.modules['dgl.graphbolt'] = MockModule()\n",
        "\n",
        "# Import DGL\n",
        "import dgl\n",
        "\n",
        "# Import other libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, accuracy_score, average_precision_score, precision_recall_curve, auc\n",
        "import dgl.function as fn\n",
        "\n",
        "# Test everything\n",
        "print(f\"DGL version: {dgl.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Test creating a hypergraph\n",
        "try:\n",
        "    data_dict = {\n",
        "        ('node', 'in', 'edge'): ([0, 1], [0, 0]),\n",
        "        ('edge', 'con', 'node'): ([0, 0], [0, 1])\n",
        "    }\n",
        "    test_hyG = dgl.heterograph(data_dict)\n",
        "    print(\"✅ Hypergraph creation successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwkN3wk7Tsi5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc\n",
        "import dgl.function as fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul3GWWDJTwiy",
        "outputId": "0e2c641d-2250-4821-947b-71da01a2e66f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')# Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtco-SynTyAu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ======================= EXACT DECODERS FROM ORIGINAL CODE =======================\n",
        "\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h # here h is drug features and g is the pos/neg train/test graph\n",
        "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
        "            return g.edata['score'][:, 0]\n",
        "\n",
        "#Only we use that\n",
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
        "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7cLIsQITyuI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ======================= EXACT LOSS/EVAL FROM ORIGINAL CODE =======================\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "def compute_auc(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
        "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
        "    auc_precision_recall = auc(recall, precision)\n",
        "    return roc_auc_score(labels, scores),auc(recall, precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI4BXriiT1f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ======================= EXACT HYGNN FROM ORIGINAL CODE =======================\n",
        "\n",
        "class HyGNN(nn.Module):\n",
        "    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, dropout):\n",
        "        super(HyGNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.query_dim = query_dim\n",
        "        self.in_first_layer = torch.nn.Linear(input_dim, vertex_dim)\n",
        "        self.not_in_first_layer = torch.nn.Linear(vertex_dim, vertex_dim)\n",
        "        self.w6 = torch.nn.Linear(edge_dim, query_dim)\n",
        "        self.w5 = torch.nn.Linear(vertex_dim, query_dim)\n",
        "        self.w4 = torch.nn.Linear(vertex_dim, edge_dim)\n",
        "        self.w3 = torch.nn.Linear(vertex_dim, query_dim)\n",
        "        self.w2 = torch.nn.Linear(edge_dim, query_dim)\n",
        "        self.w1 = torch.nn.Linear(edge_dim, vertex_dim)\n",
        "\n",
        "    def red_function(self, nodes):\n",
        "        attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n",
        "        aggregated = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n",
        "        return {'h': aggregated}\n",
        "\n",
        "    def attention(self, edges):\n",
        "        attn_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n",
        "        return {'Attn': attn_score/np.sqrt(self.query_dim)}\n",
        "\n",
        "    def msg_fucntion(self, edges):\n",
        "        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n",
        "\n",
        "    def forward(self, hyG, vfeat, efeat, first_layer, last_layer):\n",
        "            if first_layer:\n",
        "                feat_e = self.in_first_layer(efeat)\n",
        "            else:\n",
        "                feat_e = self.not_in_first_layer(efeat)\n",
        "            feat_v = vfeat\n",
        "            #Hyperedge-level attention\n",
        "            hyG.ndata['h'] = {'edge': feat_e}\n",
        "            hyG.ndata['k'] = {'edge' : self.w2(feat_e)}\n",
        "            hyG.ndata['v'] = {'edge' : self.w1(feat_e)}\n",
        "            hyG.ndata['q'] = {'node' : self.w3(feat_v)}\n",
        "            hyG.apply_edges(self.attention, etype='con')\n",
        "            hyG.update_all(self.msg_fucntion, self.red_function, etype='con')\n",
        "\n",
        "            #Node-level attention\n",
        "            feat_v = hyG.ndata['h']['node']\n",
        "            hyG.ndata['k'] = {'node' : self.w5(feat_v)}\n",
        "            hyG.ndata['v'] = {'node' : self.w4(feat_v)}\n",
        "            hyG.ndata['q'] = {'edge' : self.w6(feat_e)}\n",
        "            hyG.apply_edges(self.attention, etype='in')\n",
        "            hyG.update_all(self.msg_fucntion, self.red_function, etype='in')\n",
        "            feat_e = hyG.ndata['h']['edge']\n",
        "\n",
        "            if not last_layer :\n",
        "                feat_v = F.dropout(feat_v, self.dropout)\n",
        "            if last_layer:\n",
        "                return feat_v, feat_e\n",
        "            else:\n",
        "                return [hyG, feat_v, feat_e]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSvQsRJuT2Ur"
      },
      "outputs": [],
      "source": [
        "def load_data_and_create_graphs():\n",
        "    \"\"\"Load your actual data files and create the necessary graphs\"\"\"\n",
        "\n",
        "    # Load metadata to get dimensions\n",
        "    metadata = torch.load('/content/drive/MyDrive/MyModel/hypergraphs/hyG_drug_drugbank_kmer_9_metadata.pt', weights_only=False)\n",
        "\n",
        "    # Extract dimensions from metadata\n",
        "    num_drugs = len(metadata['drug_to_idx'])\n",
        "    num_substructures = len(metadata['node_to_idx'])\n",
        "\n",
        "    print(f\"Number of drugs: {num_drugs}\")\n",
        "    print(f\"Number of substructures: {num_substructures}\")\n",
        "\n",
        "    # Load hypergraph data\n",
        "    chemicalsub_drug = torch.load('/content/drive/MyDrive/MyModel/hypergraphs/hyG_drug_drugbank_kmer_9.pt', weights_only=False)\n",
        "\n",
        "    # Create hypergraph\n",
        "    data_dict = {\n",
        "        ('node', 'in', 'edge'): (chemicalsub_drug[:,0], chemicalsub_drug[:,1]),\n",
        "        ('edge', 'con', 'node'): (chemicalsub_drug[:,1], chemicalsub_drug[:,0])\n",
        "    }\n",
        "\n",
        "    hyG = dgl.heterograph(data_dict)\n",
        "    print(\"Hypergraph structure:\")\n",
        "    print(hyG)\n",
        "    print(\"=\" * 500)\n",
        "\n",
        "    # Create drug identity matrix (sparse)\n",
        "    from scipy.sparse import coo_matrix\n",
        "    nl = coo_matrix((num_drugs, num_drugs))\n",
        "    nl.setdiag(1)\n",
        "    values = nl.data\n",
        "    indices = np.vstack((nl.row, nl.col))\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = nl.shape\n",
        "    drug_X = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
        "\n",
        "    # Create node features\n",
        "    hyG.ndata['h'] = {'edge': torch.tensor(drug_X).type('torch.FloatTensor'), 'node': torch.ones(num_substructures, 128)}\n",
        "    e_feat = torch.tensor(drug_X).type('torch.FloatTensor')\n",
        "    v_feat = torch.ones(num_substructures, 128)\n",
        "\n",
        "    return hyG, v_feat, e_feat, drug_X, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiGrSB4BT5Mu"
      },
      "outputs": [],
      "source": [
        "def load_train_test_data():\n",
        "    \"\"\"Load your CSV files and create DGL graphs for training\"\"\"\n",
        "\n",
        "    # Load positive samples\n",
        "    train_pos = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/train.csv')\n",
        "    val_pos = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/val.csv')\n",
        "    test_pos = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/test.csv')\n",
        "\n",
        "    # Load negative samples\n",
        "    train_neg = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/processed_with_negatives/train_negatives.csv')\n",
        "    val_neg = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/processed_with_negatives/val_negatives.csv')\n",
        "    test_neg = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/processed_with_negatives/test_negatives.csv')\n",
        "\n",
        "    # Get metadata and drug mapping\n",
        "    metadata = torch.load('/content/drive/MyDrive/MyModel/hypergraphs/hyG_drug_drugbank_kmer_9_metadata.pt', weights_only=False)\n",
        "\n",
        "\n",
        "    num_drugs = len(metadata['drug_to_idx'])\n",
        "    drug_to_id_mapping = metadata['drug_to_idx']\n",
        "\n",
        "    def create_dgl_graph(df, num_nodes, drug_mapping):\n",
        "        \"\"\"Create DGL graph from dataframe with drug pairs\"\"\"\n",
        "        if 'Drug1_ID' in df.columns and 'Drug2_ID' in df.columns:\n",
        "            src_ids = df['Drug1_ID'].values\n",
        "            dst_ids = df['Drug2_ID'].values\n",
        "        else:\n",
        "            print(\"Available columns:\", df.columns.tolist())\n",
        "            src_ids = df.iloc[:, 0].values  # First column\n",
        "            dst_ids = df.iloc[:, 1].values  # Second column\n",
        "\n",
        "        # Convert DrugBank IDs to integer indices using the mapping\n",
        "        src = torch.tensor([drug_mapping[drug_id] for drug_id in src_ids], dtype=torch.long)\n",
        "        dst = torch.tensor([drug_mapping[drug_id] for drug_id in dst_ids], dtype=torch.long)\n",
        "\n",
        "        return dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "\n",
        "    # Create DGL graphs\n",
        "    train_pos_g = create_dgl_graph(train_pos, num_drugs, drug_to_id_mapping)\n",
        "    val_pos_g = create_dgl_graph(val_pos, num_drugs, drug_to_id_mapping)\n",
        "    test_pos_g = create_dgl_graph(test_pos, num_drugs, drug_to_id_mapping)\n",
        "\n",
        "    train_neg_g = create_dgl_graph(train_neg, num_drugs, drug_to_id_mapping)\n",
        "    val_neg_g = create_dgl_graph(val_neg, num_drugs, drug_to_id_mapping)\n",
        "    test_neg_g = create_dgl_graph(test_neg, num_drugs, drug_to_id_mapping)\n",
        "\n",
        "    print(f\"Train positive edges: {train_pos_g.number_of_edges()}\")\n",
        "    print(f\"Train negative edges: {train_neg_g.number_of_edges()}\")\n",
        "    print(f\"Validation positive edges: {val_pos_g.number_of_edges()}\")\n",
        "    print(f\"Validation negative edges: {val_neg_g.number_of_edges()}\")\n",
        "    print(f\"Test positive edges: {test_pos_g.number_of_edges()}\")\n",
        "    print(f\"Test negative edges: {test_neg_g.number_of_edges()}\")\n",
        "\n",
        "    return train_pos_g, train_neg_g, val_pos_g, val_neg_g, test_pos_g, test_neg_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYMB8IsaT61w"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, drug_feature_dim, config):\n",
        "        super(Model, self).__init__()\n",
        "        self.gat1 = HyGNN(\n",
        "            drug_feature_dim,\n",
        "            config['hidden_units'],  # query_dim\n",
        "            config['hidden_units'],     # vertex_dim\n",
        "            config['hidden_units'],     # edge_dim\n",
        "            config['dropout']           # dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, hyG, v_feat, e_feat, f, l):\n",
        "        h = self.gat1(hyG, v_feat, e_feat, f, l)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L4VTpCEjPuT"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import os\n",
        "import time\n",
        "def calculate_ram_usage():\n",
        "    \"\"\"Calculate current RAM usage in GB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    ram_gb = process.memory_info().rss / (1024 ** 3)  # Convert to GB\n",
        "    return ram_gb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCD4lBM0T8jf",
        "outputId": "c34095f1-2490-42a4-b724-df75f58a38e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: aggressive_01\n",
            "Config: {'learning_rate': 0.005, 'hidden_units': 128, 'dropout': 0.1, 'weight_decay': 0.0, 'training_seed': 42, 'experiment_name': 'aggressive_01'}\n",
            "Directory: /content/drive/MyDrive/MyModel/Stage1_Chemical_NetworkModel1_Seed42-k9/aggressive_01/\n",
            "Number of drugs: 1709\n",
            "Number of substructures: 29462\n",
            "Hypergraph structure:\n",
            "Graph(num_nodes={'edge': 1709, 'node': 29462},\n",
            "      num_edges={('edge', 'con', 'node'): 96656, ('node', 'in', 'edge'): 96656},\n",
            "      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])\n",
            "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "Train positive edges: 153501\n",
            "Train negative edges: 153501\n",
            "Validation positive edges: 19187\n",
            "Validation negative edges: 19187\n",
            "Test positive edges: 19189\n",
            "Test negative edges: 19189\n",
            "RAM usage before training: 1.42 GB\n",
            "\n",
            "Starting Stage 2 (Metabolic Network) training...\n",
            "Start time: 2025-11-17 11:24:28\n",
            "Epoch 0, train loss: 0.6939, val loss: 0.6948 (best: 0.6948, patience: 0)\n",
            "Epoch 10, train loss: 0.4934, val loss: 0.5148 (best: 0.5148, patience: 0)\n",
            "Epoch 20, train loss: 0.3772, val loss: 0.4075 (best: 0.4075, patience: 0)\n",
            "Epoch 30, train loss: 0.3281, val loss: 0.3752 (best: 0.3749, patience: 1)\n",
            "Epoch 40, train loss: 0.3101, val loss: 0.3549 (best: 0.3549, patience: 0)\n",
            "Epoch 50, train loss: 0.2999, val loss: 0.3478 (best: 0.3474, patience: 2)\n",
            "Epoch 60, train loss: 0.2930, val loss: 0.3393 (best: 0.3393, patience: 0)\n",
            "Epoch 70, train loss: 0.2870, val loss: 0.3337 (best: 0.3337, patience: 0)\n",
            "Epoch 80, train loss: 0.2778, val loss: 0.3252 (best: 0.3252, patience: 0)\n",
            "Epoch 90, train loss: 0.2659, val loss: 0.3112 (best: 0.3112, patience: 0)\n",
            "Epoch 100, train loss: 0.2570, val loss: 0.3027 (best: 0.3021, patience: 1)\n",
            "Epoch 110, train loss: 0.2521, val loss: 0.2971 (best: 0.2956, patience: 1)\n",
            "Epoch 120, train loss: 0.2403, val loss: 0.2875 (best: 0.2864, patience: 1)\n",
            "Epoch 130, train loss: 0.2294, val loss: 0.2776 (best: 0.2776, patience: 0)\n",
            "Epoch 140, train loss: 0.2200, val loss: 0.2699 (best: 0.2697, patience: 1)\n",
            "Epoch 150, train loss: 0.2199, val loss: 0.2677 (best: 0.2644, patience: 1)\n",
            "Epoch 160, train loss: 0.2078, val loss: 0.2543 (best: 0.2543, patience: 0)\n",
            "Epoch 170, train loss: 0.1949, val loss: 0.2399 (best: 0.2399, patience: 0)\n",
            "Epoch 180, train loss: 0.1942, val loss: 0.2325 (best: 0.2325, patience: 0)\n",
            "Epoch 190, train loss: 0.1778, val loss: 0.2253 (best: 0.2214, patience: 1)\n",
            "Epoch 200, train loss: 0.1678, val loss: 0.2136 (best: 0.2136, patience: 0)\n",
            "Epoch 210, train loss: 0.1606, val loss: 0.2077 (best: 0.2077, patience: 0)\n",
            "Epoch 220, train loss: 0.1554, val loss: 0.2043 (best: 0.2043, patience: 0)\n",
            "Epoch 230, train loss: 0.1561, val loss: 0.2074 (best: 0.2015, patience: 1)\n",
            "Epoch 240, train loss: 0.1534, val loss: 0.2027 (best: 0.2015, patience: 11)\n",
            "Epoch 250, train loss: 0.1533, val loss: 0.2042 (best: 0.1993, patience: 7)\n",
            "Epoch 260, train loss: 0.1501, val loss: 0.1961 (best: 0.1958, patience: 4)\n",
            "Epoch 270, train loss: 0.1459, val loss: 0.1972 (best: 0.1958, patience: 14)\n",
            "Epoch 280, train loss: 0.1434, val loss: 0.1952 (best: 0.1952, patience: 0)\n",
            "Epoch 290, train loss: 0.1413, val loss: 0.1938 (best: 0.1938, patience: 0)\n",
            "Epoch 300, train loss: 0.1533, val loss: 0.1930 (best: 0.1927, patience: 4)\n",
            "Epoch 310, train loss: 0.1382, val loss: 0.1890 (best: 0.1890, patience: 0)\n",
            "Epoch 320, train loss: 0.1334, val loss: 0.1855 (best: 0.1840, patience: 1)\n",
            "Epoch 330, train loss: 0.1290, val loss: 0.1816 (best: 0.1804, patience: 1)\n",
            "Epoch 340, train loss: 0.1282, val loss: 0.1779 (best: 0.1779, patience: 0)\n",
            "Epoch 350, train loss: 0.1229, val loss: 0.1769 (best: 0.1764, patience: 3)\n",
            "Epoch 360, train loss: 0.1206, val loss: 0.1747 (best: 0.1747, patience: 0)\n",
            "Epoch 370, train loss: 0.1206, val loss: 0.1735 (best: 0.1735, patience: 0)\n",
            "Epoch 380, train loss: 0.1171, val loss: 0.1740 (best: 0.1728, patience: 3)\n",
            "Epoch 390, train loss: 0.1151, val loss: 0.1726 (best: 0.1725, patience: 2)\n",
            "Epoch 400, train loss: 0.1183, val loss: 0.1722 (best: 0.1716, patience: 4)\n",
            "Epoch 410, train loss: 0.1140, val loss: 0.1730 (best: 0.1711, patience: 3)\n",
            "Epoch 420, train loss: 0.1110, val loss: 0.1723 (best: 0.1711, patience: 13)\n",
            "Epoch 430, train loss: 0.1118, val loss: 0.1757 (best: 0.1711, patience: 23)\n",
            "Epoch 440, train loss: 0.1150, val loss: 0.1717 (best: 0.1711, patience: 33)\n",
            "Epoch 450, train loss: 0.1084, val loss: 0.1712 (best: 0.1708, patience: 2)\n",
            "Epoch 460, train loss: 0.1071, val loss: 0.1711 (best: 0.1708, patience: 12)\n",
            "Epoch 470, train loss: 0.1058, val loss: 0.1712 (best: 0.1708, patience: 22)\n",
            "Epoch 480, train loss: 0.1100, val loss: 0.1802 (best: 0.1708, patience: 32)\n",
            "Epoch 490, train loss: 0.1050, val loss: 0.1724 (best: 0.1706, patience: 1)\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED\n",
            "================================================================================\n",
            "Best epoch: 494\n",
            "Best validation loss: 0.1706\n",
            "\n",
            "Timing Statistics:\n",
            "  Total training time: 8519.82 seconds\n",
            "  Total training time: 142.00 minutes\n",
            "  Total training time: 2.37 hours\n",
            "  Average time per epoch: 17.04 seconds\n",
            "RAM usage before training: 1.42 GB\n",
            "RAM usage after training:  1.88 GB\n",
            "RAM used during training:  0.46 GB\n",
            "  Epochs completed: 500\n",
            "  End time: 2025-11-17 13:46:28\n"
          ]
        }
      ],
      "source": [
        "EXPERIMENT_CONFIG = {\n",
        "    'learning_rate': 0.005,\n",
        "    'hidden_units': 128,\n",
        "    'dropout': 0.1,\n",
        "    'weight_decay': 0.0,\n",
        "    'training_seed': 42,\n",
        "    'experiment_name': '---named----'\n",
        "}\n",
        "base_path = f'/content/drive/MyDrive/MyModel/Stage1_Chemical_NetworkModel1_Seed42-k9/{EXPERIMENT_CONFIG[\"experiment_name\"]}/'\n",
        "\n",
        "\n",
        "# Create experiment-specific directory\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "print(f\"Experiment: {EXPERIMENT_CONFIG['experiment_name']}\")\n",
        "print(f\"Config: {EXPERIMENT_CONFIG}\")\n",
        "print(f\"Directory: {base_path}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(EXPERIMENT_CONFIG['training_seed'])\n",
        "np.random.seed(EXPERIMENT_CONFIG['training_seed'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(EXPERIMENT_CONFIG['training_seed'])\n",
        "\n",
        "# Load data\n",
        "hyG, v_feat, e_feat, drug_X, metadata = load_data_and_create_graphs()\n",
        "train_pos_g, train_neg_g, val_pos_g, val_neg_g, test_pos_g, test_neg_g = load_train_test_data()\n",
        "\n",
        "# Create model and decoder\n",
        "model = Model(drug_X.shape[1], EXPERIMENT_CONFIG)\n",
        "decoder = MLPPredictor(EXPERIMENT_CONFIG['hidden_units'])\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    itertools.chain(model.parameters(), decoder.parameters()),\n",
        "    lr=EXPERIMENT_CONFIG['learning_rate']\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# Training variables\n",
        "best_val_loss = 1e10\n",
        "patience = 0\n",
        "best_embeddings = None\n",
        "best_epoch = 0\n",
        "\n",
        "training_start_time = time.time()\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "\n",
        "print(\"\\nStarting Stage 2 (Metabolic Network) training...\")\n",
        "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(training_start_time))}\")\n",
        "\n",
        "for e in range(500):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    decoder.train()\n",
        "    h = model(hyG, v_feat, e_feat, True, True)\n",
        "    h_drug = h[1]  # Get drug embeddings\n",
        "    pos_score = decoder(train_pos_g, h_drug)\n",
        "    neg_score = decoder(train_neg_g, h_drug)\n",
        "    loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "    # Simple backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        decoder.eval()\n",
        "    \n",
        "        pos_score_val = decoder(val_pos_g, h_drug)\n",
        "        neg_score_val = decoder(val_neg_g, h_drug)\n",
        "        val_loss = compute_loss(pos_score_val, neg_score_val)\n",
        "\n",
        "        # Simple model selection\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_embeddings = h_drug.clone()  # Save training embeddings\n",
        "            best_epoch = e\n",
        "            patience = 0\n",
        "\n",
        "            # Save models\n",
        "            torch.save(decoder.state_dict(), f'{base_path}decoder_best.pth')\n",
        "            torch.save(model.state_dict(), f'{base_path}model_best.pth')\n",
        "\n",
        "        else:\n",
        "            patience += 1\n",
        "\n",
        "\n",
        "        # Simple early stopping\n",
        "        if patience > 200:\n",
        "            print(f\"Early stopping at epoch {e}\")\n",
        "            break\n",
        "\n",
        "\n",
        "    # Progress reporting\n",
        "    if e % 10 == 0:\n",
        "        print(f'Epoch {e}, train loss: {loss:.4f}, val loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience})')\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "total_training_time = training_end_time - training_start_time\n",
        "# Get RAM usage after training\n",
        "ram_after = calculate_ram_usage()\n",
        "ram_used = ram_after - ram_before\n",
        "\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"\\nTiming Statistics:\")\n",
        "print(f\"  Total training time: {total_training_time:.2f} seconds\")\n",
        "print(f\"  Total training time: {total_training_time/60:.2f} minutes\")\n",
        "print(f\"  Total training time: {total_training_time/3600:.2f} hours\")\n",
        "print(f\"  Average time per epoch: {total_training_time/(e+1):.2f} seconds\")\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"RAM usage after training:  {ram_after:.2f} GB\")\n",
        "print(f\"RAM used during training:  {ram_used:.2f} GB\")\n",
        "print(f\"  Epochs completed: {e+1}\")\n",
        "print(f\"  End time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(training_end_time))}\")\n",
        "\n",
        "# Use best embeddings for final evaluation\n",
        "H = best_embeddings\n",
        "E = best_epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK5FvA9gT-qU",
        "outputId": "3901f264-da2f-434b-fb7c-291e3948fa32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Epoch: 494, Accuracy: 0.9320, Precision: 0.9236, Recall: 0.9449, F1-score 0.9341, ROC-AUC 0.9846, PR-AUC 0.9847\n",
            "Final drug embeddings shape: torch.Size([1709, 128])\n",
            "These embeddings will be used as input to the second (metabolic) network\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluation\n",
        "decoder.load_state_dict(torch.load(f'{base_path}decoder_best.pth'))\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    pos_score = decoder(test_pos_g, H)\n",
        "    neg_score = decoder(test_neg_g, H)\n",
        "    test_acc = compute_auc(pos_score, neg_score)\n",
        "\n",
        "scores = torch.cat([pos_score, neg_score])\n",
        "labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "\n",
        "m1 = tf.keras.metrics.BinaryAccuracy()\n",
        "m1.update_state(labels, scores)\n",
        "\n",
        "sig_scores = F.sigmoid(scores)\n",
        "m2 = tf.keras.metrics.Precision()\n",
        "m2.update_state(labels, sig_scores)\n",
        "M2 = m2.result().numpy()\n",
        "\n",
        "m3 = tf.keras.metrics.Recall()\n",
        "m3.update_state(labels, sig_scores)\n",
        "M3 = m3.result().numpy()\n",
        "\n",
        "F1 = 2*(M2*M3)/(M2+M3)\n",
        "print('Best Epoch: {}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score {:.4f}, ROC-AUC {:.4f}, PR-AUC {:.4f}'.format(\n",
        "    E, m1.result().numpy(), M2, M3, F1, test_acc[0], test_acc[1]))\n",
        "\n",
        "# Save the final drug embeddings for the second network\n",
        "print(f\"Final drug embeddings shape: {H.shape}\")\n",
        "print(\"These embeddings will be used as input to the second (metabolic) network\")\n",
        "\n",
        "# Save embeddings and metadata for second network\n",
        "torch.save({\n",
        "    'drug_embeddings': H,\n",
        "    'drug_to_id': metadata['drug_to_idx'],  # Changed key name\n",
        "    'best_epoch': E,\n",
        "    'final_performance': {\n",
        "        'accuracy': m1.result().numpy(),\n",
        "        'precision': M2,\n",
        "        'recall': M3,\n",
        "        'f1': F1,\n",
        "        'roc_auc': test_acc[0],\n",
        "        'pr_auc': test_acc[1]\n",
        "    }\n",
        "}, f'{base_path}chemical_network_output.pt')\n",
        "#torch.save(chemical_output, f'{base_path}embeddings_and_metadata.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The data required by the final module is loaded for a comprehensive view of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8LHoEApMZ4-"
      },
      "outputs": [],
      "source": [
        "def load_train_test_data():\n",
        "    \"\"\"Load your CSV files and create DGL graphs for training\"\"\"\n",
        "\n",
        "    # Load positive samples\n",
        "    train_pos = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/train.csv')\n",
        "    val_pos = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/val.csv')\n",
        "    test_pos = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/test.csv')\n",
        "\n",
        "    # Load negative samples\n",
        "    train_neg = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/processed_with_negatives/train_negatives.csv')\n",
        "    val_neg = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/processed_with_negatives/val_negatives.csv')\n",
        "    test_neg = pd.read_csv('/content/drive/MyDrive/MyModel/train-seed42/processed_with_negatives/test_negatives.csv')\n",
        "\n",
        "    # Get metadata and drug mapping\n",
        "    metadata = torch.load('/content/drive/MyDrive/MyModel/hypergraphs/hyG_drug_drugbank_kmer_9_metadata.pt', weights_only=False)\n",
        "\n",
        "    num_drugs = len(metadata['drug_to_idx'])\n",
        "    drug_to_id_mapping = metadata['drug_to_idx']\n",
        "\n",
        "    def create_dgl_graph(df, num_nodes, drug_mapping):\n",
        "        \"\"\"Create DGL graph from dataframe with drug pairs\"\"\"\n",
        "        if 'Drug1_ID' in df.columns and 'Drug2_ID' in df.columns:\n",
        "            src_ids = df['Drug1_ID'].values\n",
        "            dst_ids = df['Drug2_ID'].values\n",
        "        else:\n",
        "            print(\"Available columns:\", df.columns.tolist())\n",
        "            src_ids = df.iloc[:, 0].values  # First column\n",
        "            dst_ids = df.iloc[:, 1].values  # Second column\n",
        "\n",
        "        # Convert DrugBank IDs to integer indices using the mapping\n",
        "        src = torch.tensor([drug_mapping[drug_id] for drug_id in src_ids], dtype=torch.long)\n",
        "        dst = torch.tensor([drug_mapping[drug_id] for drug_id in dst_ids], dtype=torch.long)\n",
        "\n",
        "        return dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "\n",
        "    # Create DGL graphs\n",
        "    train_pos_g = create_dgl_graph(train_pos, num_drugs, drug_to_id_mapping)\n",
        "    val_pos_g = create_dgl_graph(val_pos, num_drugs, drug_to_id_mapping)\n",
        "    test_pos_g = create_dgl_graph(test_pos, num_drugs, drug_to_id_mapping)\n",
        "\n",
        "    train_neg_g = create_dgl_graph(train_neg, num_drugs, drug_to_id_mapping)\n",
        "    val_neg_g = create_dgl_graph(val_neg, num_drugs, drug_to_id_mapping)\n",
        "    test_neg_g = create_dgl_graph(test_neg, num_drugs, drug_to_id_mapping)\n",
        "\n",
        "    print(f\"Train positive edges: {train_pos_g.number_of_edges()}\")\n",
        "    print(f\"Train negative edges: {train_neg_g.number_of_edges()}\")\n",
        "    print(f\"Validation positive edges: {val_pos_g.number_of_edges()}\")\n",
        "    print(f\"Validation negative edges: {val_neg_g.number_of_edges()}\")\n",
        "    print(f\"Test positive edges: {test_pos_g.number_of_edges()}\")\n",
        "    print(f\"Test negative edges: {test_neg_g.number_of_edges()}\")\n",
        "\n",
        "    # Return both graphs AND dataframes\n",
        "    return (train_pos_g, train_neg_g, val_pos_g, val_neg_g, test_pos_g, test_neg_g,\n",
        "            train_pos, train_neg, val_pos, val_neg, test_pos, test_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBEAp-hSMgZb",
        "outputId": "0d964b5c-612a-4f43-eb39-dddb83d82a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of drugs: 1709\n",
            "Number of substructures: 29462\n",
            "Hypergraph structure:\n",
            "Graph(num_nodes={'edge': 1709, 'node': 29462},\n",
            "      num_edges={('edge', 'con', 'node'): 96656, ('node', 'in', 'edge'): 96656},\n",
            "      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])\n",
            "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "Train positive edges: 153501\n",
            "Train negative edges: 153501\n",
            "Validation positive edges: 19187\n",
            "Validation negative edges: 19187\n",
            "Test positive edges: 19189\n",
            "Test negative edges: 19189\n",
            "Loaded embeddings from epoch 494\n",
            "Embedding shape: torch.Size([1709, 128])\n",
            "================================================================================\n",
            "AGGREGATE PERFORMANCE METRICS\n",
            "================================================================================\n",
            "Best Epoch: 494\n",
            "Accuracy:   0.9320\n",
            "Precision:  0.9236\n",
            "Recall:     0.9449\n",
            "F1-Score:   0.9341\n",
            "ROC-AUC:    0.9846\n",
            "PR-AUC:     0.9847\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted Negative    Predicted Positive\n",
            "Actual Negative         17690                1499\n",
            "Actual Positive          1058               18131\n",
            "\n",
            "================================================================================\n",
            "INDIVIDUAL PREDICTIONS\n",
            "================================================================================\n",
            "\n",
            "All predictions saved to: /content/drive/MyDrive/MyModel/Stage1_Chemical_NetworkModel1_Seed42-k9/aggressive_01/test_predictions_detailed.csv\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SAMPLE PREDICTIONS (First 20)\n",
            "--------------------------------------------------------------------------------\n",
            "Drug1_ID Drug2_ID  True_Label  Predicted_Label  Prediction_Score  Correct\n",
            " DB00177  DB00266         1.0                1          0.999970        1\n",
            " DB00270  DB00426         1.0                1          0.999972        1\n",
            " DB00218  DB01075         1.0                1          0.779016        1\n",
            " DB00655  DB01250         1.0                1          0.999989        1\n",
            " DB01590  DB08933         1.0                1          0.987086        1\n",
            " DB00762  DB08896         1.0                1          1.000000        1\n",
            " DB01626  DB06826         1.0                1          0.714525        1\n",
            " DB00191  DB01142         1.0                1          0.999979        1\n",
            " DB00344  DB06701         1.0                1          1.000000        1\n",
            " DB00679  DB09242         1.0                1          0.977105        1\n",
            " DB01045  DB01181         1.0                1          0.987480        1\n",
            " DB00807  DB06716         1.0                1          1.000000        1\n",
            " DB00243  DB04794         1.0                1          0.801562        1\n",
            " DB00872  DB05109         1.0                1          0.998242        1\n",
            " DB00334  DB01081         1.0                1          0.999694        1\n",
            " DB00701  DB06616         1.0                1          1.000000        1\n",
            " DB01168  DB04840         1.0                1          0.965928        1\n",
            " DB00298  DB01154         1.0                1          1.000000        1\n",
            " DB00244  DB08439         1.0                1          1.000000        1\n",
            " DB00404  DB00623         1.0                1          1.000000        1\n",
            "\n",
            "================================================================================\n",
            "ERROR ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "False Positives: 1499 cases\n",
            "(Predicted interaction, but drugs DON'T actually interact)\n",
            "--------------------------------------------------------------------------------\n",
            "Drug1_ID Drug2_ID  True_Label  Predicted_Label  Prediction_Score  Correct\n",
            " DB06707  DB12332         0.0                1          0.739660        0\n",
            " DB00651  DB01223         0.0                1          0.625469        0\n",
            " DB01254  DB13879         0.0                1          0.502548        0\n",
            " DB00484  DB01255         0.0                1          0.721426        0\n",
            " DB00285  DB01255         0.0                1          0.723580        0\n",
            " DB01120  DB08895         0.0                1          0.660028        0\n",
            " DB01065  DB01253         0.0                1          0.964842        0\n",
            " DB00648  DB01685         0.0                1          0.884422        0\n",
            " DB00413  DB08893         0.0                1          0.668368        0\n",
            " DB00905  DB01045         0.0                1          0.780231        0\n",
            "\n",
            "All false positives saved to: /content/drive/MyDrive/MyModel/Stage1_Chemical_NetworkModel1_Seed42-k9/aggressive_01/false_positives.csv\n",
            "\n",
            "\n",
            "False Negatives: 1058 cases\n",
            "(Predicted NO interaction, but drugs DO actually interact)\n",
            "--------------------------------------------------------------------------------\n",
            "Drug1_ID Drug2_ID  True_Label  Predicted_Label  Prediction_Score  Correct\n",
            " DB00334  DB01416         1.0                0          0.010515        0\n",
            " DB00850  DB01214         1.0                0          0.359366        0\n",
            " DB00331  DB04576         1.0                0          0.340235        0\n",
            " DB00651  DB01032         1.0                0          0.013695        0\n",
            " DB00608  DB04786         1.0                0          0.228101        0\n",
            " DB00399  DB00653         1.0                0          0.018206        0\n",
            " DB00852  DB06603         1.0                0          0.325250        0\n",
            " DB00255  DB09119         1.0                0          0.284545        0\n",
            " DB00435  DB01210         1.0                0          0.209870        0\n",
            " DB00223  DB00783         1.0                0          0.371884        0\n",
            "\n",
            "All false negatives saved to: /content/drive/MyDrive/MyModel/Stage1_Chemical_NetworkModel1_Seed42-k9/aggressive_01/false_negatives.csv\n",
            "\n",
            "================================================================================\n",
            "PREDICTION CONFIDENCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "High Confidence Correct: 31480 cases\n",
            "Uncertain Predictions (0.4-0.6): 1193 cases\n",
            "\n",
            "Sample Uncertain Predictions:\n",
            "Drug1_ID Drug2_ID  True_Label  Predicted_Label  Prediction_Score  Correct\n",
            " DB00541  DB13179         1.0                1          0.592641        1\n",
            " DB00163  DB00974         1.0                1          0.552880        1\n",
            " DB01029  DB08941         1.0                1          0.514839        1\n",
            " DB00539  DB00927         1.0                1          0.599637        1\n",
            " DB00696  DB01587         1.0                0          0.477747        0\n",
            " DB00514  DB00601         1.0                1          0.562861        1\n",
            " DB01418  DB09289         1.0                1          0.552701        1\n",
            " DB00944  DB09095         1.0                1          0.577350        1\n",
            " DB00904  DB01001         1.0                1          0.503585        1\n",
            " DB00246  DB01410         1.0                0          0.471200        0\n",
            "\n",
            "================================================================================\n",
            "STATISTICS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Total Test Samples: 38378\n",
            "  - Positive (interact): 19189\n",
            "  - Negative (no interact): 19189\n",
            "\n",
            "Correct Predictions: 35821 (93.34%)\n",
            "Incorrect Predictions: 2557 (6.66%)\n",
            "\n",
            "Prediction Score Statistics:\n",
            "  Mean: 0.5051\n",
            "  Std:  0.4618\n",
            "  Min:  0.0000\n",
            "  Max:  1.0000\n",
            "\n",
            "\n",
            "Evaluation summary saved to: /content/drive/MyDrive/MyModel/Stage1_Chemical_NetworkModel1_Seed42-k9/aggressive_01/evaluation_summary.json\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# ======================= LOAD DATA =======================\n",
        "# Load hypergraph and test graphs\n",
        "hyG, v_feat, e_feat, drug_X, metadata = load_data_and_create_graphs()\n",
        "# Load train/test data - now receiving both graphs AND dataframes\n",
        "(train_pos_g, train_neg_g, val_pos_g, val_neg_g, test_pos_g, test_neg_g,\n",
        " train_pos_df, train_neg_df, val_pos_df, val_neg_df, test_pos, test_neg) = load_train_test_data()\n",
        "# ======================= CREATE MODEL INSTANCES =======================\n",
        "\n",
        "model = Model(drug_X.shape[1], EXPERIMENT_CONFIG)\n",
        "decoder = MLPPredictor(EXPERIMENT_CONFIG['hidden_units'])\n",
        "\n",
        "# ======================= LOAD TRAINED WEIGHTS =======================\n",
        "# Load the saved checkpoint\n",
        "checkpoint = torch.load(f'{base_path}chemical_network_output.pt', weights_only=False)\n",
        "\n",
        "H = checkpoint['drug_embeddings']  # The best drug embeddings [1709, 128]\n",
        "E = checkpoint['best_epoch']       # Best epoch number\n",
        "\n",
        "print(f\"Loaded embeddings from epoch {E}\")\n",
        "print(f\"Embedding shape: {H.shape}\")\n",
        "\n",
        "# Load model weights\n",
        "\n",
        "model.load_state_dict(torch.load(f'{base_path}model_best.pth', weights_only=False))\n",
        "decoder.load_state_dict(torch.load(f'{base_path}decoder_best.pth', weights_only=False))\n",
        "\n",
        "# ======================= EVALUATION =======================\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Get predictions\n",
        "    pos_score = decoder(test_pos_g, H)\n",
        "    neg_score = decoder(test_neg_g, H)\n",
        "\n",
        "    # Compute AUC metrics\n",
        "    test_acc = compute_auc(pos_score, neg_score)\n",
        "\n",
        "# Prepare data\n",
        "scores = torch.cat([pos_score, neg_score])\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_score.shape[0]),\n",
        "    torch.zeros(neg_score.shape[0])\n",
        "])\n",
        "\n",
        "# Convert to probabilities\n",
        "sig_scores = F.sigmoid(scores)\n",
        "predictions = (sig_scores > 0.5).long()\n",
        "\n",
        "# ======================= AGGREGATE METRICS =======================\n",
        "\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AGGREGATE PERFORMANCE METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Using TensorFlow metrics\n",
        "m1 = tf.keras.metrics.BinaryAccuracy()\n",
        "m1.update_state(labels, scores)\n",
        "\n",
        "m2 = tf.keras.metrics.Precision()\n",
        "m2.update_state(labels, sig_scores)\n",
        "M2 = m2.result().numpy()\n",
        "\n",
        "m3 = tf.keras.metrics.Recall()\n",
        "m3.update_state(labels, sig_scores)\n",
        "M3 = m3.result().numpy()\n",
        "\n",
        "F1 = 2 * (M2 * M3) / (M2 + M3)\n",
        "\n",
        "print(f'Best Epoch: {E}')\n",
        "print(f'Accuracy:   {m1.result().numpy():.4f}')\n",
        "print(f'Precision:  {M2:.4f}')\n",
        "print(f'Recall:     {M3:.4f}')\n",
        "print(f'F1-Score:   {F1:.4f}')\n",
        "print(f'ROC-AUC:    {test_acc[0]:.4f}')\n",
        "print(f'PR-AUC:     {test_acc[1]:.4f}')\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(labels.numpy(), predictions.numpy())\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"                Predicted Negative    Predicted Positive\")\n",
        "print(f\"Actual Negative        {cm[0,0]:6d}              {cm[0,1]:6d}\")\n",
        "print(f\"Actual Positive        {cm[1,0]:6d}              {cm[1,1]:6d}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"INDIVIDUAL PREDICTIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "\n",
        "# Get drug IDs (handle different column names)\n",
        "if 'Drug1_ID' in test_pos.columns:\n",
        "    pos_drug1 = test_pos['Drug1_ID'].tolist()\n",
        "    pos_drug2 = test_pos['Drug2_ID'].tolist()\n",
        "    neg_drug1 = test_neg['Drug1_ID'].tolist()\n",
        "    neg_drug2 = test_neg['Drug2_ID'].tolist()\n",
        "else:\n",
        "    pos_drug1 = test_pos.iloc[:, 0].tolist()\n",
        "    pos_drug2 = test_pos.iloc[:, 1].tolist()\n",
        "    neg_drug1 = test_neg.iloc[:, 0].tolist()\n",
        "    neg_drug2 = test_neg.iloc[:, 1].tolist()\n",
        "\n",
        "# Create results dataframe\n",
        "#positive pairs THEN negative pairs\n",
        "results_df = pd.DataFrame({\n",
        "    'Drug1_ID': pos_drug1 + neg_drug1,\n",
        "    'Drug2_ID': pos_drug2 + neg_drug2,\n",
        "    'True_Label': labels.numpy(),\n",
        "    'Predicted_Label': predictions.numpy(),\n",
        "    'Prediction_Score': sig_scores.numpy(),\n",
        "    'Correct': (predictions.numpy() == labels.numpy()).astype(int)\n",
        "})\n",
        "\n",
        "# Save all predictions\n",
        "results_df.to_csv(f'{base_path}test_predictions_detailed.csv', index=False)\n",
        "print(f\"\\nAll predictions saved to: {base_path}test_predictions_detailed.csv\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"SAMPLE PREDICTIONS (First 20)\")\n",
        "print(\"-\" * 80)\n",
        "print(results_df.head(20).to_string(index=False))\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# False Positives (Type I Error)\n",
        "false_positives = results_df[\n",
        "    (results_df['True_Label'] == 0) & (results_df['Predicted_Label'] == 1)\n",
        "]\n",
        "print(f\"\\nFalse Positives: {len(false_positives)} cases\")\n",
        "print(\"(Predicted interaction, but drugs DON'T actually interact)\")\n",
        "print(\"-\" * 80)\n",
        "if len(false_positives) > 0:\n",
        "    print(false_positives.head(10).to_string(index=False))\n",
        "    false_positives.to_csv(f'{base_path}false_positives.csv', index=False)\n",
        "    print(f\"\\nAll false positives saved to: {base_path}false_positives.csv\")\n",
        "\n",
        "# False Negatives (Type II Error)\n",
        "false_negatives = results_df[\n",
        "    (results_df['True_Label'] == 1) & (results_df['Predicted_Label'] == 0)\n",
        "]\n",
        "print(f\"\\n\\nFalse Negatives: {len(false_negatives)} cases\")\n",
        "print(\"(Predicted NO interaction, but drugs DO actually interact)\")\n",
        "print(\"-\" * 80)\n",
        "if len(false_negatives) > 0:\n",
        "    print(false_negatives.head(10).to_string(index=False))\n",
        "    false_negatives.to_csv(f'{base_path}false_negatives.csv', index=False)\n",
        "    print(f\"\\nAll false negatives saved to: {base_path}false_negatives.csv\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PREDICTION CONFIDENCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# High confidence correct predictions\n",
        "high_conf_correct = results_df[\n",
        "    (results_df['Correct'] == 1) &\n",
        "    ((results_df['Prediction_Score'] > 0.9) | (results_df['Prediction_Score'] < 0.1))\n",
        "]\n",
        "print(f\"\\nHigh Confidence Correct: {len(high_conf_correct)} cases\")\n",
        "\n",
        "# Low confidence predictions (uncertain)\n",
        "uncertain = results_df[\n",
        "    (results_df['Prediction_Score'] > 0.4) &\n",
        "    (results_df['Prediction_Score'] < 0.6)\n",
        "]\n",
        "print(f\"Uncertain Predictions (0.4-0.6): {len(uncertain)} cases\")\n",
        "if len(uncertain) > 0:\n",
        "    print(\"\\nSample Uncertain Predictions:\")\n",
        "    print(uncertain.head(10).to_string(index=False))\n",
        "    uncertain.to_csv(f'{base_path}uncertain_predictions.csv', index=False)\n",
        "\n",
        "# ======================= STATISTICS SUMMARY =======================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STATISTICS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nTotal Test Samples: {len(results_df)}\")\n",
        "print(f\"  - Positive (interact): {int(labels.sum())}\")\n",
        "print(f\"  - Negative (no interact): {len(labels) - int(labels.sum())}\")\n",
        "\n",
        "print(f\"\\nCorrect Predictions: {results_df['Correct'].sum()} ({results_df['Correct'].mean()*100:.2f}%)\")\n",
        "print(f\"Incorrect Predictions: {len(results_df) - results_df['Correct'].sum()} ({(1-results_df['Correct'].mean())*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nPrediction Score Statistics:\")\n",
        "print(f\"  Mean: {results_df['Prediction_Score'].mean():.4f}\")\n",
        "print(f\"  Std:  {results_df['Prediction_Score'].std():.4f}\")\n",
        "print(f\"  Min:  {results_df['Prediction_Score'].min():.4f}\")\n",
        "print(f\"  Max:  {results_df['Prediction_Score'].max():.4f}\")\n",
        "\n",
        "\n",
        "summary_report = {\n",
        "    'Best_Epoch': E,\n",
        "    'Accuracy': float(m1.result().numpy()),\n",
        "    'Precision': float(M2),\n",
        "    'Recall': float(M3),\n",
        "    'F1_Score': float(F1),\n",
        "    'ROC_AUC': float(test_acc[0]),\n",
        "    'PR_AUC': float(test_acc[1]),\n",
        "    'Total_Samples': len(results_df),\n",
        "    'True_Positives': int(cm[1,1]),\n",
        "    'True_Negatives': int(cm[0,0]),\n",
        "    'False_Positives': int(cm[0,1]),\n",
        "    'False_Negatives': int(cm[1,0]),\n",
        "    'High_Confidence_Correct': len(high_conf_correct),\n",
        "    'Uncertain_Predictions': len(uncertain)\n",
        "}\n",
        "\n",
        "# Save as JSON\n",
        "import json\n",
        "with open(f'{base_path}evaluation_summary.json', 'w') as f:\n",
        "    json.dump(summary_report, f, indent=4)\n",
        "\n",
        "print(f\"\\n\\nEvaluation summary saved to: {base_path}evaluation_summary.json\")\n",
        "print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhNu1-1uZ8em",
        "outputId": "2d2d1646-3402-48ec-bd48-c6dad170f42c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0zmkt_9aBQC",
        "outputId": "bbabcd1d-467e-48b9-87c2-8618ffabf0dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Starting FIXED drug interaction dataset partitioning...\n",
            " Target ratios: 80% train, 10% validation, 10% test\n",
            " Seeds: [32, 42, 46]\n",
            " Strategy: Guarantee all drugs in training first, then distribute remaining interactions\n",
            "================================================================================\n",
            "DRUG INTERACTION DATASET PARTITIONING - FIXED NO COLD START\n",
            "================================================================================\n",
            " Loading data from: /content/drive/MyDrive/MLHygnn/DB/DataSetOf1709Drugs/DDI_unique_interactionsAnalysis.csv\n",
            " Total interactions: 191,877\n",
            " Total unique drugs: 1,709\n",
            "\n",
            " TARGET SPLIT SIZES:\n",
            "Training: 153,501 samples (80.0%)\n",
            "Validation: 19,187 samples (10.0%)\n",
            "Test: 19,189 samples (10.0%)\n",
            "\n",
            "============================================================\n",
            " PROCESSING SEED 32\n",
            "============================================================\n",
            "ðŸ” Step 1: Ensuring all drugs appear in training...\n",
            "    Guaranteed training interactions (covers all drugs): 1,435\n",
            "    Remaining interactions for distribution: 190,442\n",
            "    Drugs covered in training: 1,709/1,709\n",
            "\n",
            " DISTRIBUTION CALCULATION:\n",
            "   Guaranteed training samples: 1,435\n",
            "   Additional training needed: 152,066\n",
            "   Remaining for val/test distribution: 190,442\n",
            "\n",
            " FINAL VERIFICATION:\n",
            "   Training samples: 153,501\n",
            "   Validation samples: 19,187\n",
            "   Test samples: 19,189\n",
            "   Total samples: 191,877\n",
            "   Training drugs: 1,709\n",
            "   Validation drugs: 1,534\n",
            "   Test drugs: 1,517\n",
            "\n",
            " FILES SAVED:\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed32/train_accurate_redistribution.csv\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed32/val_accurate_redistribution.csv\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed32/test_accurate_redistribution.csv\n",
            "    SUCCESS: All 1,709 drugs are present in training!\n",
            "\n",
            "============================================================\n",
            " PROCESSING SEED 42\n",
            "============================================================\n",
            "ðŸ” Step 1: Ensuring all drugs appear in training...\n",
            "    Guaranteed training interactions (covers all drugs): 1,409\n",
            "    Remaining interactions for distribution: 190,468\n",
            "    Drugs covered in training: 1,709/1,709\n",
            "\n",
            " DISTRIBUTION CALCULATION:\n",
            "   Guaranteed training samples: 1,409\n",
            "   Additional training needed: 152,092\n",
            "   Remaining for val/test distribution: 190,468\n",
            "\n",
            " FINAL VERIFICATION:\n",
            "   Training samples: 153,501\n",
            "   Validation samples: 19,187\n",
            "   Test samples: 19,189\n",
            "   Total samples: 191,877\n",
            "   Training drugs: 1,709\n",
            "   Validation drugs: 1,536\n",
            "   Test drugs: 1,540\n",
            "\n",
            " FILES SAVED:\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed42/train_accurate_redistribution.csv\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed42/val_accurate_redistribution.csv\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed42/test_accurate_redistribution.csv\n",
            "    SUCCESS: All 1,709 drugs are present in training!\n",
            "\n",
            "============================================================\n",
            " PROCESSING SEED 46\n",
            "============================================================\n",
            "ðŸ” Step 1: Ensuring all drugs appear in training...\n",
            "    Guaranteed training interactions (covers all drugs): 1,420\n",
            "    Remaining interactions for distribution: 190,457\n",
            "    Drugs covered in training: 1,709/1,709\n",
            "\n",
            " DISTRIBUTION CALCULATION:\n",
            "   Guaranteed training samples: 1,420\n",
            "   Additional training needed: 152,081\n",
            "   Remaining for val/test distribution: 190,457\n",
            "\n",
            " FINAL VERIFICATION:\n",
            "   Training samples: 153,501\n",
            "   Validation samples: 19,187\n",
            "   Test samples: 19,189\n",
            "   Total samples: 191,877\n",
            "   Training drugs: 1,709\n",
            "   Validation drugs: 1,508\n",
            "   Test drugs: 1,534\n",
            "\n",
            " FILES SAVED:\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed46/train_accurate_redistribution.csv\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed46/val_accurate_redistribution.csv\n",
            "   /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4/train-seed46/test_accurate_redistribution.csv\n",
            "    SUCCESS: All 1,709 drugs are present in training!\n",
            "\n",
            " FIXED PARTITIONING COMPLETED!\n",
            " GUARANTEED: No cold start problem - all drugs in validation/test also appear in training!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def partition_drug_interactions(input_path, output_base_path, seeds=[32, 42, 46],\n",
        "                                                   train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Partition drug interaction dataset with GUARANTEED no cold start problem\n",
        "\n",
        "    Strategy:\n",
        "    1. Ensure ALL drugs appear in training first\n",
        "    2. Then distribute remaining interactions to val/test\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"DRUG INTERACTION DATASET PARTITIONING - FIXED NO COLD START\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load the complete dataset\n",
        "    print(f\" Loading data from: {input_path}\")\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    print(f\" Total interactions: {len(df):,}\")\n",
        "\n",
        "    # Get all unique drugs\n",
        "    all_drugs = set(df['Drug1_ID'].unique()) | set(df['Drug2_ID'].unique())\n",
        "    all_drugs = list(all_drugs)\n",
        "    print(f\" Total unique drugs: {len(all_drugs):,}\")\n",
        "\n",
        "    # Calculate target split sizes\n",
        "    total_samples = len(df)\n",
        "    target_train_size = int(total_samples * train_ratio)\n",
        "    target_val_size = int(total_samples * val_ratio)\n",
        "    target_test_size = total_samples - target_train_size - target_val_size\n",
        "\n",
        "    print(f\"\\n TARGET SPLIT SIZES:\")\n",
        "    print(f\"Training: {target_train_size:,} samples ({train_ratio*100:.1f}%)\")\n",
        "    print(f\"Validation: {target_val_size:,} samples ({val_ratio*100:.1f}%)\")\n",
        "    print(f\"Test: {target_test_size:,} samples ({test_ratio*100:.1f}%)\")\n",
        "\n",
        "    # Process each seed\n",
        "    for seed in seeds:\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(f\" PROCESSING SEED {seed}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Create output directory\n",
        "        seed_output_dir = os.path.join(output_base_path, f'train-seed{seed}')\n",
        "        os.makedirs(seed_output_dir, exist_ok=True)\n",
        "\n",
        "        # Step 1: Shuffle the dataset\n",
        "        df_shuffled = shuffle(df, random_state=seed).reset_index(drop=True) # =true ; Discards the old index instead of adding it as a new column\n",
        "\n",
        "        # Step 2: Ensure all drugs appear in training\n",
        "        print(\"ðŸ” Step 1: Ensuring all drugs appear in training...\")\n",
        "\n",
        "        # Track which drugs we've seen in training\n",
        "        drugs_in_training = set()\n",
        "        guaranteed_training_indices = [] #If both drugs/one of them fisrt seen /exist in training\n",
        "        remaining_indices = [] #If both drugs already exist in training\n",
        "\n",
        "        # Go through shuffled data and collect interactions until all drugs are covered\n",
        "        for idx, row in df_shuffled.iterrows():\n",
        "            drug1, drug2 = row['Drug1_ID'], row['Drug2_ID']\n",
        "\n",
        "            # If we haven't seen both drugs in training yet, add to guaranteed training\n",
        "            if drug1 not in drugs_in_training or drug2 not in drugs_in_training:\n",
        "                guaranteed_training_indices.append(idx)\n",
        "                drugs_in_training.add(drug1)\n",
        "                drugs_in_training.add(drug2)\n",
        "            else:\n",
        "                remaining_indices.append(idx)\n",
        "\n",
        "            # Check if we have all drugs || Early stop optimization; Once all drugs are covered, stop looping\n",
        "            if len(drugs_in_training) == len(all_drugs):\n",
        "                # Add remaining indices to remaining_indices\n",
        "                remaining_indices.extend(range(idx+1, len(df_shuffled)))\n",
        "                break\n",
        "\n",
        "        print(f\"    Guaranteed training interactions (covers all drugs): {len(guaranteed_training_indices):,}\")\n",
        "        print(f\"    Remaining interactions for distribution: {len(remaining_indices):,}\")\n",
        "        print(f\"    Drugs covered in training: {len(drugs_in_training):,}/{len(all_drugs):,}\")\n",
        "\n",
        "        # Step 3: Create DataFrames from indices\n",
        "        guaranteed_train_df = df_shuffled.iloc[guaranteed_training_indices].copy()\n",
        "        remaining_df = df_shuffled.iloc[remaining_indices].copy() if remaining_indices else pd.DataFrame(columns=df.columns)\n",
        "\n",
        "        # Shuffle remaining interactions\n",
        "        if len(remaining_df) > 0:\n",
        "            remaining_df = shuffle(remaining_df, random_state=seed + 1000).reset_index(drop=True)\n",
        "\n",
        "        # Calculate how many more samples we need for each split\n",
        "        guaranteed_train_size = len(guaranteed_training_indices)\n",
        "        additional_train_needed = max(0, target_train_size - guaranteed_train_size)\n",
        "\n",
        "        print(f\"\\n DISTRIBUTION CALCULATION:\")\n",
        "        print(f\"   Guaranteed training samples: {guaranteed_train_size:,}\")\n",
        "        print(f\"   Additional training needed: {additional_train_needed:,}\")\n",
        "        print(f\"   Remaining for val/test distribution: {len(remaining_indices):,}\")\n",
        "\n",
        "        # Distribute remaining interactions\n",
        "        if len(remaining_df) > 0:\n",
        "            # Take additional training samples if needed\n",
        "            additional_train_end = min(additional_train_needed, len(remaining_df))\n",
        "            additional_train_df = remaining_df.iloc[:additional_train_end].copy()\n",
        "\n",
        "            # Distribute the rest to val and test\n",
        "            remaining_for_val_test = remaining_df.iloc[additional_train_end:].copy()\n",
        "            val_size_from_remaining = min(target_val_size, len(remaining_for_val_test))\n",
        "\n",
        "            val_df_temp = remaining_for_val_test.iloc[:val_size_from_remaining].copy()\n",
        "            test_df_temp = remaining_for_val_test.iloc[val_size_from_remaining:].copy()\n",
        "        else:\n",
        "             #creates empty DataFrames\n",
        "            additional_train_df = pd.DataFrame(columns=df.columns)\n",
        "            val_df_temp = pd.DataFrame(columns=df.columns)\n",
        "            test_df_temp = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "        # Step 4: Combine final splits\n",
        "        train_df_final = pd.concat([guaranteed_train_df, additional_train_df], ignore_index=True)\n",
        "        val_df_final = val_df_temp.copy()\n",
        "        test_df_final = test_df_temp.copy()\n",
        "\n",
        "        # Shuffle final datasets\n",
        "        train_df_final = shuffle(train_df_final, random_state=seed + 2000).reset_index(drop=True) # or can use just seed without any plus value like 2000 or 3000\n",
        "        if len(val_df_final) > 0: val_df_final = shuffle(val_df_final, random_state=seed + 3000).reset_index(drop=True)\n",
        "        else:\n",
        "          break  # WRONG!\n",
        "        if len(test_df_final) > 0:\n",
        "            test_df_final = shuffle(test_df_final, random_state=seed + 4000).reset_index(drop=True)\n",
        "        else:\n",
        "          break  # WRONG!\n",
        "\n",
        "        # Step 5: Final verification\n",
        "        train_drugs_final = set(train_df_final['Drug1_ID']) | set(train_df_final['Drug2_ID'])\n",
        "        val_drugs_final = set(val_df_final['Drug1_ID']) | set(val_df_final['Drug2_ID']) if len(val_df_final) > 0 else set()\n",
        "        test_drugs_final = set(test_df_final['Drug1_ID']) | set(test_df_final['Drug2_ID']) if len(test_df_final) > 0 else set()\n",
        "\n",
        "\n",
        "        print(f\"\\n FINAL VERIFICATION:\")\n",
        "        print(f\"   Training samples: {len(train_df_final):,}\")\n",
        "        print(f\"   Validation samples: {len(val_df_final):,}\")\n",
        "        print(f\"   Test samples: {len(test_df_final):,}\")\n",
        "        print(f\"   Total samples: {len(train_df_final) + len(val_df_final) + len(test_df_final):,}\")\n",
        "        print(f\"   Training drugs: {len(train_drugs_final):,}\")\n",
        "        print(f\"   Validation drugs: {len(val_drugs_final):,}\")\n",
        "        print(f\"   Test drugs: {len(test_drugs_final):,}\")\n",
        "\n",
        "        # Step 6: Save the files\n",
        "        train_path = os.path.join(seed_output_dir, 'train.csv')\n",
        "        val_path = os.path.join(seed_output_dir, 'val.csv')\n",
        "        test_path = os.path.join(seed_output_dir, 'test.csv')\n",
        "\n",
        "        train_df_final.to_csv(train_path, index=False)\n",
        "        val_df_final.to_csv(val_path, index=False)\n",
        "        test_df_final.to_csv(test_path, index=False)\n",
        "\n",
        "        print(f\"\\n FILES SAVED:\")\n",
        "        print(f\"   {train_path}\")\n",
        "        print(f\"   {val_path}\")\n",
        "        print(f\"   {test_path}\")\n",
        "\n",
        "        # Verify all drugs are in training\n",
        "        if len(train_drugs_final) == len(all_drugs):\n",
        "            print(f\"    SUCCESS: All {len(all_drugs):,} drugs are present in training!\")\n",
        "        else:\n",
        "            print(f\"    ERROR: Only {len(train_drugs_final):,}/{len(all_drugs):,} drugs in training!\")\n",
        "            missing_drugs = set(all_drugs) - train_drugs_final\n",
        "            print(f\"   Missing drugs: {list(missing_drugs)[:10]}...\")\n",
        "\n",
        "# Define paths\n",
        "input_path = '/content/drive/MyDrive/MLHygnn/DB/DataSetOf1709Drugs/DDI_unique_interactionsAnalysis.csv'\n",
        "output_base_path = '/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset4'\n",
        "\n",
        "# Define seeds and ratios\n",
        "seeds = [32, 42, 46]\n",
        "train_ratio = 0.8  # 80%\n",
        "val_ratio = 0.1    # 10%\n",
        "test_ratio = 0.1   # 10%\n",
        "\n",
        "print(\" Starting FIXED drug interaction dataset partitioning...\")\n",
        "print(f\" Target ratios: {train_ratio*100:.0f}% train, {val_ratio*100:.0f}% validation, {test_ratio*100:.0f}% test\")\n",
        "print(f\" Seeds: {seeds}\")\n",
        "print(\" Strategy: Guarantee all drugs in training first, then distribute remaining interactions\")\n",
        "\n",
        "# Run partitioning\n",
        "partition_drug_interactions(\n",
        "    input_path=input_path,\n",
        "    output_base_path=output_base_path,\n",
        "    seeds=seeds,\n",
        "    train_ratio=train_ratio,\n",
        "    val_ratio=val_ratio,\n",
        "    test_ratio=test_ratio\n",
        ")\n",
        "\n",
        "print(f\"\\n FIXED PARTITIONING COMPLETED!\")\n",
        "print(\" GUARANTEED: No cold start problem - all drugs in validation/test also appear in training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ylvsudc6E90"
      },
      "source": [
        "\n",
        "### Note on Drugs Appearing Only in the Training Split\n",
        "\n",
        "We observed that some drugs appear exclusively in the training partition and do not show up in the validation or test sets. This happens when a drug has **only one interaction** across the entire dataset. In these cases, we assign that interaction to the training split so the model can still learn from the drugâ€™s chemical structure.\n",
        "This ensures the neural network gains exposure to all available drug features during training, even when interaction data is extremely sparse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8s4oYk2eDtC",
        "outputId": "8db5387b-71ff-455d-c78d-8b772a9d23cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DRUG ANALYSIS ACROSS ALL SPLITS\n",
            "======================================================================\n",
            "Loading data...\n",
            " Training samples: 153501\n",
            " Validation samples: 19187\n",
            " Test samples: 19189\n",
            " Total positive samples: 191877\n",
            "\n",
            " DRUGS PER SPLIT:\n",
            "Training split drugs: 1709\n",
            "Validation split drugs: 1536\n",
            "Test split drugs: 1540\n",
            "\n",
            " TOTAL UNIQUE DRUGS ACROSS ALL SPLITS: 1709\n",
            "\n",
            " DRUG OVERLAP ANALYSIS:\n",
            "Drugs only in Training: 121\n",
            "Drugs only in Validation: 0\n",
            "Drugs only in Test: 0\n",
            "Drugs in ALL three splits: 1488\n",
            "Drugs in Training + Validation only: 48\n",
            "Drugs in Training + Test only: 52\n",
            "Drugs in Validation + Test only: 0\n",
            "\n",
            " VERIFICATION:\n",
            "Sum of all categories: 1709\n",
            "Total unique drugs: 1709\n",
            "Match:  YES\n",
            "\n",
            "  WARNING: Some drugs don't appear in all splits!\n",
            "This might affect negative sampling if we only use drugs from individual splits.\n",
            "Example drugs only in Training: ['DB00832', 'DB09014', 'DB00198', 'DB01073', 'DB09135']\n",
            "\n",
            "======================================================================\n",
            "\n",
            " FINAL ANSWER: Your dataset contains 1709 unique drugs total.\n",
            "If this doesn't match your expected 1,694, we need to investigate further!\n"
          ]
        }
      ],
      "source": [
        "def analyze_drugs_across_splits(train_path, val_path, test_path):\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"DRUG ANALYSIS ACROSS ALL SPLITS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load all splits\n",
        "    print(\"Loading data...\")\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    val_df = pd.read_csv(val_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    print(f\" Training samples: {len(train_df)}\")\n",
        "    print(f\" Validation samples: {len(val_df)}\")\n",
        "    print(f\" Test samples: {len(test_df)}\")\n",
        "    print(f\" Total positive samples: {len(train_df) + len(val_df) + len(test_df)}\")\n",
        "\n",
        "    # Get unique drugs from each split\n",
        "    train_drugs = set(train_df['Drug1_ID'].unique()) | set(train_df['Drug2_ID'].unique())\n",
        "    val_drugs = set(val_df['Drug1_ID'].unique()) | set(val_df['Drug2_ID'].unique())\n",
        "    test_drugs = set(test_df['Drug1_ID'].unique()) | set(test_df['Drug2_ID'].unique())\n",
        "\n",
        "    print(f\"\\n DRUGS PER SPLIT:\")\n",
        "    print(f\"Training split drugs: {len(train_drugs)}\")\n",
        "    print(f\"Validation split drugs: {len(val_drugs)}\")\n",
        "    print(f\"Test split drugs: {len(test_drugs)}\")\n",
        "\n",
        "    # Find total unique drugs across ALL splits\n",
        "    all_drugs = train_drugs | val_drugs | test_drugs\n",
        "    print(f\"\\n TOTAL UNIQUE DRUGS ACROSS ALL SPLITS: {len(all_drugs)}\")\n",
        "\n",
        "    # Analyze drug overlap between splits\n",
        "    print(f\"\\n DRUG OVERLAP ANALYSIS:\")\n",
        "\n",
        "    # Drugs only in training\n",
        "    train_only = train_drugs - val_drugs - test_drugs\n",
        "    print(f\"Drugs only in Training: {len(train_only)}\")\n",
        "\n",
        "    # Drugs only in validation\n",
        "    val_only = val_drugs - train_drugs - test_drugs\n",
        "    print(f\"Drugs only in Validation: {len(val_only)}\")\n",
        "\n",
        "    # Drugs only in test\n",
        "    test_only = test_drugs - train_drugs - val_drugs\n",
        "    print(f\"Drugs only in Test: {len(test_only)}\")\n",
        "\n",
        "    # Drugs in all three splits\n",
        "    all_three = train_drugs & val_drugs & test_drugs\n",
        "    print(f\"Drugs in ALL three splits: {len(all_three)}\")\n",
        "\n",
        "    # Drugs in training + validation only\n",
        "    train_val_only = (train_drugs & val_drugs) - test_drugs\n",
        "    print(f\"Drugs in Training + Validation only: {len(train_val_only)}\")\n",
        "\n",
        "    # Drugs in training + test only\n",
        "    train_test_only = (train_drugs & test_drugs) - val_drugs\n",
        "    print(f\"Drugs in Training + Test only: {len(train_test_only)}\")\n",
        "\n",
        "    # Drugs in validation + test only\n",
        "    val_test_only = (val_drugs & test_drugs) - train_drugs\n",
        "    print(f\"Drugs in Validation + Test only: {len(val_test_only)}\")\n",
        "\n",
        "    # Verification\n",
        "    total_check = (len(train_only) + len(val_only) + len(test_only) +\n",
        "                   len(all_three) + len(train_val_only) + len(train_test_only) + len(val_test_only))\n",
        "\n",
        "    print(f\"\\n VERIFICATION:\")\n",
        "    print(f\"Sum of all categories: {total_check}\")\n",
        "    print(f\"Total unique drugs: {len(all_drugs)}\")\n",
        "    print(f\"Match: {' YES' if total_check == len(all_drugs) else ' NO'}\")\n",
        "\n",
        "    # Show some examples if there are drugs missing from training\n",
        "    if train_only or val_only or test_only:\n",
        "        print(f\"\\n  WARNING: Some drugs don't appear in all splits!\")\n",
        "        print(\"This might affect negative sampling if we only use drugs from individual splits.\")\n",
        "\n",
        "        if val_only:\n",
        "            print(f\"\\nExample drugs only in Validation: {list(val_only)[:5]}\")\n",
        "        if test_only:\n",
        "            print(f\"Example drugs only in Test: {list(test_only)[:5]}\")\n",
        "        if train_only:\n",
        "            print(f\"Example drugs only in Training: {list(train_only)[:5]}\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'all_drugs': all_drugs,\n",
        "        'train_drugs': train_drugs,\n",
        "        'val_drugs': val_drugs,\n",
        "        'test_drugs': test_drugs,\n",
        "        'total_unique_drugs': len(all_drugs)\n",
        "    }\n",
        "\n",
        "train_path = '/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed42/train_accurate_redistribution.csv'\n",
        "val_path = '/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed42/val_accurate_redistribution.csv'\n",
        "test_path = '/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed42/test_accurate_redistribution.csv'\n",
        "\n",
        "# Run analysis\n",
        "drug_analysis = analyze_drugs_across_splits(train_path, val_path, test_path)\n",
        "\n",
        "print(f\"\\n FINAL ANSWER: Your dataset contains {drug_analysis['total_unique_drugs']} unique drugs total.\")\n",
        "print(\"If this doesn't match your expected 1,694, we need to investigate further!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Swle7nW_IeS"
      },
      "source": [
        "# **Partition metabolic data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4DM-lkz8TBc"
      },
      "source": [
        "\n",
        "\n",
        "### Metabolic Network Data Partitioning Overview\n",
        "\n",
        "This section describes the workflow used to partition interaction data for the metabolic neural network. The partitioning is performed **by interaction type** to maintain consistent and balanced splits.\n",
        "\n",
        "1. **Load Previous Partitions**\n",
        "   We begin by loading the existing training, validation, and testing partitions.\n",
        "\n",
        "2. **Evaluate Partition Balance**\n",
        "   We run the `analyze_partition_balance` function to inspect how each interaction type is distributed across the three splits. This step highlights any interaction types that are poorly balanced or incorrectly assigned.\n",
        "\n",
        "3. **Extract Problematic Types**\n",
        "   Using the `data-partition-metabolic` function, we isolate only the problematic interaction types from the current partitions. All correctly distributed interactions remain unchanged.\n",
        "\n",
        "4. **Repartition Problematic Interactions**\n",
        "   The extracted types are repartitioned to achieve the correct distribution across train/validation/test.\n",
        "\n",
        "5. **Re-verify Partitioning**\n",
        "   We rerun the analysis to confirm that the updated partitioning is correct.\n",
        "   This workflow is repeated for all remaining random seeds(42,32).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0qG7trg-495"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6wIf82a7euH"
      },
      "source": [
        "## Data partation in correct\n",
        "\n",
        "**fun data-partation-metabolic ðŸ‘‡**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INXLTDgds9u3",
        "outputId": "29b675de-de51-4d64-f8a7-370332767ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SIMPLE LABEL REDISTRIBUTION\n",
            "============================================================\n",
            "\n",
            "Problematic labels to fix: [1, 26, 41, 42, 44, 52, 86, 79]\n",
            "\n",
            "Reading files...\n",
            "Train: 153501, Val: 19187, Test: 19189\n",
            "\n",
            "Extracting problematic rows...\n",
            "From Train: 100 rows\n",
            "From Val: 9 rows\n",
            "From Test: 6 rows\n",
            "\n",
            "Total problematic rows to redistribute: 115\n",
            "\n",
            "Redistributing: 80% train, 10% val, 10% test...\n",
            "Minimum samples per label: 6\n",
            "Using manual split (labels have too few samples for stratification)...\n",
            "New distribution: Train=88, Val=10, Test=17\n",
            "\n",
            "Final sizes: Train=153489, Val=19188, Test=19200\n",
            "\n",
            "New distribution of problematic labels:\n",
            "Label      Train      Val        Test      \n",
            "----------------------------------------\n",
            "1          8          1          2         \n",
            "26         5          1          1         \n",
            "41         11         1          2         \n",
            "42         4          1          1         \n",
            "44         10         1          2         \n",
            "52         8          1          1         \n",
            "86         21         2          4         \n",
            "79         21         2          4         \n",
            "\n",
            "âœ“ Files saved to: /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/Metabolic-Seed42/fixed/\n",
            "  - train_fixed.csv\n",
            "  - val_fixed.csv\n",
            "  - test_fixed.csv\n",
            "\n",
            "âœ“ Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "base_path = '/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/Metabolic-Seed42/'\n",
        "train_path = base_path + 'train_metabolic.csv'\n",
        "val_path = base_path + 'val_metabolic.csv'\n",
        "test_path = base_path + 'test_metabolic.csv'\n",
        "\n",
        "# PROBLEMATIC LABELS\n",
        "problematic_labels = [1,26,41,42,44,52,86,79]  # All problematic labels that get from (analyze_partition_balance)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SIMPLE LABEL REDISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nProblematic labels to fix: {problematic_labels}\")\n",
        "\n",
        "# Read files\n",
        "print(\"\\nReading files...\")\n",
        "train_df = pd.read_csv(train_path)\n",
        "val_df = pd.read_csv(val_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "# Extract problematic rows from all files\n",
        "print(\"\\nExtracting problematic rows...\")\n",
        "train_prob = train_df[train_df['Label'].isin(problematic_labels)]\n",
        "val_prob = val_df[val_df['Label'].isin(problematic_labels)]\n",
        "test_prob = test_df[test_df['Label'].isin(problematic_labels)]\n",
        "\n",
        "print(f\"From Train: {len(train_prob)} rows\")\n",
        "print(f\"From Val: {len(val_prob)} rows\")\n",
        "print(f\"From Test: {len(test_prob)} rows\")\n",
        "\n",
        "# Keep non-problematic rows unchanged\n",
        "train_keep = train_df[~train_df['Label'].isin(problematic_labels)]\n",
        "val_keep = val_df[~val_df['Label'].isin(problematic_labels)]\n",
        "test_keep = test_df[~test_df['Label'].isin(problematic_labels)]\n",
        "\n",
        "# Combine all problematic rows\n",
        "combined_prob = pd.concat([train_prob, val_prob, test_prob], ignore_index=True)\n",
        "print(f\"\\nTotal problematic rows to redistribute: {len(combined_prob)}\")\n",
        "\n",
        "# Split: 80% train, 10% val, 10% test\n",
        "print(\"\\nRedistributing: 80% train, 10% val, 10% test...\")\n",
        "\n",
        "# Check if we can use stratification\n",
        "label_counts = combined_prob['Label'].value_counts()\n",
        "min_count = label_counts.min()\n",
        "\n",
        "print(f\"Minimum samples per label: {min_count}\")\n",
        "\n",
        "if min_count >= 10:\n",
        "    # Enough samples for stratified split\n",
        "    print(\"Using stratified split...\")\n",
        "    train_new, temp = train_test_split(\n",
        "        combined_prob,\n",
        "        test_size=0.2, # Keep 20% for val+test\n",
        "        stratify=combined_prob['Label'],\n",
        "        random_state=32\n",
        "    )\n",
        "\n",
        "    val_new, test_new = train_test_split(\n",
        "        temp,\n",
        "        test_size=0.5,  # Split temp 50/50\n",
        "        stratify=temp['Label'],\n",
        "        random_state=32\n",
        "    )\n",
        "else:\n",
        "    # Manual split for small labels\n",
        "    print(\"Using manual split (labels have too few samples for stratification)...\")\n",
        "    train_new = []\n",
        "    val_new = []\n",
        "    test_new = []\n",
        "\n",
        "    for label in problematic_labels:\n",
        "        label_data = combined_prob[combined_prob['Label'] == label].copy()\n",
        "        n = len(label_data)\n",
        "\n",
        "        # Shuffle\n",
        "        label_data = label_data.sample(frac=1, random_state=32).reset_index(drop=True)\n",
        "\n",
        "        # Calculate splits: 80% train, 10% val, 10% test\n",
        "        n_train = max(1, int(n * 0.8))\n",
        "        n_val = max(1, int(n * 0.1))\n",
        "        n_test = n - n_train - n_val\n",
        "\n",
        "        # Ensure at least 1 in test if there are samples left\n",
        "        if n_test < 1 and n > n_train + n_val:\n",
        "            n_test = 1\n",
        "            n_val = n - n_train - n_test\n",
        "\n",
        "        train_new.append(label_data.iloc[:n_train])\n",
        "        val_new.append(label_data.iloc[n_train:n_train+n_val])\n",
        "        test_new.append(label_data.iloc[n_train+n_val:])\n",
        "\n",
        "    train_new = pd.concat(train_new, ignore_index=True)\n",
        "    val_new = pd.concat(val_new, ignore_index=True)\n",
        "    test_new = pd.concat(test_new, ignore_index=True)\n",
        "\n",
        "print(f\"New distribution: Train={len(train_new)}, Val={len(val_new)}, Test={len(test_new)}\")\n",
        "\n",
        "# Combine with kept rows\n",
        "train_final = pd.concat([train_keep, train_new], ignore_index=True)\n",
        "val_final = pd.concat([val_keep, val_new], ignore_index=True)\n",
        "test_final = pd.concat([test_keep, test_new], ignore_index=True)\n",
        "\n",
        "print(f\"\\nFinal sizes: Train={len(train_final)}, Val={len(val_final)}, Test={len(test_final)}\")\n",
        "\n",
        "# Show distribution of fixed labels\n",
        "print(\"\\nNew distribution of problematic labels:\")\n",
        "print(f\"{'Label':<10} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
        "print(\"-\"*40)\n",
        "for label in problematic_labels:\n",
        "    t = len(train_final[train_final['Label'] == label])\n",
        "    v = len(val_final[val_final['Label'] == label])\n",
        "    te = len(test_final[test_final['Label'] == label])\n",
        "    print(f\"{label:<10} {t:<10} {v:<10} {te:<10}\")\n",
        "\n",
        "# Save files\n",
        "import os\n",
        "output_path = base_path + 'fixed/'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "train_final.to_csv(output_path + 'train_fixed.csv', index=False)\n",
        "val_final.to_csv(output_path + 'val_fixed.csv', index=False)\n",
        "test_final.to_csv(output_path + 'test_fixed.csv', index=False)\n",
        "\n",
        "print(f\"\\nâœ“ Files saved to: {output_path}\")\n",
        "print(\"  - train_fixed.csv\")\n",
        "print(\"  - val_fixed.csv\")\n",
        "print(\"  - test_fixed.csv\")\n",
        "print(\"\\nâœ“ Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQfois3Iqmx1",
        "outputId": "4020a6e5-6de2-4763-9a35-26fbcec99929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PARTITION BALANCE VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "SEED 32 ANALYSIS\n",
            "--------------------------------------------------\n",
            "Train samples: 153,489\n",
            "Val samples: 19,188\n",
            "Test samples: 19,200\n",
            "\n",
            "DISTRIBUTION SUMMARY:\n",
            "Total interaction types: 86\n",
            "Types missing from train: 0\n",
            "Types missing from val: 0\n",
            "Types missing from test: 0\n",
            "\n",
            "PROBLEMATIC DISTRIBUTIONS (1 types):\n",
            "Type | Train% | Val% | Test% | Train_Count | Val_Count | Test_Count\n",
            "----------------------------------------------------------------------\n",
            "42 | 66.7% | 16.7% | 16.7% | 4 | 1 | 1\n",
            "\n",
            "TOP 10 MOST COMMON INTERACTION TYPES:\n",
            "Type | Total_Count | Train | Val | Test | Train%\n",
            "-------------------------------------------------------\n",
            "49 | 60971 | 48746 | 6126 | 6099 | 79.9%\n",
            "47 | 34359 | 27490 | 3464 | 3405 | 80.0%\n",
            "73 | 23540 | 18789 | 2345 | 2406 | 79.8%\n",
            "75 | 9393 | 7439 | 993 | 961 | 79.2%\n",
            "60 | 8393 | 6728 | 834 | 831 | 80.2%\n",
            "70 | 7787 | 6217 | 762 | 808 | 79.8%\n",
            "20 | 6140 | 4927 | 617 | 596 | 80.2%\n",
            "16 | 5435 | 4384 | 537 | 514 | 80.7%\n",
            "4 | 5011 | 3996 | 511 | 504 | 79.7%\n",
            "6 | 3160 | 2572 | 292 | 296 | 81.4%\n",
            "\n",
            "BOTTOM 10 LEAST COMMON INTERACTION TYPES:\n",
            "Type | Total_Count | Train | Val | Test | Train%\n",
            "-------------------------------------------------------\n",
            "42 | 6 | 4 | 1 | 1 | 66.7%\n",
            "26 | 7 | 5 | 1 | 1 | 71.4%\n",
            "52 | 10 | 8 | 1 | 1 | 80.0%\n",
            "1 | 11 | 8 | 1 | 2 | 72.7%\n",
            "28 | 11 | 9 | 1 | 1 | 81.8%\n",
            "43 | 11 | 9 | 1 | 1 | 81.8%\n",
            "62 | 11 | 9 | 1 | 1 | 81.8%\n",
            "44 | 13 | 10 | 1 | 2 | 76.9%\n",
            "31 | 14 | 12 | 1 | 1 | 85.7%\n",
            "41 | 14 | 11 | 1 | 2 | 78.6%\n",
            "\n",
            "Detailed analysis saved to: /content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/Metabolic-Seed42/fixed/balance_analysis.csv\n",
            "\n",
            "======================================================================\n",
            "CROSS-SEED COMPARISON\n",
            "======================================================================\n",
            "\n",
            "ANALYSIS COMPLETE!\n"
          ]
        }
      ],
      "source": [
        "def analyze_partition_balance(partition_base_path, seeds=[32, 42]):\n",
        "\n",
        "    print(\"PARTITION BALANCE VERIFICATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"\\nSEED {seed} ANALYSIS\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Define file paths\n",
        "        seed_dir = '/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/Metabolic-Seed42/fixed/'\n",
        "        train_path = os.path.join(seed_dir, 'train_fixed.csv')\n",
        "        val_path = os.path.join(seed_dir, 'val_fixed.csv')\n",
        "        test_path = os.path.join(seed_dir, 'test_fixed.csv')\n",
        "\n",
        "        # Load data\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        val_df = pd.read_csv(val_path)\n",
        "        test_df = pd.read_csv(test_path)\n",
        "\n",
        "        # Convert Label to integer to avoid format issues\n",
        "        train_df['Label'] = train_df['Label'].astype(int)\n",
        "        val_df['Label'] = val_df['Label'].astype(int)\n",
        "        test_df['Label'] = test_df['Label'].astype(int)\n",
        "\n",
        "        print(f\"Train samples: {len(train_df):,}\")\n",
        "        print(f\"Val samples: {len(val_df):,}\")\n",
        "        print(f\"Test samples: {len(test_df):,}\")\n",
        "\n",
        "        # Count interaction types in each split\n",
        "        train_counts = train_df['Label'].value_counts().sort_index()\n",
        "        val_counts = val_df['Label'].value_counts().sort_index()\n",
        "        test_counts = test_df['Label'].value_counts().sort_index()\n",
        "\n",
        "        # Create comprehensive analysis\n",
        "        all_labels = sorted(set(train_df['Label'].unique()) |\n",
        "                           set(val_df['Label'].unique()) |\n",
        "                           set(test_df['Label'].unique()))\n",
        "\n",
        "        analysis_data = []\n",
        "\n",
        "        for label in all_labels:\n",
        "            train_count = int(train_counts.get(label, 0))\n",
        "            val_count = int(val_counts.get(label, 0))\n",
        "            test_count = int(test_counts.get(label, 0))\n",
        "            total_count = train_count + val_count + test_count\n",
        "\n",
        "            # Calculate percentages\n",
        "            train_pct = (train_count / total_count * 100) if total_count > 0 else 0\n",
        "            val_pct = (val_count / total_count * 100) if total_count > 0 else 0\n",
        "            test_pct = (test_count / total_count * 100) if total_count > 0 else 0\n",
        "\n",
        "            analysis_data.append({\n",
        "                'Interaction_Type': int(label),\n",
        "                'Train_Count': train_count,\n",
        "                'Val_Count': val_count,\n",
        "                'Test_Count': test_count,\n",
        "                'Total_Count': total_count,\n",
        "                'Train_Pct': round(train_pct, 1),\n",
        "                'Val_Pct': round(val_pct, 1),\n",
        "                'Test_Pct': round(test_pct, 1)\n",
        "            })\n",
        "\n",
        "        # Convert to DataFrame for analysis\n",
        "        analysis_df = pd.DataFrame(analysis_data)\n",
        "        all_results[seed] = analysis_df\n",
        "\n",
        "        # Summary statistics\n",
        "        print(f\"\\nDISTRIBUTION SUMMARY:\")\n",
        "        print(f\"Total interaction types: {len(all_labels)}\")\n",
        "        print(f\"Types missing from train: {len([l for l in all_labels if train_counts.get(l, 0) == 0])}\")\n",
        "        print(f\"Types missing from val: {len([l for l in all_labels if val_counts.get(l, 0) == 0])}\")\n",
        "        print(f\"Types missing from test: {len([l for l in all_labels if test_counts.get(l, 0) == 0])}\")\n",
        "\n",
        "        # Identify problematic distributions\n",
        "        problematic = analysis_df[\n",
        "            (analysis_df['Train_Pct'] < 70) |\n",
        "            (analysis_df['Train_Pct'] > 90) |\n",
        "            (analysis_df['Val_Count'] == 0) |\n",
        "            (analysis_df['Test_Count'] == 0)\n",
        "        ]\n",
        "\n",
        "        if len(problematic) > 0:\n",
        "            print(f\"\\nPROBLEMATIC DISTRIBUTIONS ({len(problematic)} types):\")\n",
        "            print(\"Type | Train% | Val% | Test% | Train_Count | Val_Count | Test_Count\")\n",
        "            print(\"-\" * 70)\n",
        "            for _, row in problematic.iterrows():\n",
        "                print(f\"{int(row['Interaction_Type'])} | {row['Train_Pct']}% | {row['Val_Pct']}% | {row['Test_Pct']}% | {int(row['Train_Count'])} | {int(row['Val_Count'])} | {int(row['Test_Count'])}\")\n",
        "        else:\n",
        "            print(\"\\nNo problematic distributions detected!\")\n",
        "\n",
        "        # Top 10 most common types\n",
        "        top_10 = analysis_df.nlargest(10, 'Total_Count')\n",
        "        print(f\"\\nTOP 10 MOST COMMON INTERACTION TYPES:\")\n",
        "        print(\"Type | Total_Count | Train | Val | Test | Train%\")\n",
        "        print(\"-\" * 55)\n",
        "        for _, row in top_10.iterrows():\n",
        "            print(f\"{int(row['Interaction_Type'])} | {int(row['Total_Count'])} | {int(row['Train_Count'])} | {int(row['Val_Count'])} | {int(row['Test_Count'])} | {row['Train_Pct']}%\")\n",
        "\n",
        "        # Bottom 10 least common types\n",
        "        bottom_10 = analysis_df.nsmallest(10, 'Total_Count')\n",
        "        print(f\"\\nBOTTOM 10 LEAST COMMON INTERACTION TYPES:\")\n",
        "        print(\"Type | Total_Count | Train | Val | Test | Train%\")\n",
        "        print(\"-\" * 55)\n",
        "        for _, row in bottom_10.iterrows():\n",
        "            print(f\"{int(row['Interaction_Type'])} | {int(row['Total_Count'])} | {int(row['Train_Count'])} | {int(row['Val_Count'])} | {int(row['Test_Count'])} | {row['Train_Pct']}%\")\n",
        "\n",
        "        # Save detailed report\n",
        "        output_path = os.path.join(seed_dir, 'balance_analysis.csv')\n",
        "        analysis_df.to_csv(output_path, index=False)\n",
        "        print(f\"\\nDetailed analysis saved to: {output_path}\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"CROSS-SEED COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Compare consistency across seeds\n",
        "    if len(all_results) > 1:\n",
        "        # Check if the same types are problematic across seeds\n",
        "        problematic_types = set()\n",
        "        for seed, df in all_results.items():\n",
        "            problematic = df[\n",
        "                (df['Train_Pct'] < 70) |\n",
        "                (df['Train_Pct'] > 90) |\n",
        "                (df['Val_Count'] == 0) |\n",
        "                (df['Test_Count'] == 0)\n",
        "            ]\n",
        "            problematic_types.update(problematic['Interaction_Type'].tolist())\n",
        "\n",
        "        if problematic_types:\n",
        "            print(f\"Interaction types with issues across any seed: {sorted(list(problematic_types))}\")\n",
        "            print(\"These types may need special attention during training (class weighting, etc.)\")\n",
        "        else:\n",
        "            print(\"No consistent distribution problems across seeds!\")\n",
        "\n",
        "        # Calculate coefficient of variation for train percentages across seeds\n",
        "        train_pct_variations = {}\n",
        "        for interaction_type in all_labels:\n",
        "            train_pcts = []\n",
        "            for seed, df in all_results.items():\n",
        "                row = df[df['Interaction_Type'] == interaction_type]\n",
        "                if len(row) > 0:\n",
        "                    train_pcts.append(row['Train_Pct'].iloc[0])\n",
        "\n",
        "            if len(train_pcts) > 1:\n",
        "                cv = np.std(train_pcts) / np.mean(train_pcts) if np.mean(train_pcts) > 0 else 0\n",
        "                train_pct_variations[interaction_type] = cv\n",
        "\n",
        "        # Show types with high variation across seeds\n",
        "        high_variation = {k: v for k, v in train_pct_variations.items() if v > 0.1}\n",
        "        if high_variation:\n",
        "            print(f\"\\nTypes with high train% variation across seeds:\")\n",
        "            for interaction_type, cv in sorted(high_variation.items(), key=lambda x: x[1], reverse=True):\n",
        "                print(f\"  Type {interaction_type}: CV = {cv:.3f}\")\n",
        "        else:\n",
        "            print(\"\\nConsistent train% distributions across all seeds!\")\n",
        "\n",
        "    print(\"\\nANALYSIS COMPLETE!\")\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# Usage\n",
        "partition_base_path = '/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/'\n",
        "seeds = [32]\n",
        "\n",
        "results = analyze_partition_balance(partition_base_path, seeds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

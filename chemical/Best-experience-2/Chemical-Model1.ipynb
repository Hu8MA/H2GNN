{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go6GIcRIhVB7",
        "outputId": "ec64549a-644a-4713-e019-c20b16984cae"
      },
      "outputs": [],
      "source": [
        "# ================= CELL 1: INSTALLATION =================\n",
        "!pip install torch torchvision torchaudio --upgrade\n",
        "!pip install transformers datasets scikit-learn\n",
        "!pip uninstall dgl -y -q\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/repo.html\n",
        "!pip install torchdata==0.7.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i52EdkCeImt1",
        "outputId": "afc11c35-8e7e-4f15-9869-d2cdfe4aae12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGL version: 2.1.0\n",
            "PyTorch version: 2.9.0+cu128\n",
            "Hypergraph creation successful!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set DGL backend before importing\n",
        "os.environ['DGLBACKEND'] = 'pytorch'\n",
        "\n",
        "# Mock GraphBolt to prevent loading issues\n",
        "class MockModule:\n",
        "    def __getattr__(self, name):\n",
        "        return lambda *args, **kwargs: None\n",
        "\n",
        "sys.modules['dgl.graphbolt'] = MockModule()\n",
        "\n",
        "# Import DGL\n",
        "import dgl\n",
        "\n",
        "# Import other libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, accuracy_score, average_precision_score, precision_recall_curve, auc\n",
        "import dgl.function as fn\n",
        "\n",
        "# Test everything\n",
        "print(f\"DGL version: {dgl.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Test creating a hypergraph\n",
        "try:\n",
        "    data_dict = {\n",
        "        ('node', 'in', 'edge'): ([0, 1], [0, 0]),\n",
        "        ('edge', 'con', 'node'): ([0, 0], [0, 1])\n",
        "    }\n",
        "    test_hyG = dgl.heterograph(data_dict)\n",
        "    print(\"Hypergraph creation successful!\")\n",
        "except Exception as e:\n",
        "    print(\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8H3f1T_HP0l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc\n",
        "import dgl.function as fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0CyCDy2HcbF",
        "outputId": "ad0f2a08-6b35-43b1-a857-f40428f88f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')# Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlOUy8uWViIy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h # here h is drug features and g is the pos/neg train/test graph\n",
        "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
        "            return g.edata['score'][:, 0]\n",
        "\n",
        "\n",
        "#We use this only\n",
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
        "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h  # Assign embeddings to nodes\n",
        "            g.apply_edges(self.apply_edges)  # Compute scores for all edges\n",
        "            return g.edata['score'] # Return edge scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5hHBwqwVkyH"
      },
      "outputs": [],
      "source": [
        "#binary cross-entropy loss\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "\n",
        "def compute_auc(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
        "    precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
        "    auc_precision_recall = auc(recall, precision)\n",
        "    return roc_auc_score(labels, scores),auc(recall, precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg98AoQcVnVK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class HyGNN(nn.Module):\n",
        "    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, dropout):\n",
        "        super(HyGNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.query_dim = query_dim\n",
        "        self.in_first_layer = torch.nn.Linear(input_dim, vertex_dim)\n",
        "        self.not_in_first_layer = torch.nn.Linear(vertex_dim, vertex_dim)\n",
        "        # Attention mechanism weights (Key, Query, Value transformations)\n",
        "        self.w6 = torch.nn.Linear(edge_dim, query_dim)   # Edge â†’ Query\n",
        "        self.w5 = torch.nn.Linear(vertex_dim, query_dim)  # Node â†’ Key\n",
        "        self.w4 = torch.nn.Linear(vertex_dim, edge_dim)  # Node â†’ Value (for edges)\n",
        "\n",
        "        self.w3 = torch.nn.Linear(vertex_dim, query_dim) # Node â†’ Query\n",
        "        self.w2 = torch.nn.Linear(edge_dim, query_dim)  # Edge â†’ Key\n",
        "        self.w1 = torch.nn.Linear(edge_dim, vertex_dim) # Edge â†’ Value (for nodes)\n",
        "\n",
        "    def red_function(self, nodes):\n",
        "        attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n",
        "        aggregated = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n",
        "        return {'h': aggregated}\n",
        "\n",
        "    def attention(self, edges):\n",
        "        attn_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n",
        "        return {'Attn': attn_score/np.sqrt(self.query_dim)}\n",
        "\n",
        "    def msg_fucntion(self, edges):\n",
        "        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, hyG, vfeat, efeat, first_layer, last_layer):\n",
        "            if first_layer:\n",
        "                feat_e = self.in_first_layer(efeat)\n",
        "            else:\n",
        "                feat_e = self.not_in_first_layer(efeat)\n",
        "            feat_v = vfeat\n",
        "            #Hyperedge-level attention\n",
        "            hyG.ndata['h'] = {'edge': feat_e}\n",
        "            hyG.ndata['k'] = {'edge' : self.w2(feat_e)} # Keys from drugs\n",
        "            hyG.ndata['v'] = {'edge' : self.w1(feat_e)} # Values from drugs\n",
        "            hyG.ndata['q'] = {'node' : self.w3(feat_v)} # Queries from substructures\n",
        "            hyG.apply_edges(self.attention, etype='con') #Computes attention scores between each drug (src) and its substructures (dst)\n",
        "            hyG.update_all(self.msg_fucntion, self.red_function, etype='con')  # drug -> node\n",
        "\n",
        "            #Node-level attention\n",
        "            feat_v = hyG.ndata['h']['node']  # Updated node features\n",
        "            hyG.ndata['k'] = {'node' : self.w5(feat_v)}  # Keys from substructures\n",
        "            hyG.ndata['v'] = {'node' : self.w4(feat_v)}  # Values from substructures\n",
        "            hyG.ndata['q'] = {'edge' : self.w6(feat_e)}  # Queries from drugs\n",
        "            hyG.apply_edges(self.attention, etype='in')\n",
        "            hyG.update_all(self.msg_fucntion, self.red_function, etype='in')\n",
        "            feat_e = hyG.ndata['h']['edge'] # GET the updated drug features!  node->drug\n",
        "\n",
        "            if not last_layer :\n",
        "                feat_v = F.dropout(feat_v, self.dropout)\n",
        "            if last_layer:\n",
        "                return feat_v, feat_e\n",
        "            else:\n",
        "                return [hyG, feat_v, feat_e]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDG9OE1Y1-d2"
      },
      "outputs": [],
      "source": [
        "def load_data_and_create_graphs():\n",
        "    \"\"\"Load your actual data files and create the necessary graphs\"\"\"\n",
        "\n",
        "    # Load metadata to get dimensions\n",
        "    metadata = torch.load('/content/drive/MyDrive/MLHygnn/DB/hypergraphs/hyG_drug_drugbank_kmer_12_metadata.pt', weights_only=False)\n",
        "\n",
        "    # Extract dimensions from metadata\n",
        "    num_drugs = len(metadata['drug_to_idx'])\n",
        "    num_substructures = len(metadata['node_to_idx'])\n",
        "\n",
        "    print(f\"Number of drugs: {num_drugs}\")\n",
        "    print(f\"Number of substructures: {num_substructures}\")\n",
        "\n",
        "    # Load hypergraph data\n",
        "    chemicalsub_drug = torch.load('/content/drive/MyDrive/MLHygnn/DB/hypergraphs/hyG_drug_drugbank_kmer_12.pt', weights_only=False)\n",
        "\n",
        "    # Create hypergraph\n",
        "    data_dict = {\n",
        "        ('node', 'in', 'edge'): (chemicalsub_drug[:,0], chemicalsub_drug[:,1]),\n",
        "        ('edge', 'con', 'node'): (chemicalsub_drug[:,1], chemicalsub_drug[:,0])\n",
        "    }\n",
        "\n",
        "    hyG = dgl.heterograph(data_dict)\n",
        "    print(\"Hypergraph structure:\")\n",
        "    print(hyG)\n",
        "    print(\"=\" * 500)\n",
        "\n",
        "    # Create drug identity matrix (sparse)\n",
        "    from scipy.sparse import coo_matrix\n",
        "    nl = coo_matrix((num_drugs, num_drugs))\n",
        "    nl.setdiag(1)\n",
        "    values = nl.data # Array of non-zero values\n",
        "    indices = np.vstack((nl.row, nl.col))\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    shape = nl.shape\n",
        "    drug_X = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
        "\n",
        "    # Create node features\n",
        "    hyG.ndata['h'] = {'edge': torch.tensor(drug_X).type('torch.FloatTensor'), 'node': torch.ones(num_substructures, 128)}\n",
        "    e_feat = torch.tensor(drug_X).type('torch.FloatTensor')\n",
        "    v_feat = torch.ones(num_substructures, 128)\n",
        "\n",
        "    return hyG, v_feat, e_feat, drug_X, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgUB_-g9VsRs"
      },
      "outputs": [],
      "source": [
        "def load_train_test_data():\n",
        "    \"\"\"Load your CSV files and create DGL graphs for training\"\"\"\n",
        "\n",
        "    # Load positive samples\n",
        "    train_pos = pd.read_csv('/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed32/train.csv')\n",
        "    val_pos = pd.read_csv('/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed32/val.csv')\n",
        "    test_pos = pd.read_csv('/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed32/test.csv')\n",
        "\n",
        "    # Load negative samples\n",
        "    train_neg = pd.read_csv('/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed32/processed_with_negatives_fixed/train_negatives.csv')\n",
        "    val_neg = pd.read_csv('/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed32/processed_with_negatives_fixed/val_negatives.csv')\n",
        "    test_neg = pd.read_csv('/content/drive/MyDrive/MLHygnn/DB/Partition-Dataset/train-seed32/processed_with_negatives_fixed/test_negatives.csv')\n",
        "\n",
        "    # Get metadata and drug mapping\n",
        "    metadata = torch.load('/content/drive/MyDrive/MLHygnn/DB/hypergraphs/hyG_drug_drugbank_kmer_12_metadata.pt', weights_only=False)\n",
        "    num_drugs = len(metadata['drug_to_idx'])\n",
        "    drug_to_id_mapping = metadata['drug_to_idx']\n",
        "\n",
        "    def create_dgl_graph(df, num_nodes, drug_mapping):\n",
        "        \"\"\"Create DGL graph from dataframe with drug pairs\"\"\"\n",
        "        if 'Drug1_ID' in df.columns and 'Drug2_ID' in df.columns:\n",
        "            src_ids = df['Drug1_ID'].values\n",
        "            dst_ids = df['Drug2_ID'].values\n",
        "        else:\n",
        "            print(\"Available columns:\", df.columns.tolist())\n",
        "            src_ids = df.iloc[:, 0].values  # First column\n",
        "            dst_ids = df.iloc[:, 1].values  # Second column\n",
        "\n",
        "        # Convert DrugBank IDs to integer indices using the mapping\n",
        "        src = torch.tensor([drug_mapping[drug_id] for drug_id in src_ids], dtype=torch.long)\n",
        "        dst = torch.tensor([drug_mapping[drug_id] for drug_id in dst_ids], dtype=torch.long)\n",
        "\n",
        "        return dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "\n",
        "    # Create DGL graphs\n",
        "    train_pos_g = create_dgl_graph(train_pos, num_drugs, drug_to_id_mapping)\n",
        "    val_pos_g = create_dgl_graph(val_pos, num_drugs, drug_to_id_mapping)\n",
        "    test_pos_g = create_dgl_graph(test_pos, num_drugs, drug_to_id_mapping)\n",
        "\n",
        "    train_neg_g = create_dgl_graph(train_neg, num_drugs, drug_to_id_mapping)\n",
        "    val_neg_g = create_dgl_graph(val_neg, num_drugs, drug_to_id_mapping)\n",
        "    test_neg_g = create_dgl_graph(test_neg, num_drugs, drug_to_id_mapping)\n",
        "\n",
        "    print(f\"Train positive edges: {train_pos_g.number_of_edges()}\")\n",
        "    print(f\"Train negative edges: {train_neg_g.number_of_edges()}\")\n",
        "    print(f\"Validation positive edges: {val_pos_g.number_of_edges()}\")\n",
        "    print(f\"Validation negative edges: {val_neg_g.number_of_edges()}\")\n",
        "    print(f\"Test positive edges: {test_pos_g.number_of_edges()}\")\n",
        "    print(f\"Test negative edges: {test_neg_g.number_of_edges()}\")\n",
        "\n",
        "    return train_pos_g, train_neg_g, val_pos_g, val_neg_g, test_pos_g, test_neg_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqeNL8IkVvXV"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, drug_feature_dim, config):\n",
        "        super(Model, self).__init__()\n",
        "        self.gat1 = HyGNN(\n",
        "            drug_feature_dim,\n",
        "            config['hidden_units'],     # query_dim\n",
        "            config['hidden_units'],     # vertex_dim\n",
        "            config['hidden_units'],     # edge_dim\n",
        "            config['dropout']           # dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, hyG, v_feat, e_feat, f, l):\n",
        "        h = self.gat1(hyG, v_feat, e_feat, f, l)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O57hLPPVnHb"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import os\n",
        "import time\n",
        "def calculate_ram_usage():\n",
        "    \"\"\"Calculate current RAM usage in GB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    ram_gb = process.memory_info().rss / (1024 ** 3)  # Convert to GB\n",
        "    return ram_gb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r18d72rV6ts",
        "outputId": "7d798b78-f04f-40ff-d02f-80d19f12dd70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: fixed_conservative\n",
            "Config: {'learning_rate': 0.005, 'hidden_units': 128, 'dropout': 0.5, 'weight_decay': 0.0, 'training_seed': 42, 'experiment_name': 'fixed_conservative'}\n",
            "Directory: /content/drive/MyDrive/MLHygnn/DB/Stage1_Chemical_NetworkModel1-ForTime/fixed_conservative/\n",
            "Number of drugs: 1709\n",
            "Number of substructures: 43655\n",
            "Hypergraph structure:\n",
            "Graph(num_nodes={'edge': 1709, 'node': 43655},\n",
            "      num_edges={('edge', 'con', 'node'): 91615, ('node', 'in', 'edge'): 91615},\n",
            "      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])\n",
            "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "Train positive edges: 153501\n",
            "Train negative edges: 153501\n",
            "Validation positive edges: 19187\n",
            "Validation negative edges: 19187\n",
            "Test positive edges: 19189\n",
            "Test negative edges: 19189\n",
            "ðŸ“Š RAM usage before training: 1.42 GB\n",
            "\n",
            "Starting Stage 2 (Metabolic Network) training...\n",
            "Start time: 2025-10-21 15:25:18\n",
            "Epoch 0, train loss: 0.6939, val loss: 0.6947 (best: 0.6947, patience: 0)\n",
            "Epoch 10, train loss: 0.4350, val loss: 0.4623 (best: 0.4623, patience: 0)\n",
            "Epoch 20, train loss: 0.3542, val loss: 0.3962 (best: 0.3850, patience: 3)\n",
            "Epoch 30, train loss: 0.3130, val loss: 0.3584 (best: 0.3584, patience: 0)\n",
            "Epoch 40, train loss: 0.3037, val loss: 0.3518 (best: 0.3518, patience: 0)\n",
            "Epoch 50, train loss: 0.2963, val loss: 0.3455 (best: 0.3450, patience: 2)\n",
            "Epoch 60, train loss: 0.2902, val loss: 0.3381 (best: 0.3381, patience: 0)\n",
            "Epoch 70, train loss: 0.2807, val loss: 0.3293 (best: 0.3293, patience: 0)\n",
            "Epoch 80, train loss: 0.2682, val loss: 0.3168 (best: 0.3168, patience: 0)\n",
            "Epoch 90, train loss: 0.2598, val loss: 0.3097 (best: 0.3097, patience: 0)\n",
            "Epoch 100, train loss: 0.2502, val loss: 0.2983 (best: 0.2983, patience: 0)\n",
            "Epoch 110, train loss: 0.2399, val loss: 0.2861 (best: 0.2861, patience: 0)\n",
            "Epoch 120, train loss: 0.2323, val loss: 0.2797 (best: 0.2782, patience: 1)\n",
            "Epoch 130, train loss: 0.2242, val loss: 0.2697 (best: 0.2697, patience: 0)\n",
            "Epoch 140, train loss: 0.2180, val loss: 0.2615 (best: 0.2615, patience: 0)\n",
            "Epoch 150, train loss: 0.2060, val loss: 0.2546 (best: 0.2526, patience: 1)\n",
            "Epoch 160, train loss: 0.1961, val loss: 0.2417 (best: 0.2417, patience: 0)\n",
            "Epoch 170, train loss: 0.1887, val loss: 0.2373 (best: 0.2373, patience: 1)\n",
            "Epoch 180, train loss: 0.1837, val loss: 0.2316 (best: 0.2316, patience: 0)\n",
            "Epoch 190, train loss: 0.1862, val loss: 0.2273 (best: 0.2268, patience: 2)\n",
            "Epoch 200, train loss: 0.1715, val loss: 0.2240 (best: 0.2224, patience: 4)\n",
            "Epoch 210, train loss: 0.1669, val loss: 0.2186 (best: 0.2186, patience: 0)\n",
            "Epoch 220, train loss: 0.1701, val loss: 0.2149 (best: 0.2149, patience: 0)\n",
            "Epoch 230, train loss: 0.1580, val loss: 0.2110 (best: 0.2110, patience: 0)\n",
            "Epoch 240, train loss: 0.1603, val loss: 0.2079 (best: 0.2079, patience: 0)\n",
            "Epoch 250, train loss: 0.1531, val loss: 0.2055 (best: 0.2055, patience: 0)\n",
            "Epoch 260, train loss: 0.1487, val loss: 0.2022 (best: 0.2022, patience: 0)\n",
            "Epoch 270, train loss: 0.1446, val loss: 0.2017 (best: 0.2011, patience: 1)\n",
            "Epoch 280, train loss: 0.1438, val loss: 0.1998 (best: 0.1994, patience: 7)\n",
            "Epoch 290, train loss: 0.1497, val loss: 0.1980 (best: 0.1960, patience: 1)\n",
            "Epoch 300, train loss: 0.1385, val loss: 0.1951 (best: 0.1944, patience: 2)\n",
            "Epoch 310, train loss: 0.1342, val loss: 0.1942 (best: 0.1927, patience: 5)\n",
            "Epoch 320, train loss: 0.1311, val loss: 0.1924 (best: 0.1919, patience: 1)\n",
            "Epoch 330, train loss: 0.1286, val loss: 0.1916 (best: 0.1916, patience: 0)\n",
            "Epoch 340, train loss: 0.1262, val loss: 0.1909 (best: 0.1905, patience: 1)\n",
            "Epoch 350, train loss: 0.1515, val loss: 0.2026 (best: 0.1888, patience: 3)\n",
            "Epoch 360, train loss: 0.1403, val loss: 0.1882 (best: 0.1882, patience: 0)\n",
            "Epoch 370, train loss: 0.1283, val loss: 0.1863 (best: 0.1861, patience: 6)\n",
            "Epoch 380, train loss: 0.1210, val loss: 0.1838 (best: 0.1835, patience: 3)\n",
            "Epoch 390, train loss: 0.1180, val loss: 0.1835 (best: 0.1834, patience: 6)\n",
            "Epoch 400, train loss: 0.1151, val loss: 0.1838 (best: 0.1834, patience: 7)\n",
            "Epoch 410, train loss: 0.1129, val loss: 0.1832 (best: 0.1832, patience: 0)\n",
            "Epoch 420, train loss: 0.1108, val loss: 0.1830 (best: 0.1830, patience: 0)\n",
            "Epoch 430, train loss: 0.1094, val loss: 0.1817 (best: 0.1817, patience: 0)\n",
            "Epoch 440, train loss: 0.1114, val loss: 0.1882 (best: 0.1811, patience: 1)\n",
            "Epoch 450, train loss: 0.1082, val loss: 0.1795 (best: 0.1795, patience: 0)\n",
            "Epoch 460, train loss: 0.1046, val loss: 0.1778 (best: 0.1778, patience: 0)\n",
            "Epoch 470, train loss: 0.1021, val loss: 0.1788 (best: 0.1777, patience: 1)\n",
            "Epoch 480, train loss: 0.1000, val loss: 0.1786 (best: 0.1777, patience: 11)\n",
            "Epoch 490, train loss: 0.1089, val loss: 0.2031 (best: 0.1777, patience: 21)\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED\n",
            "================================================================================\n",
            "Best epoch: 469\n",
            "Best validation loss: 0.1777\n",
            "\n",
            "Timing Statistics:\n",
            "  Total training time: 7810.30 seconds\n",
            "  Total training time: 130.17 minutes\n",
            "  Total training time: 2.17 hours\n",
            "  Average time per epoch: 15.62 seconds\n",
            "RAM usage before training: 1.42 GB\n",
            "RAM usage after training:  1.77 GB\n",
            "RAM used during training:  0.35 GB\n",
            "  Epochs completed: 500\n",
            "  End time: 2025-10-21 17:35:29\n"
          ]
        }
      ],
      "source": [
        "EXPERIMENT_CONFIG = {\n",
        "    'learning_rate': 0.005,\n",
        "    'hidden_units': 128,\n",
        "    'dropout': 0.5,\n",
        "    'weight_decay': 0.0,\n",
        "    'training_seed': 42,\n",
        "    'experiment_name': 'fixed_conservative'\n",
        "}\n",
        "\n",
        "base_path = f'/content/drive/MyDrive/MLHygnn/DB/Stage1_Chemical_NetworkModel1-ForTime/{EXPERIMENT_CONFIG[\"experiment_name\"]}/'\n",
        "\n",
        "# Create experiment-specific directory\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "print(f\"Experiment: {EXPERIMENT_CONFIG['experiment_name']}\")\n",
        "print(f\"Config: {EXPERIMENT_CONFIG}\")\n",
        "print(f\"Directory: {base_path}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(EXPERIMENT_CONFIG['training_seed'])\n",
        "np.random.seed(EXPERIMENT_CONFIG['training_seed'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(EXPERIMENT_CONFIG['training_seed'])\n",
        "\n",
        "# Load data \n",
        "hyG, v_feat, e_feat, drug_X, metadata = load_data_and_create_graphs()\n",
        "train_pos_g, train_neg_g, val_pos_g, val_neg_g, test_pos_g, test_neg_g = load_train_test_data()\n",
        "\n",
        "# Create model and decoder\n",
        "model = Model(drug_X.shape[1], EXPERIMENT_CONFIG)\n",
        "decoder = MLPPredictor(EXPERIMENT_CONFIG['hidden_units'])\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    itertools.chain(model.parameters(), decoder.parameters()),\n",
        "    lr=EXPERIMENT_CONFIG['learning_rate']\n",
        "    \n",
        ")\n",
        "\n",
        "\n",
        "# Training variables\n",
        "best_val_loss = 1e10\n",
        "patience = 0\n",
        "best_embeddings = None\n",
        "best_epoch = 0\n",
        "\n",
        "training_start_time = time.time()\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"ðŸ“Š RAM usage before training: {ram_before:.2f} GB\")\n",
        "\n",
        "print(\"\\nStarting Stage 2 (Metabolic Network) training...\")\n",
        "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(training_start_time))}\")\n",
        "\n",
        "for e in range(500):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    decoder.train()\n",
        "    h = model(hyG, v_feat, e_feat, True, True)\n",
        "    h_drug = h[1]  # Get drug embeddings\n",
        "    pos_score = decoder(train_pos_g, h_drug)\n",
        "    neg_score = decoder(train_neg_g, h_drug)\n",
        "    loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "    # Simple backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        decoder.eval()\n",
        "       \n",
        "        pos_score_val = decoder(val_pos_g, h_drug)\n",
        "        neg_score_val = decoder(val_neg_g, h_drug)\n",
        "        val_loss = compute_loss(pos_score_val, neg_score_val)\n",
        "\n",
        "        # Simple model selection\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_embeddings = h_drug.clone()  # Save training embeddings\n",
        "            best_epoch = e\n",
        "            patience = 0\n",
        "\n",
        "            # Save models\n",
        "            torch.save(decoder.state_dict(), f'{base_path}decoder_best.pth')\n",
        "            torch.save(model.state_dict(), f'{base_path}model_best.pth')\n",
        "\n",
        "        else:\n",
        "            patience += 1\n",
        "\n",
        "       \n",
        "\n",
        "        # Simple early stopping\n",
        "        if patience > 200:\n",
        "            print(f\"Early stopping at epoch {e}\")\n",
        "            break\n",
        "\n",
        "\n",
        "    # Progress reporting\n",
        "    if e % 10 == 0:\n",
        "        print(f'Epoch {e}, train loss: {loss:.4f}, val loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience})')\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "total_training_time = training_end_time - training_start_time\n",
        "# Get RAM usage after training\n",
        "ram_after = calculate_ram_usage()\n",
        "ram_used = ram_after - ram_before\n",
        "\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"\\nTiming Statistics:\")\n",
        "print(f\"  Total training time: {total_training_time:.2f} seconds\")\n",
        "print(f\"  Total training time: {total_training_time/60:.2f} minutes\")\n",
        "print(f\"  Total training time: {total_training_time/3600:.2f} hours\")\n",
        "print(f\"  Average time per epoch: {total_training_time/(e+1):.2f} seconds\")\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"RAM usage after training:  {ram_after:.2f} GB\")\n",
        "print(f\"RAM used during training:  {ram_used:.2f} GB\")\n",
        "print(f\"  Epochs completed: {e+1}\")\n",
        "print(f\"  End time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(training_end_time))}\")\n",
        "\n",
        "# Use best embeddings for final evaluation\n",
        "H = best_embeddings\n",
        "E = best_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVvaE2GhYLkq",
        "outputId": "de4d306c-5f94-4f76-8cff-7b7232c75913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Epoch: 469, Accuracy: 0.9313, Precision: 0.9196, Recall: 0.9469, F1-score 0.9330, ROC-AUC 0.9842, PR-AUC 0.9841\n",
            "Final drug embeddings shape: torch.Size([1709, 128])\n",
            "These embeddings will be used as input to the second (metabolic) network\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluation\n",
        "decoder.load_state_dict(torch.load(f'{base_path}decoder_best.pth'))\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    pos_score = decoder(test_pos_g, H)\n",
        "    neg_score = decoder(test_neg_g, H)\n",
        "    test_acc = compute_auc(pos_score, neg_score)\n",
        "\n",
        "scores = torch.cat([pos_score, neg_score])\n",
        "labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "\n",
        "m1 = tf.keras.metrics.BinaryAccuracy()\n",
        "m1.update_state(labels, scores)\n",
        "\n",
        "#Compute Precision  = True Positives / (True Positives + False Positives)\n",
        "sig_scores = F.sigmoid(scores)\n",
        "m2 = tf.keras.metrics.Precision()\n",
        "m2.update_state(labels, sig_scores)\n",
        "M2 = m2.result().numpy()\n",
        "\n",
        "#Recall = True Positives / (True Positives + False Negatives)\n",
        "m3 = tf.keras.metrics.Recall()\n",
        "m3.update_state(labels, sig_scores)\n",
        "M3 = m3.result().numpy()\n",
        "\n",
        "#F1-Score = Harmonic mean of Precision and Recall\n",
        "F1 = 2*(M2*M3)/(M2+M3)\n",
        "print('Best Epoch: {}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score {:.4f}, ROC-AUC {:.4f}, PR-AUC {:.4f}'.format(\n",
        "    E, m1.result().numpy(), M2, M3, F1, test_acc[0], test_acc[1]))\n",
        "\n",
        "# Save the final drug embeddings for the second network\n",
        "print(f\"Final drug embeddings shape: {H.shape}\")\n",
        "print(\"These embeddings will be used as input to the second (metabolic) network\")\n",
        "\n",
        "# Save embeddings and metadata for second network\n",
        "torch.save({\n",
        "    'drug_embeddings': H,\n",
        "    'drug_to_id': metadata['drug_to_idx'],  # Changed key name\n",
        "    'best_epoch': E,\n",
        "    'final_performance': {\n",
        "        'accuracy': m1.result().numpy(),\n",
        "        'precision': M2,\n",
        "        'recall': M3,\n",
        "        'f1': F1,\n",
        "        'roc_auc': test_acc[0],\n",
        "        'pr_auc': test_acc[1]\n",
        "    }\n",
        "}, f'{base_path}chemical_network_output.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The data required by the final module is loaded for a comprehensive view of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s1Xz8Ypuli4",
        "outputId": "48b347df-9b34-46ec-b79c-3538dded5f92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of drugs: 1709\n",
            "Number of substructures: 43655\n",
            "Hypergraph structure:\n",
            "Graph(num_nodes={'edge': 1709, 'node': 43655},\n",
            "      num_edges={('edge', 'con', 'node'): 91615, ('node', 'in', 'edge'): 91615},\n",
            "      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])\n",
            "====================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "Train positive edges: 153501\n",
            "Train negative edges: 153501\n",
            "Validation positive edges: 19187\n",
            "Validation negative edges: 19187\n",
            "Test positive edges: 19189\n",
            "Test negative edges: 19189\n",
            "Loaded embeddings from epoch 469\n",
            "Embedding shape: torch.Size([1709, 128])\n",
            "================================================================================\n",
            "AGGREGATE PERFORMANCE METRICS\n",
            "================================================================================\n",
            "Best Epoch: 469\n",
            "Accuracy:   0.9313\n",
            "Precision:  0.9196\n",
            "Recall:     0.9469\n",
            "F1-Score:   0.9330\n",
            "ROC-AUC:    0.9842\n",
            "PR-AUC:     0.9841\n",
            "\n",
            "Confusion Matrix:\n",
            "                Predicted Negative    Predicted Positive\n",
            "Actual Negative         17600                1589\n",
            "Actual Positive          1019               18170\n",
            "\n",
            "================================================================================\n",
            "INDIVIDUAL PREDICTIONS\n",
            "================================================================================\n",
            "\n",
            "All predictions saved to: /content/drive/MyDrive/MLHygnn/DB/Model-K12/Stage1_Chemical_NetworkModel1/fixed_conservative/test_predictions_detailed.csv\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SAMPLE PREDICTIONS (First 20)\n",
            "--------------------------------------------------------------------------------\n",
            "Drug1_ID Drug2_ID  True_Label  Predicted_Label  Prediction_Score  Correct\n",
            " DB00421  DB01628         1.0                1          0.989217        1\n",
            " DB00196  DB01234         1.0                1          0.999948        1\n",
            " DB00391  DB00557         1.0                1          1.000000        1\n",
            " DB00907  DB06589         1.0                1          0.977875        1\n",
            " DB00860  DB01435         1.0                1          0.997825        1\n",
            " DB06781  DB06802         1.0                1          0.999972        1\n",
            " DB00661  DB00701         1.0                1          0.982648        1\n",
            " DB01095  DB09065         1.0                1          0.999562        1\n",
            " DB00275  DB00861         1.0                1          1.000000        1\n",
            " DB00477  DB01591         1.0                1          0.999947        1\n",
            " DB01501  DB01580         1.0                1          0.856672        1\n",
            " DB00273  DB00903         1.0                1          0.630787        1\n",
            " DB01588  DB09065         1.0                1          0.999797        1\n",
            " DB00575  DB08877         1.0                1          0.755939        1\n",
            " DB00203  DB00738         1.0                1          0.998726        1\n",
            " DB00415  DB00479         1.0                1          0.970921        1\n",
            " DB00890  DB01072         1.0                1          0.785448        1\n",
            " DB00207  DB00582         1.0                1          0.999351        1\n",
            " DB00567  DB05812         1.0                1          0.989749        1\n",
            " DB00468  DB00951         1.0                1          0.987908        1\n",
            "\n",
            "================================================================================\n",
            "ERROR ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "False Positives: 1589 cases\n",
            "(Predicted interaction, but drugs DON'T actually interact)\n",
            "--------------------------------------------------------------------------------\n",
            "Drug1_ID Drug2_ID  True_Label  Predicted_Label  Prediction_Score  Correct\n",
            " DB00877  DB06595         0.0                1          0.902864        0\n",
            " DB00727  DB02638         0.0                1          0.868570        0\n",
            " DB00877  DB01395         0.0                1          0.705531        0\n",
            " DB01201  DB01345         0.0                1          0.566925        0\n",
            " DB00373  DB04839         0.0                1          0.507622        0\n",
            " DB01370  DB01452         0.0                1          0.798427        0\n",
            " DB01603  DB09320         0.0                1          0.941531        0\n",
            " DB00207  DB06725         0.0                1          0.849661        0\n",
            " DB00313  DB09352         0.0                1          0.744100        0\n",
            " DB01035  DB01030         0.0                1          0.644749        0\n",
            "\n",
            "All false positives saved to: /content/drive/MyDrive/MLHygnn/DB/Model-K12/Stage1_Chemical_NetworkModel1/fixed_conservative/false_positives.csv\n",
            "\n",
            "\n",
            "False Negatives: 1019 cases\n",
            "(Predicted NO interaction, but drugs DO actually interact)\n",
            "--------------------------------------------------------------------------------\n",
            "Drug1_ID Drug2_ID  True_Label  Predicted_Label  Prediction_Score  Correct\n",
            " DB00492  DB11079         1.0                0          0.467679        0\n",
            " DB00375  DB13873         1.0                0          0.047017        0\n",
            " DB00208  DB00569         1.0                0          0.041048        0\n",
            " DB00214  DB05219         1.0                0          0.414851        0\n",
            " DB11148  DB13323         1.0                0          0.022971        0\n",
            " DB00778  DB09270         1.0                0          0.350233        0\n",
            " DB01155  DB09267         1.0                0          0.306459        0\n",
            " DB06704  DB09242         1.0                0          0.312047        0\n",
            " DB00908  DB01090         1.0                0          0.454339        0\n",
            " DB00207  DB00740         1.0                0          0.115918        0\n",
            "\n",
            "All false negatives saved to: /content/drive/MyDrive/MLHygnn/DB/Model-K12/Stage1_Chemical_NetworkModel1/fixed_conservative/false_negatives.csv\n",
            "\n",
            "================================================================================\n",
            "PREDICTION CONFIDENCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "High Confidence Correct: 31415 cases\n",
            "Uncertain Predictions (0.4-0.6): 1200 cases\n",
            "\n",
            "Sample Uncertain Predictions:\n",
            "Drug1_ID Drug2_ID  True_Label  Predicted_Label  Prediction_Score  Correct\n",
            " DB00363  DB00670         1.0                1          0.577843        1\n",
            " DB01355  DB01356         1.0                1          0.584143        1\n",
            " DB00492  DB11079         1.0                0          0.467679        0\n",
            " DB00214  DB05219         1.0                0          0.414851        0\n",
            " DB01377  DB08971         1.0                1          0.530131        1\n",
            " DB00908  DB01090         1.0                0          0.454339        0\n",
            " DB00938  DB01035         1.0                1          0.565783        1\n",
            " DB00798  DB01060         1.0                0          0.468534        0\n",
            " DB00035  DB00611         1.0                1          0.597079        1\n",
            " DB00170  DB00215         1.0                0          0.459049        0\n",
            "\n",
            "================================================================================\n",
            "STATISTICS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Total Test Samples: 38378\n",
            "  - Positive (interact): 19189\n",
            "  - Negative (no interact): 19189\n",
            "\n",
            "Correct Predictions: 35770 (93.20%)\n",
            "Incorrect Predictions: 2608 (6.80%)\n",
            "\n",
            "Prediction Score Statistics:\n",
            "  Mean: 0.5082\n",
            "  Std:  0.4611\n",
            "  Min:  0.0000\n",
            "  Max:  1.0000\n",
            "\n",
            "\n",
            "Evaluation summary saved to: /content/drive/MyDrive/MLHygnn/DB/Model-K12/Stage1_Chemical_NetworkModel1/fixed_conservative/evaluation_summary.json\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# ======================= SETUP =======================\n",
        "EXPERIMENT_CONFIG = {\n",
        "    'learning_rate': 0.005,\n",
        "    'hidden_units': 128,\n",
        "    'dropout': 0.5,\n",
        "    'weight_decay': 0.0,\n",
        "    'training_seed': 42,\n",
        "    'experiment_name': 'fixed_conservative'\n",
        "}\n",
        "\n",
        "base_path = f'/content/drive/MyDrive/MLHygnn/DB/Model-K12/Stage1_Chemical_NetworkModel1/{EXPERIMENT_CONFIG[\"experiment_name\"]}/'\n",
        "\n",
        "# ======================= LOAD DATA =======================\n",
        "# Load hypergraph and test graphs\n",
        "hyG, v_feat, e_feat, drug_X, metadata = load_data_and_create_graphs()\n",
        "train_pos_g, train_neg_g, val_pos_g, val_neg_g, test_pos_g, test_neg_g = load_train_test_data()\n",
        "\n",
        "# ======================= CREATE MODEL INSTANCES =======================\n",
        "model = Model(drug_X.shape[1], EXPERIMENT_CONFIG)\n",
        "decoder = MLPPredictor(EXPERIMENT_CONFIG['hidden_units'])\n",
        "\n",
        "# ======================= LOAD TRAINED WEIGHTS =======================\n",
        "# Load the saved checkpoint\n",
        "checkpoint = torch.load(f'{base_path}chemical_network_output.pt', weights_only=False)\n",
        "\n",
        "H = checkpoint['drug_embeddings']  # The best drug embeddings [1709, 128]\n",
        "E = checkpoint['best_epoch']       # Best epoch number\n",
        "\n",
        "print(f\"Loaded embeddings from epoch {E}\")\n",
        "print(f\"Embedding shape: {H.shape}\")\n",
        "\n",
        "# Load model weights\n",
        "\n",
        "model.load_state_dict(torch.load(f'{base_path}model_best.pth', weights_only=False))\n",
        "decoder.load_state_dict(torch.load(f'{base_path}decoder_best.pth', weights_only=False))\n",
        "\n",
        "# ======================= EVALUATION =======================\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Get predictions\n",
        "    pos_score = decoder(test_pos_g, H)\n",
        "    neg_score = decoder(test_neg_g, H)\n",
        "\n",
        "    # Compute AUC metrics\n",
        "    test_acc = compute_auc(pos_score, neg_score)\n",
        "\n",
        "# Prepare data\n",
        "scores = torch.cat([pos_score, neg_score])\n",
        "labels = torch.cat([\n",
        "    torch.ones(pos_score.shape[0]),\n",
        "    torch.zeros(neg_score.shape[0])\n",
        "])\n",
        "\n",
        "# Convert to probabilities\n",
        "sig_scores = F.sigmoid(scores)\n",
        "predictions = (sig_scores > 0.5).long()\n",
        "\n",
        "# ======================= AGGREGATE METRICS =======================\n",
        "\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AGGREGATE PERFORMANCE METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Using TensorFlow metrics\n",
        "m1 = tf.keras.metrics.BinaryAccuracy()\n",
        "m1.update_state(labels, scores)\n",
        "\n",
        "m2 = tf.keras.metrics.Precision()\n",
        "m2.update_state(labels, sig_scores)\n",
        "M2 = m2.result().numpy()\n",
        "\n",
        "m3 = tf.keras.metrics.Recall()\n",
        "m3.update_state(labels, sig_scores)\n",
        "M3 = m3.result().numpy()\n",
        "\n",
        "F1 = 2 * (M2 * M3) / (M2 + M3)\n",
        "\n",
        "print(f'Best Epoch: {E}')\n",
        "print(f'Accuracy:   {m1.result().numpy():.4f}')\n",
        "print(f'Precision:  {M2:.4f}')\n",
        "print(f'Recall:     {M3:.4f}')\n",
        "print(f'F1-Score:   {F1:.4f}')\n",
        "print(f'ROC-AUC:    {test_acc[0]:.4f}')\n",
        "print(f'PR-AUC:     {test_acc[1]:.4f}')\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(labels.numpy(), predictions.numpy())\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"                Predicted Negative    Predicted Positive\")\n",
        "print(f\"Actual Negative        {cm[0,0]:6d}              {cm[0,1]:6d}\")\n",
        "print(f\"Actual Positive        {cm[1,0]:6d}              {cm[1,1]:6d}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"INDIVIDUAL PREDICTIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "\n",
        "# Get drug IDs (handle different column names)\n",
        "if 'Drug1_ID' in test_pos.columns:\n",
        "    pos_drug1 = test_pos['Drug1_ID'].tolist()\n",
        "    pos_drug2 = test_pos['Drug2_ID'].tolist()\n",
        "    neg_drug1 = test_neg['Drug1_ID'].tolist()\n",
        "    neg_drug2 = test_neg['Drug2_ID'].tolist()\n",
        "else:\n",
        "    pos_drug1 = test_pos.iloc[:, 0].tolist()\n",
        "    pos_drug2 = test_pos.iloc[:, 1].tolist()\n",
        "    neg_drug1 = test_neg.iloc[:, 0].tolist()\n",
        "    neg_drug2 = test_neg.iloc[:, 1].tolist()\n",
        "\n",
        "# Create results dataframe\n",
        "#positive pairs THEN negative pairs\n",
        "results_df = pd.DataFrame({\n",
        "    'Drug1_ID': pos_drug1 + neg_drug1,\n",
        "    'Drug2_ID': pos_drug2 + neg_drug2,\n",
        "    'True_Label': labels.numpy(),\n",
        "    'Predicted_Label': predictions.numpy(),\n",
        "    'Prediction_Score': sig_scores.numpy(),\n",
        "    'Correct': (predictions.numpy() == labels.numpy()).astype(int)\n",
        "})\n",
        "\n",
        "# Save all predictions\n",
        "results_df.to_csv(f'{base_path}test_predictions_detailed.csv', index=False)\n",
        "print(f\"\\nAll predictions saved to: {base_path}test_predictions_detailed.csv\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"SAMPLE PREDICTIONS (First 20)\")\n",
        "print(\"-\" * 80)\n",
        "print(results_df.head(20).to_string(index=False))\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# False Positives (Type I Error)\n",
        "false_positives = results_df[\n",
        "    (results_df['True_Label'] == 0) & (results_df['Predicted_Label'] == 1)\n",
        "]\n",
        "print(f\"\\nFalse Positives: {len(false_positives)} cases\")\n",
        "print(\"(Predicted interaction, but drugs DON'T actually interact)\")\n",
        "print(\"-\" * 80)\n",
        "if len(false_positives) > 0:\n",
        "    print(false_positives.head(10).to_string(index=False))\n",
        "    false_positives.to_csv(f'{base_path}false_positives.csv', index=False)\n",
        "    print(f\"\\nAll false positives saved to: {base_path}false_positives.csv\")\n",
        "\n",
        "# False Negatives (Type II Error)\n",
        "false_negatives = results_df[\n",
        "    (results_df['True_Label'] == 1) & (results_df['Predicted_Label'] == 0)\n",
        "]\n",
        "print(f\"\\n\\nFalse Negatives: {len(false_negatives)} cases\")\n",
        "print(\"(Predicted NO interaction, but drugs DO actually interact)\")\n",
        "print(\"-\" * 80)\n",
        "if len(false_negatives) > 0:\n",
        "    print(false_negatives.head(10).to_string(index=False))\n",
        "    false_negatives.to_csv(f'{base_path}false_negatives.csv', index=False)\n",
        "    print(f\"\\nAll false negatives saved to: {base_path}false_negatives.csv\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PREDICTION CONFIDENCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# High confidence correct predictions\n",
        "high_conf_correct = results_df[\n",
        "    (results_df['Correct'] == 1) &\n",
        "    ((results_df['Prediction_Score'] > 0.9) | (results_df['Prediction_Score'] < 0.1))\n",
        "]\n",
        "print(f\"\\nHigh Confidence Correct: {len(high_conf_correct)} cases\")\n",
        "\n",
        "# Low confidence predictions (uncertain)\n",
        "uncertain = results_df[\n",
        "    (results_df['Prediction_Score'] > 0.4) &\n",
        "    (results_df['Prediction_Score'] < 0.6)\n",
        "]\n",
        "print(f\"Uncertain Predictions (0.4-0.6): {len(uncertain)} cases\")\n",
        "if len(uncertain) > 0:\n",
        "    print(\"\\nSample Uncertain Predictions:\")\n",
        "    print(uncertain.head(10).to_string(index=False))\n",
        "    uncertain.to_csv(f'{base_path}uncertain_predictions.csv', index=False)\n",
        "\n",
        "# ======================= STATISTICS SUMMARY =======================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STATISTICS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nTotal Test Samples: {len(results_df)}\")\n",
        "print(f\"  - Positive (interact): {int(labels.sum())}\")\n",
        "print(f\"  - Negative (no interact): {len(labels) - int(labels.sum())}\")\n",
        "\n",
        "print(f\"\\nCorrect Predictions: {results_df['Correct'].sum()} ({results_df['Correct'].mean()*100:.2f}%)\")\n",
        "print(f\"Incorrect Predictions: {len(results_df) - results_df['Correct'].sum()} ({(1-results_df['Correct'].mean())*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nPrediction Score Statistics:\")\n",
        "print(f\"  Mean: {results_df['Prediction_Score'].mean():.4f}\")\n",
        "print(f\"  Std:  {results_df['Prediction_Score'].std():.4f}\")\n",
        "print(f\"  Min:  {results_df['Prediction_Score'].min():.4f}\")\n",
        "print(f\"  Max:  {results_df['Prediction_Score'].max():.4f}\")\n",
        "\n",
        "\n",
        "summary_report = {\n",
        "    'Best_Epoch': E,\n",
        "    'Accuracy': float(m1.result().numpy()),\n",
        "    'Precision': float(M2),\n",
        "    'Recall': float(M3),\n",
        "    'F1_Score': float(F1),\n",
        "    'ROC_AUC': float(test_acc[0]),\n",
        "    'PR_AUC': float(test_acc[1]),\n",
        "    'Total_Samples': len(results_df),\n",
        "    'True_Positives': int(cm[1,1]),\n",
        "    'True_Negatives': int(cm[0,0]),\n",
        "    'False_Positives': int(cm[0,1]),\n",
        "    'False_Negatives': int(cm[1,0]),\n",
        "    'High_Confidence_Correct': len(high_conf_correct),\n",
        "    'Uncertain_Predictions': len(uncertain)\n",
        "}\n",
        "\n",
        "# Save as JSON\n",
        "import json\n",
        "with open(f'{base_path}evaluation_summary.json', 'w') as f:\n",
        "    json.dump(summary_report, f, indent=4)\n",
        "\n",
        "print(f\"\\n\\nEvaluation summary saved to: {base_path}evaluation_summary.json\")\n",
        "print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

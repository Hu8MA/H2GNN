{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjBqvd9PNA9v",
        "outputId": "7c8c4698-048f-4cc7-90bc-9ed37fa51be5"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --upgrade\n",
        "!pip install transformers datasets scikit-learn\n",
        "!pip uninstall dgl -y -q\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/repo.html\n",
        "!pip install torchdata==0.7.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTdp4ctvNCNI",
        "outputId": "736af78a-9d23-4af0-886c-8eb130f366d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DGL version: 2.1.0\n",
            "PyTorch version: 2.9.1+cu128\n",
            "Hypergraph creation successful!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set DGL backend before importing\n",
        "os.environ['DGLBACKEND'] = 'pytorch'\n",
        "\n",
        "# Mock GraphBolt to prevent loading issues\n",
        "class MockModule:\n",
        "    def __getattr__(self, name):\n",
        "        return lambda *args, **kwargs: None\n",
        "\n",
        "sys.modules['dgl.graphbolt'] = MockModule()\n",
        "\n",
        "# Import DGL\n",
        "import dgl\n",
        "\n",
        "# Import other libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, accuracy_score, average_precision_score, precision_recall_curve, auc, confusion_matrix,  classification_report\n",
        "import dgl.function as fn\n",
        "\n",
        "# Test everything\n",
        "print(f\"DGL version: {dgl.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Test creating a hypergraph\n",
        "try:\n",
        "    data_dict = {\n",
        "        ('node', 'in', 'edge'): ([0, 1], [0, 0]),\n",
        "        ('edge', 'con', 'node'): ([0, 0], [0, 1])\n",
        "    }\n",
        "    test_hyG = dgl.heterograph(data_dict)\n",
        "    print(\"Hypergraph creation successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy-G5IFsNETU",
        "outputId": "96b97e12-a4ab-46be-8d25-54e87b3c7d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPULOkzdNGLi"
      },
      "outputs": [],
      "source": [
        "class MLPPredictor(nn.Module):\n",
        "    \"\"\"Multi-class predictor for 86 interaction types\"\"\"\n",
        "    def __init__(self, h_feats, num_classes=86):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, num_classes)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
        "        return {'score': self.W2(F.relu(self.W1(h)))}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']  # Shape: [num_edges, 87]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8JRX-hLNH-s"
      },
      "outputs": [],
      "source": [
        "def compute_loss(pos_score, pos_labels, class_weights=None):\n",
        "    \"\"\"Loss for 86-class classification on positives only\"\"\"\n",
        "    if class_weights is not None:\n",
        "        return F.cross_entropy(pos_score, pos_labels, weight=class_weights)\n",
        "    return F.cross_entropy(pos_score, pos_labels)\n",
        "\n",
        "\n",
        "def compute_auc(pos_score, pos_labels):\n",
        "    \"\"\"Compute metrics for 86 classes\"\"\"\n",
        "    probs = F.softmax(pos_score, dim=1).cpu().detach().numpy()\n",
        "    labels_np = pos_labels.cpu().numpy()\n",
        "\n",
        "    labels_onehot = np.eye(86)[labels_np]\n",
        "\n",
        "    roc_auc = roc_auc_score(labels_onehot, probs, multi_class='ovr', average='macro')\n",
        "    pr_auc = average_precision_score(labels_onehot, probs, average='macro')\n",
        "\n",
        "    return roc_auc, pr_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKF2MvPGNKDw"
      },
      "outputs": [],
      "source": [
        "class HyGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Original HyGNN with Edge→Node→Edge flow\n",
        "    Adapted for Stage 2: Nodes=Drugs, Edges=Interaction Types\n",
        "    Flow: Type → Drug → Type\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, dropout):\n",
        "        super(HyGNN, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.query_dim = query_dim\n",
        "\n",
        "        self.in_first_layer = torch.nn.Linear(input_dim, vertex_dim)\n",
        "        self.not_in_first_layer = torch.nn.Linear(vertex_dim, vertex_dim)\n",
        "\n",
        "        # Hyperedge-level attention (Type → Drug)\n",
        "        self.w6 = torch.nn.Linear(edge_dim, query_dim)    # Edge → Query\n",
        "        self.w5 = torch.nn.Linear(vertex_dim, query_dim)  # Node → Key\n",
        "        self.w4 = torch.nn.Linear(vertex_dim, edge_dim)   # Node → Value\n",
        "\n",
        "        # Node-level attention (Drug → Type)\n",
        "        self.w3 = torch.nn.Linear(vertex_dim, query_dim)  # Node → Query\n",
        "        self.w2 = torch.nn.Linear(edge_dim, query_dim)    # Edge → Key\n",
        "        self.w1 = torch.nn.Linear(edge_dim, vertex_dim)   # Edge → Value\n",
        "\n",
        "    def red_function(self, nodes):\n",
        "        attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n",
        "        aggregated = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n",
        "        return {'h': aggregated}\n",
        "\n",
        "    def attention(self, edges):\n",
        "        attn_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n",
        "        return {'Attn': attn_score / np.sqrt(self.query_dim)}\n",
        "\n",
        "    def msg_fucntion(self, edges):\n",
        "        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n",
        "\n",
        "    def forward(self, hyG, vfeat, efeat, first_layer, last_layer):\n",
        "        if first_layer:\n",
        "            feat_v = self.in_first_layer(vfeat)  # vfeat = type features [86,128]\n",
        "        else:\n",
        "            feat_v = self.not_in_first_layer(vfeat)\n",
        "        feat_e = efeat  # efeat = drug features [1709,128]\n",
        "\n",
        "        # Stage 1: Type → Drug (Edge → Node)\n",
        "        hyG.ndata['h'] = {'edge': feat_v}  # Types are EDGES\n",
        "        hyG.ndata['k'] = {'edge': self.w2(feat_v)}  # Keys from types\n",
        "        hyG.ndata['v'] = {'edge': self.w1(feat_v)}  # Values from types\n",
        "        hyG.ndata['q'] = {'node': self.w3(feat_e)}  # Queries from drugs\n",
        "        hyG.apply_edges(self.attention, etype='con')\n",
        "        hyG.update_all(self.msg_fucntion, self.red_function, etype='con')\n",
        "\n",
        "        # Stage 2: Drug → Type (Node → Edge)\n",
        "        feat_e = hyG.ndata['h']['node']  # Updated drugs\n",
        "        hyG.ndata['k'] = {'node': self.w5(feat_e)}  # Keys from drugs\n",
        "        hyG.ndata['v'] = {'node': self.w4(feat_e)}  # Values from drugs\n",
        "        hyG.ndata['q'] = {'edge': self.w6(feat_v)}  # Queries from types\n",
        "        hyG.apply_edges(self.attention, etype='in')\n",
        "        hyG.update_all(self.msg_fucntion, self.red_function, etype='in')\n",
        "        feat_v = hyG.ndata['h']['edge']  # Final type features\n",
        "\n",
        "        if not last_layer:\n",
        "            feat_e = F.dropout(feat_e, self.dropout)\n",
        "\n",
        "        if last_layer:\n",
        "            return feat_v, feat_e\n",
        "        else:\n",
        "            return [hyG, feat_v, feat_e]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo9SINd0NxwS"
      },
      "outputs": [],
      "source": [
        "class HyGNN_Modified(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified HyGNN with Node→Edge→Node flow\n",
        "    Starts with drugs (which have pre-trained features)\n",
        "    Flow: Drug → Type → Drug\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, query_dim, vertex_dim, edge_dim, dropout):\n",
        "        super(HyGNN_Modified, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.query_dim = query_dim\n",
        "\n",
        "        self.in_first_layer = torch.nn.Linear(input_dim, vertex_dim)\n",
        "        self.not_in_first_layer = torch.nn.Linear(vertex_dim, vertex_dim)\n",
        "\n",
        "        # Node-level attention (Drug → Type)\n",
        "        self.w6 = torch.nn.Linear(edge_dim, query_dim)    # Edge → Query\n",
        "        self.w5 = torch.nn.Linear(vertex_dim, query_dim)  # Node → Key\n",
        "        self.w4 = torch.nn.Linear(vertex_dim, edge_dim)   # Node → Value\n",
        "\n",
        "        # Hyperedge-level attention (Type → Drug)\n",
        "        self.w3 = torch.nn.Linear(vertex_dim, query_dim)  # Node → Query\n",
        "        self.w2 = torch.nn.Linear(edge_dim, query_dim)    # Edge → Key\n",
        "        self.w1 = torch.nn.Linear(edge_dim, vertex_dim)   # Edge → Value\n",
        "\n",
        "    def red_function(self, nodes):\n",
        "        attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n",
        "        aggregated = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n",
        "        return {'h': aggregated}\n",
        "\n",
        "    def attention(self, edges):\n",
        "        attn_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n",
        "        return {'Attn': attn_score / np.sqrt(self.query_dim)}\n",
        "\n",
        "    def msg_fucntion(self, edges):\n",
        "        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n",
        "\n",
        "    def forward(self, hyG, vfeat, efeat, first_layer, last_layer):\n",
        "        if first_layer:\n",
        "            feat_e = self.in_first_layer(efeat)  # efeat = drug features [1709,128]\n",
        "        else:\n",
        "            feat_e = self.not_in_first_layer(efeat)\n",
        "        feat_v = vfeat  # vfeat = type features [86,128]\n",
        "\n",
        "        # Stage 1: Drug → Type (Node → Edge) - YOUR IDEA\n",
        "        hyG.ndata['h'] = {'node': feat_e}  # Start with drugs\n",
        "        hyG.ndata['k'] = {'node': self.w5(feat_e)}  # Keys from drugs\n",
        "        hyG.ndata['v'] = {'node': self.w4(feat_e)}  # Values from drugs\n",
        "        hyG.ndata['q'] = {'edge': self.w6(feat_v)}  # Queries from types\n",
        "        hyG.apply_edges(self.attention, etype='in')  # Types query drugs\n",
        "        hyG.update_all(self.msg_fucntion, self.red_function, etype='in')\n",
        "\n",
        "        # Stage 2: Type → Drug (Edge → Node)\n",
        "        feat_v = hyG.ndata['h']['edge']  # Updated types\n",
        "        hyG.ndata['k'] = {'edge': self.w2(feat_v)}  # Keys from types\n",
        "        hyG.ndata['v'] = {'edge': self.w1(feat_v)}  # Values from types\n",
        "        hyG.ndata['q'] = {'node': self.w3(feat_e)}  # Queries from drugs\n",
        "        hyG.apply_edges(self.attention, etype='con')  # Drugs query types\n",
        "        hyG.update_all(self.msg_fucntion, self.red_function, etype='con')\n",
        "        feat_e = hyG.ndata['h']['node']  # Final drug features\n",
        "\n",
        "        if not last_layer:\n",
        "            feat_v = F.dropout(feat_v, self.dropout)\n",
        "\n",
        "        if last_layer:\n",
        "            return feat_v, feat_e\n",
        "        else:\n",
        "            return [hyG, feat_v, feat_e]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV37t8tLN6xY"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"Wrapper for HyGNN - can switch between original and modified\"\"\"\n",
        "    def __init__(self, drug_feature_dim, config, use_modified=False):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        if use_modified:\n",
        "            self.gat1 = HyGNN_Modified(\n",
        "                drug_feature_dim,\n",
        "                config['hidden_units'],\n",
        "                config['hidden_units'],\n",
        "                config['hidden_units'],\n",
        "                config['dropout']\n",
        "            )\n",
        "        else:\n",
        "            self.gat1 = HyGNN(\n",
        "                drug_feature_dim,\n",
        "                config['hidden_units'],\n",
        "                config['hidden_units'],\n",
        "                config['hidden_units'],\n",
        "                config['dropout']\n",
        "            )\n",
        "\n",
        "    def forward(self, hyG, v_feat, e_feat, f, l):\n",
        "        h = self.gat1(hyG, v_feat, e_feat, f, l)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqsCPdK0N7B4"
      },
      "outputs": [],
      "source": [
        "def load_stage1_embeddings():\n",
        "    \"\"\"Load drug embeddings from Stage 1 (Chemical Network)\"\"\"\n",
        "    print(\"Loading Stage 1 embeddings...\")\n",
        "\n",
        "    path = '/content/drive/MyDrive/MyModel/Model2-S42-K9-Experment-1/chemical_network_output.pt' # we use Model2-S42-K9-Experment-1\n",
        "    checkpoint = torch.load(path, weights_only=False)\n",
        "\n",
        "    H = checkpoint['drug_embeddings']  # [1709, 128]\n",
        "    drug_to_idx = checkpoint['drug_to_id']\n",
        "\n",
        "    print(f\"  Drug embeddings shape: {H.shape}\")\n",
        "    print(f\"  Number of drugs: {len(drug_to_idx)}\")\n",
        "    print(f\"  Stage 1 best epoch: {checkpoint['best_epoch']}\")\n",
        "    print(f\"  Stage 1 ROC-AUC: {checkpoint['final_performance']['roc_auc']:.4f}\")\n",
        "\n",
        "    return H, drug_to_idx\n",
        "\n",
        "\n",
        "def load_train_test_data_multiclass(drug_to_idx):\n",
        "    \"\"\"Load training/validation/test data with interaction type labels (including negatives)\"\"\"\n",
        "    print(\"\\nLoading multi-class training data...\")\n",
        "\n",
        "    # Load positive samples with interaction types (1-86)\n",
        "    train_pos = pd.read_csv('/content/drive/MyDrive/MyModel/Metabolic-Seed42/train_fixed.csv')\n",
        "    val_pos = pd.read_csv('/content/drive/MyDrive/MyModel/Metabolic-Seed42/val_fixed.csv')\n",
        "    test_pos = pd.read_csv('/content/drive/MyDrive/MyModel/Metabolic-Seed42/test_fixed.csv')\n",
        "\n",
        "    def create_dgl_graph_with_types(df, num_nodes, drug_mapping):\n",
        "        \"\"\"Create DGL graph with interaction type labels\"\"\"\n",
        "        # Get drug IDs\n",
        "        if 'Drug1_ID' in df.columns and 'Drug2_ID' in df.columns:\n",
        "            src_ids = df['Drug1_ID'].values\n",
        "            dst_ids = df['Drug2_ID'].values\n",
        "        else:\n",
        "            src_ids = df.iloc[:, 0].values\n",
        "            dst_ids = df.iloc[:, 1].values\n",
        "\n",
        "        # Get interaction types/labels\n",
        "        if 'Label' in df.columns:\n",
        "            types = df['Label'].values \n",
        "        elif 'Y' in df.columns:\n",
        "            types = df['Y'].values\n",
        "        elif 'Interaction_Type' in df.columns:\n",
        "            types = df['Interaction_Type'].values\n",
        "        else:\n",
        "            raise ValueError(f\"No label column found. Available: {df.columns.tolist()}\")\n",
        "\n",
        "        types = types - 1\n",
        "        # Convert to indices\n",
        "        src = torch.tensor([drug_mapping[drug_id] for drug_id in src_ids], dtype=torch.long)\n",
        "        dst = torch.tensor([drug_mapping[drug_id] for drug_id in dst_ids], dtype=torch.long)\n",
        "        types = torch.tensor(types, dtype=torch.long)  # Keep as-is: 0-86\n",
        "\n",
        "        # Create graph\n",
        "        g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
        "        g.edata['type'] = types\n",
        "\n",
        "        return g\n",
        "\n",
        "    # Create graphs for positive samples\n",
        "    train_pos_g = create_dgl_graph_with_types(train_pos, len(drug_to_idx), drug_to_idx)\n",
        "    val_pos_g = create_dgl_graph_with_types(val_pos, len(drug_to_idx), drug_to_idx)\n",
        "    test_pos_g = create_dgl_graph_with_types(test_pos, len(drug_to_idx), drug_to_idx)\n",
        "\n",
        "\n",
        "    print(f\"  Train positive samples: {train_pos_g.number_of_edges()}\")\n",
        "    print(f\"  Validation positive samples: {val_pos_g.number_of_edges()}\")\n",
        "    print(f\"  Test positive samples: {test_pos_g.number_of_edges()}\")\n",
        "\n",
        "    return train_pos_g, val_pos_g, test_pos_g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KrB2xL_C8RNd"
      },
      "outputs": [],
      "source": [
        "def load_metabolic_hypergraph(hyg_path, metadata_path):\n",
        "    \"\"\"Load metabolic hypergraph structure\"\"\"\n",
        "    print(\"\\nLoading metabolic hypergraph...\")\n",
        "\n",
        "    # Load hypergraph edges\n",
        "    edges = torch.load(hyg_path, weights_only=False)\n",
        "    metadata = torch.load(metadata_path, weights_only=False)\n",
        "\n",
        "    print(f\"  Hypergraph edges shape: {edges.shape}\")\n",
        "    print(f\"  Total connections: {edges.shape[0]:,}\")\n",
        "    print(f\"  Unique drugs: {len(torch.unique(edges[:, 0]))}\")\n",
        "    print(f\"  Unique interaction types: {len(torch.unique(edges[:, 1]))}\")\n",
        "\n",
        "    # Create DGL heterograph\n",
        "    # edges[:, 0] = drug indices (nodes)\n",
        "    # edges[:, 1] = interaction type indices (hyperedges)\n",
        "    data_dict = {\n",
        "        ('node', 'in', 'edge'): (edges[:, 0], edges[:, 1]),\n",
        "        ('edge', 'con', 'node'): (edges[:, 1], edges[:, 0])\n",
        "    }\n",
        "\n",
        "    hyG = dgl.heterograph(data_dict)\n",
        "\n",
        "    print(f\"\\nHypergraph structure:\")\n",
        "    print(hyG)\n",
        "\n",
        "    return hyG, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d_vW-pm_UcFM"
      },
      "outputs": [],
      "source": [
        "def create_interaction_type_features(num_types=86, hidden_dim=128):\n",
        "    \"\"\"Create one-hot features for interaction types\"\"\"\n",
        "    print(f\"\\nCreating interaction type features...\")\n",
        "    print(f\"  Using one-hot encoding for {num_types} types\")\n",
        "\n",
        "    # Create sparse identity matrix\n",
        "    from scipy.sparse import coo_matrix\n",
        "    nl = coo_matrix((num_types, num_types))\n",
        "    nl.setdiag(1)\n",
        "    values = nl.data\n",
        "    indices = np.vstack((nl.row, nl.col))\n",
        "    i = torch.LongTensor(indices)\n",
        "    v = torch.FloatTensor(values)\n",
        "    type_X = torch.sparse_coo_tensor(i, v, torch.Size((num_types, num_types)))\n",
        "\n",
        "    print(f\"  Sparse identity matrix created: {type_X.shape}\")\n",
        "\n",
        "    return type_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pja6yvNODto",
        "outputId": "4f6c7733-27d3-4720-a134-1086c23439a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment: metabolic_network_original\n",
            "Config: {'learning_rate': 0.005, 'hidden_units': 128, 'dropout': 0.5, 'weight_decay': 0.0, 'training_seed': 42, 'experiment_name': 'metabolic_network_original', 'use_modified_attention': False}\n",
            "Directory: /content/drive/MyDrive/MyModel/DB/Metabolic/Model1-metabolic_network_original/\n"
          ]
        }
      ],
      "source": [
        "# ================= CELL 10: TRAINING CONFIGURATION =================\n",
        "\n",
        "# EXPERIMENT CONFIG\n",
        "EXPERIMENT_CONFIG = {\n",
        "    'learning_rate': 0.005,\n",
        "    'hidden_units': 128,\n",
        "    'dropout': 0.5,\n",
        "    'weight_decay': 0.0,\n",
        "    'training_seed': 42,\n",
        "    'experiment_name': 'metabolic_network_original',  # or 'metabolic_network_modified'\n",
        "    'use_modified_attention': False  # Set True to test modified flow\n",
        "}\n",
        "\n",
        "base_path = f'/content/drive/MyDrive/MyModel/DB/Metabolic/Model1-{EXPERIMENT_CONFIG[\"experiment_name\"]}/'\n",
        "\n",
        "# Create experiment directory\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "print(f\"Experiment: {EXPERIMENT_CONFIG['experiment_name']}\")\n",
        "print(f\"Config: {EXPERIMENT_CONFIG}\")\n",
        "print(f\"Directory: {base_path}\")\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(EXPERIMENT_CONFIG['training_seed'])\n",
        "np.random.seed(EXPERIMENT_CONFIG['training_seed'])\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(EXPERIMENT_CONFIG['training_seed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqhc0-cgOFJa",
        "outputId": "6d052d2f-3000-46e4-cdd2-1129ea572f21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Stage 1 embeddings...\n",
            "  Drug embeddings shape: torch.Size([1709, 128])\n",
            "  Number of drugs: 1709\n",
            "  Stage 1 best epoch: 494\n",
            "  Stage 1 ROC-AUC: 0.9846\n",
            "\n",
            "Loading metabolic hypergraph...\n",
            "  Hypergraph edges shape: torch.Size([13486, 2])\n",
            "  Total connections: 13,486\n",
            "  Unique drugs: 1709\n",
            "  Unique interaction types: 86\n",
            "\n",
            "Hypergraph structure:\n",
            "Graph(num_nodes={'edge': 86, 'node': 1709},\n",
            "      num_edges={('edge', 'con', 'node'): 13486, ('node', 'in', 'edge'): 13486},\n",
            "      metagraph=[('edge', 'node', 'con'), ('node', 'edge', 'in')])\n",
            "\n",
            "Creating interaction type features...\n",
            "  Using one-hot encoding for 86 types\n",
            "  Sparse identity matrix created: torch.Size([86, 86])\n",
            "\n",
            "Loading multi-class training data...\n",
            "  Train positive samples: 153489\n",
            "  Validation positive samples: 19188\n",
            "  Test positive samples: 19200\n",
            "\n",
            "============================================================\n",
            "Data loading complete!\n",
            "============================================================\n",
            "Drug features (e_feat): torch.Size([1709, 128])\n",
            "Interaction type features (v_feat): torch.Size([86, 86])\n",
            "Hypergraph nodes (drugs): 1709\n",
            "Hypergraph edges (types): 86\n",
            "\n",
            "============================================================\n",
            "Drug Mapping Verification\n",
            "============================================================\n",
            "Stage 1 drugs: 1709\n",
            "Stage 2 drugs: 1709\n",
            "Shared drugs: 1709\n",
            "✓ Drug vocabularies are identical\n",
            "✓ Drug index mappings are identical\n",
            "✓ Stage 1 embeddings will correctly transfer to Stage 2\n"
          ]
        }
      ],
      "source": [
        "# Load Stage 1 embeddings (these become drug features)\n",
        "H_stage1, drug_to_idx = load_stage1_embeddings()\n",
        "\n",
        "# Load metabolic hypergraph\n",
        "hyG, metadata = load_metabolic_hypergraph(\n",
        "    '/content/drive/MyDrive/MyModel/metabolic_hypergraph.pt',\n",
        "    '/content/drive/MyDrive/MyModel/metabolic_hypergraph_metadata.pt'\n",
        ")\n",
        "\n",
        "# Create interaction type features\n",
        "type_X = create_interaction_type_features(num_types=86, hidden_dim=128)\n",
        "\n",
        "# Load train/val/test data\n",
        "train_pos_g, val_pos_g, test_pos_g = load_train_test_data_multiclass(drug_to_idx)\n",
        "\n",
        "# Set initial features for hypergraph\n",
        "# e_feat = drug features from Stage 1 [1709, 128]\n",
        "# v_feat = interaction type features (ones vector) [86, 128]\n",
        "e_feat = H_stage1\n",
        "v_feat = torch.tensor(type_X.to_dense())\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Data loading complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Drug features (e_feat): {e_feat.shape}\")\n",
        "print(f\"Interaction type features (v_feat): {v_feat.shape}\")\n",
        "print(f\"Hypergraph nodes (drugs): {hyG.num_nodes('node')}\")\n",
        "print(f\"Hypergraph edges (types): {hyG.num_nodes('edge')}\")\n",
        "\n",
        "\n",
        "# ================= VERIFICATION =================\n",
        "\n",
        "# Verify drug mapping consistency\n",
        "stage1_drugs = set(drug_to_idx.keys())\n",
        "stage2_drugs = set(metadata['drug_to_idx'].keys())\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Drug Mapping Verification\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Stage 1 drugs: {len(stage1_drugs)}\")\n",
        "print(f\"Stage 2 drugs: {len(stage2_drugs)}\")\n",
        "print(f\"Shared drugs: {len(stage1_drugs & stage2_drugs)}\")\n",
        "\n",
        "if stage1_drugs == stage2_drugs:\n",
        "    print(\"✓ Drug vocabularies are identical\")\n",
        "\n",
        "    # Check if indices match\n",
        "    mismatch = False\n",
        "    for drug_id in list(stage1_drugs)[:5]:\n",
        "        idx1 = drug_to_idx[drug_id]\n",
        "        idx2 = metadata['drug_to_idx'][drug_id]\n",
        "        if idx1 != idx2:\n",
        "            print(f\"✗ Mismatch: {drug_id} has index {idx1} in Stage1 but {idx2} in Stage2\")\n",
        "            mismatch = True\n",
        "\n",
        "    if not mismatch:\n",
        "        print(\"✓ Drug index mappings are identical\")\n",
        "        print(\"✓ Stage 1 embeddings will correctly transfer to Stage 2\")\n",
        "else:\n",
        "    print(\"✗ WARNING: Drug vocabularies differ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zHCYRuTVyC30"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import os\n",
        "import time\n",
        "\n",
        "def calculate_ram_usage():\n",
        "    \"\"\"Calculate current RAM usage in GB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    ram_gb = process.memory_info().rss / (1024 ** 3)  # Convert to GB\n",
        "    return ram_gb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh3QUGHpOHIt",
        "outputId": "564ca2dc-ff70-47c4-976d-11f1536a2e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use Original HyGNN attantion\n",
            "\n",
            "Class weights statistics:\n",
            "  Min weight: 0.0387\n",
            "  Max weight: 4.2745\n",
            "  Mean weight: 1.0000\n",
            " RAM usage before training: 1.39 GB\n",
            "\n",
            "Starting Stage 2 (Metabolic Network) training...\n",
            "\n",
            "Starting Stage 2 (Metabolic Network) training...\n",
            "Start time: 2025-11-17 18:14:44\n",
            "\n",
            "Starting Stage 2 (Metabolic Network) training...\n",
            "Epoch 0: loss: 4.4829, val_loss: 4.4217 (best: 4.4217, patience: 0)\n",
            "Epoch 10: loss: 2.6488, val_loss: 4.6302 (best: 3.9911, patience: 7)\n",
            "Epoch 20: loss: 2.4583, val_loss: 4.2259 (best: 3.9911, patience: 17)\n",
            "Epoch 30: loss: 2.3091, val_loss: 4.1341 (best: 3.9911, patience: 27)\n",
            "Epoch 40: loss: 2.0704, val_loss: 3.8873 (best: 3.8873, patience: 0)\n",
            "Epoch 50: loss: 1.9072, val_loss: 3.5960 (best: 3.5960, patience: 0)\n",
            "Epoch 60: loss: 1.7692, val_loss: 3.3514 (best: 3.3514, patience: 0)\n",
            "Epoch 70: loss: 1.6186, val_loss: 3.0396 (best: 3.0396, patience: 0)\n",
            "Epoch 80: loss: 1.4684, val_loss: 2.6774 (best: 2.6774, patience: 0)\n",
            "Epoch 90: loss: 1.3099, val_loss: 2.3013 (best: 2.3013, patience: 0)\n",
            "Epoch 100: loss: 1.1739, val_loss: 2.0133 (best: 2.0133, patience: 0)\n",
            "Epoch 110: loss: 1.0691, val_loss: 1.7589 (best: 1.7589, patience: 0)\n",
            "Epoch 120: loss: 0.9791, val_loss: 1.5672 (best: 1.5672, patience: 0)\n",
            "Epoch 130: loss: 0.9045, val_loss: 1.4183 (best: 1.4183, patience: 0)\n",
            "Epoch 140: loss: 0.8462, val_loss: 1.3038 (best: 1.3034, patience: 1)\n",
            "Epoch 150: loss: 0.7992, val_loss: 1.2084 (best: 1.2012, patience: 1)\n",
            "Epoch 160: loss: 0.7579, val_loss: 1.1229 (best: 1.1215, patience: 2)\n",
            "Epoch 170: loss: 0.7219, val_loss: 1.0389 (best: 1.0389, patience: 0)\n",
            "Epoch 180: loss: 0.6857, val_loss: 0.9982 (best: 0.9982, patience: 0)\n",
            "Epoch 190: loss: 0.6597, val_loss: 0.9491 (best: 0.9454, patience: 1)\n",
            "Epoch 200: loss: 0.6840, val_loss: 0.9586 (best: 0.9234, patience: 6)\n",
            "Epoch 210: loss: 0.6330, val_loss: 0.9097 (best: 0.9097, patience: 0)\n",
            "Epoch 220: loss: 0.6040, val_loss: 0.8503 (best: 0.8503, patience: 0)\n",
            "Epoch 230: loss: 0.5825, val_loss: 0.8137 (best: 0.8137, patience: 0)\n",
            "Epoch 240: loss: 0.5763, val_loss: 0.8106 (best: 0.7945, patience: 1)\n",
            "Epoch 250: loss: 0.5584, val_loss: 0.7860 (best: 0.7818, patience: 5)\n",
            "Epoch 260: loss: 0.5405, val_loss: 0.7522 (best: 0.7522, patience: 0)\n",
            "Epoch 270: loss: 0.5278, val_loss: 0.7368 (best: 0.7368, patience: 0)\n",
            "Epoch 280: loss: 0.5153, val_loss: 0.7263 (best: 0.7263, patience: 0)\n",
            "Epoch 290: loss: 0.5040, val_loss: 0.7136 (best: 0.7132, patience: 1)\n",
            "Epoch 300: loss: 0.4936, val_loss: 0.6998 (best: 0.6998, patience: 0)\n",
            "Epoch 310: loss: 0.4946, val_loss: 0.6897 (best: 0.6897, patience: 0)\n",
            "Epoch 320: loss: 0.4849, val_loss: 0.6719 (best: 0.6658, patience: 3)\n",
            "Epoch 330: loss: 0.4692, val_loss: 0.6711 (best: 0.6658, patience: 13)\n",
            "Epoch 340: loss: 0.4581, val_loss: 0.6531 (best: 0.6531, patience: 0)\n",
            "Epoch 350: loss: 0.4508, val_loss: 0.6441 (best: 0.6422, patience: 1)\n",
            "Epoch 360: loss: 0.4476, val_loss: 0.6388 (best: 0.6303, patience: 1)\n",
            "Epoch 370: loss: 0.4401, val_loss: 0.6334 (best: 0.6236, patience: 1)\n",
            "Epoch 380: loss: 0.4338, val_loss: 0.6208 (best: 0.6193, patience: 8)\n",
            "Epoch 390: loss: 0.4273, val_loss: 0.6116 (best: 0.6070, patience: 1)\n",
            "Epoch 400: loss: 0.4190, val_loss: 0.6029 (best: 0.5997, patience: 2)\n",
            "Epoch 410: loss: 0.4180, val_loss: 0.5928 (best: 0.5928, patience: 0)\n",
            "Epoch 420: loss: 0.4113, val_loss: 0.5901 (best: 0.5868, patience: 3)\n",
            "Epoch 430: loss: 0.4075, val_loss: 0.5825 (best: 0.5825, patience: 0)\n",
            "Epoch 440: loss: 0.3999, val_loss: 0.5799 (best: 0.5791, patience: 5)\n",
            "Epoch 450: loss: 0.3949, val_loss: 0.5791 (best: 0.5743, patience: 1)\n",
            "Epoch 460: loss: 0.3918, val_loss: 0.5742 (best: 0.5723, patience: 1)\n",
            "Epoch 470: loss: 0.4059, val_loss: 0.5722 (best: 0.5674, patience: 3)\n",
            "Epoch 480: loss: 0.3935, val_loss: 0.5775 (best: 0.5674, patience: 13)\n",
            "Epoch 490: loss: 0.3857, val_loss: 0.5622 (best: 0.5620, patience: 1)\n",
            "\n",
            "Training completed!\n",
            "Best epoch: 499\n",
            "Best validation loss: 0.5535\n",
            "  - Drug embeddings: torch.Size([1709, 128])\n",
            "  - Best epoch: 499\n",
            "  - Best validation loss: 0.5535\n",
            "\n",
            "Timing Statistics:\n",
            "  Total training time: 1036.80 seconds\n",
            "  Total training time: 17.28 minutes\n",
            "  Total training time: 0.29 hours\n",
            "  Average time per epoch: 2.07 seconds\n",
            "RAM usage before training: 1.39 GB\n",
            "RAM usage after training:  1.68 GB\n",
            "RAM used during training:  0.29 GB\n",
            "  Epochs completed: 500\n",
            "  End time: 2025-11-17 18:32:01\n"
          ]
        }
      ],
      "source": [
        "# Create model and decoder\n",
        "if EXPERIMENT_CONFIG['use_modified_attention']:\n",
        "    input_dim = e_feat.shape[1]  # 128 for Modified\n",
        "    print('Use Modified HyGNN attantion')\n",
        "else:\n",
        "    input_dim = type_X.shape[1]  # 86 for Original\n",
        "    print('Use Original HyGNN attantion')\n",
        "\n",
        "model = Model(\n",
        "    input_dim,  \n",
        "    EXPERIMENT_CONFIG,\n",
        "    use_modified=EXPERIMENT_CONFIG['use_modified_attention']\n",
        ")\n",
        "decoder = MLPPredictor(EXPERIMENT_CONFIG['hidden_units'], num_classes=86)  )\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    itertools.chain(model.parameters(), decoder.parameters()),\n",
        "    lr=EXPERIMENT_CONFIG['learning_rate']\n",
        ")\n",
        "\n",
        "type_counts = torch.bincount(train_pos_g.edata['type'], minlength=86).float()\n",
        "alpha = 0.5\n",
        "class_weights = 1.0 / torch.pow(type_counts.clamp(min=1.0), alpha)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "\n",
        "print(f\"\\nClass weights statistics:\")\n",
        "print(f\"  Min weight: {class_weights.min():.4f}\")\n",
        "print(f\"  Max weight: {class_weights.max():.4f}\")\n",
        "print(f\"  Mean weight: {class_weights.mean():.4f}\")\n",
        "\n",
        "\n",
        "# Training variables\n",
        "best_val_loss = 1e10\n",
        "patience = 0\n",
        "best_embeddings = None\n",
        "best_epoch = 0\n",
        "\n",
        "training_start_time = time.time()\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\" RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(\"\\nStarting Stage 2 (Metabolic Network) training...\")\n",
        "print(\"\\nStarting Stage 2 (Metabolic Network) training...\")\n",
        "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(training_start_time))}\")\n",
        "\n",
        "print(\"\\nStarting Stage 2 (Metabolic Network) training...\")\n",
        "\n",
        "for e in range(500):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # Forward pass through HyGNN\n",
        "    h = model(hyG, v_feat, e_feat, True, True)\n",
        "    h_drug = h[1]  # Get drug embeddings [1709, 128]\n",
        "\n",
        "    pos_score = decoder(train_pos_g, h_drug)  # [num_edges, 87]\n",
        "\n",
        "    pos_labels = train_pos_g.edata['type']  # [num_edges] - values 1-86\n",
        "\n",
        "    \n",
        "    loss = compute_loss(pos_score, pos_labels)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        decoder.eval()\n",
        "\n",
        "        pos_score_val = decoder(val_pos_g, h_drug)\n",
        "        pos_labels_val = val_pos_g.edata['type']\n",
        "        val_loss = compute_loss(pos_score_val, pos_labels_val, class_weights)\n",
        "\n",
        "        # Model selection based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss.item()\n",
        "            best_embeddings = h_drug.clone()\n",
        "            best_epoch = e\n",
        "            patience = 0\n",
        "\n",
        "            # Save models\n",
        "            torch.save(decoder.state_dict(), f'{base_path}decoder_best.pth')\n",
        "            torch.save(model.state_dict(), f'{base_path}model_best.pth')\n",
        "            torch.save(best_embeddings, f'{base_path}best_embeddings.pt')  # ADD THIS LINE\n",
        "\n",
        "        else:\n",
        "            patience += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience > 100:\n",
        "            print(f\"Early stopping at epoch {e}\")\n",
        "            break\n",
        "\n",
        "    # Progress reporting every 10 epochs\n",
        "    if e % 10 == 0:\n",
        "        print(f'Epoch {e}: loss: {loss:.4f}, val_loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience})')\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "H = best_embeddings\n",
        "E = best_epoch\n",
        "\n",
        "\n",
        "training_end_time = time.time()\n",
        "total_training_time = training_end_time - training_start_time\n",
        "# Get RAM usage after training\n",
        "ram_after = calculate_ram_usage()\n",
        "ram_used = ram_after - ram_before\n",
        "\n",
        "print(f\"  - Drug embeddings: {H.shape}\")\n",
        "print(f\"  - Best epoch: {E}\")\n",
        "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "print(f\"\\nTiming Statistics:\")\n",
        "print(f\"  Total training time: {total_training_time:.2f} seconds\")\n",
        "print(f\"  Total training time: {total_training_time/60:.2f} minutes\")\n",
        "print(f\"  Total training time: {total_training_time/3600:.2f} hours\")\n",
        "print(f\"  Average time per epoch: {total_training_time/(e+1):.2f} seconds\")\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"RAM usage after training:  {ram_after:.2f} GB\")\n",
        "print(f\"RAM used during training:  {ram_used:.2f} GB\")\n",
        "print(f\"  Epochs completed: {e+1}\")\n",
        "print(f\"  End time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(training_end_time))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8ZWaFo19Nfi",
        "outputId": "95d8d14f-f43e-4ab9-88b5-682ba5f3c8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 2 (METABOLIC NETWORK) - FINAL RESULTS (LINE 3: INTRINSIC)\n",
            "================================================================================\n",
            "Best Epoch: 499\n",
            "\n",
            "Test Performance (Positives Only):\n",
            "  ROC-AUC: 0.994405\n",
            "  PR-AUC:  0.872433\n",
            "  Top-1 Accuracy: 0.857344\n",
            "  Top-3 Accuracy: 0.981094\n",
            "\n",
            "Per-Class Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Type_1       1.00      0.50      0.67         2\n",
            "      Type_2       0.87      1.00      0.93        27\n",
            "      Type_3       0.98      0.98      0.98        58\n",
            "      Type_4       0.78      0.84      0.81       504\n",
            "      Type_5       0.90      0.84      0.87        32\n",
            "      Type_6       0.92      0.91      0.91       296\n",
            "      Type_7       1.00      1.00      1.00         1\n",
            "      Type_8       0.94      0.89      0.92        19\n",
            "      Type_9       0.89      0.93      0.91       230\n",
            "     Type_10       0.87      0.98      0.92        61\n",
            "     Type_11       0.83      0.19      0.30        27\n",
            "     Type_12       0.80      0.80      0.80        30\n",
            "     Type_13       1.00      0.25      0.40         4\n",
            "     Type_14       0.90      0.93      0.92        29\n",
            "     Type_15       0.78      0.88      0.82        16\n",
            "     Type_16       0.96      0.97      0.96       514\n",
            "     Type_17       1.00      1.00      1.00         6\n",
            "     Type_18       0.88      0.54      0.67        13\n",
            "     Type_19       0.81      0.87      0.84        15\n",
            "     Type_20       0.75      0.80      0.77       596\n",
            "     Type_21       0.95      0.98      0.96        42\n",
            "     Type_22       1.00      0.88      0.93         8\n",
            "     Type_23       0.75      1.00      0.86         3\n",
            "     Type_24       0.94      1.00      0.97        16\n",
            "     Type_25       0.76      0.89      0.82        62\n",
            "     Type_26       0.50      1.00      0.67         1\n",
            "     Type_27       0.97      0.99      0.98        99\n",
            "     Type_28       0.00      0.00      0.00         1\n",
            "     Type_29       1.00      1.00      1.00        36\n",
            "     Type_30       0.77      0.67      0.71        60\n",
            "     Type_31       0.00      0.00      0.00         1\n",
            "     Type_32       0.98      0.94      0.96       101\n",
            "     Type_33       0.59      0.33      0.42        51\n",
            "     Type_34       0.89      0.89      0.89        35\n",
            "     Type_35       0.33      0.20      0.25         5\n",
            "     Type_36       1.00      0.50      0.67        10\n",
            "     Type_37       0.92      0.95      0.93       258\n",
            "     Type_38       1.00      1.00      1.00         2\n",
            "     Type_39       0.87      0.93      0.90        14\n",
            "     Type_40       1.00      1.00      1.00        30\n",
            "     Type_41       1.00      0.50      0.67         2\n",
            "     Type_42       0.00      0.00      0.00         1\n",
            "     Type_43       1.00      1.00      1.00         1\n",
            "     Type_44       1.00      1.00      1.00         2\n",
            "     Type_45       1.00      1.00      1.00         2\n",
            "     Type_46       1.00      1.00      1.00         3\n",
            "     Type_47       0.78      0.87      0.82      3405\n",
            "     Type_48       0.86      0.67      0.75         9\n",
            "     Type_49       0.92      0.93      0.93      6099\n",
            "     Type_50       1.00      1.00      1.00         1\n",
            "     Type_51       0.90      1.00      0.95         9\n",
            "     Type_52       0.00      0.00      0.00         1\n",
            "     Type_53       0.72      0.79      0.75        33\n",
            "     Type_54       0.91      0.83      0.87       135\n",
            "     Type_55       1.00      1.00      1.00         9\n",
            "     Type_56       1.00      0.40      0.57         5\n",
            "     Type_57       0.85      0.90      0.87        61\n",
            "     Type_58       0.94      0.97      0.95       106\n",
            "     Type_59       1.00      1.00      1.00         5\n",
            "     Type_60       0.88      0.84      0.86       831\n",
            "     Type_61       0.98      0.96      0.97        55\n",
            "     Type_62       1.00      1.00      1.00         1\n",
            "     Type_63       1.00      1.00      1.00         3\n",
            "     Type_64       0.76      0.75      0.75        76\n",
            "     Type_65       0.75      1.00      0.86         3\n",
            "     Type_66       0.93      0.93      0.93        15\n",
            "     Type_67       0.86      0.79      0.82       122\n",
            "     Type_68       0.80      0.59      0.68        34\n",
            "     Type_69       0.86      1.00      0.93        19\n",
            "     Type_70       0.88      0.85      0.86       808\n",
            "     Type_71       0.82      0.93      0.87        57\n",
            "     Type_72       0.99      0.99      0.99       191\n",
            "     Type_73       0.79      0.72      0.75      2406\n",
            "     Type_74       0.98      0.98      0.98        57\n",
            "     Type_75       0.77      0.66      0.71       961\n",
            "     Type_76       0.84      0.90      0.87        58\n",
            "     Type_77       0.82      0.65      0.72        62\n",
            "     Type_78       1.00      1.00      1.00         2\n",
            "     Type_79       1.00      0.75      0.86         4\n",
            "     Type_80       1.00      0.43      0.60        14\n",
            "     Type_81       0.71      0.56      0.62         9\n",
            "     Type_82       0.77      0.77      0.77        31\n",
            "     Type_83       0.98      0.98      0.98       130\n",
            "     Type_84       1.00      1.00      1.00         4\n",
            "     Type_85       0.94      0.79      0.86        39\n",
            "     Type_86       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.86     19200\n",
            "   macro avg       0.85      0.80      0.81     19200\n",
            "weighted avg       0.86      0.86      0.86     19200\n",
            "\n",
            "\n",
            "================================================================================\n",
            "SAVING TRAINING OUTPUTS\n",
            "================================================================================\n",
            "✓ Complete checkpoint saved to: /content/drive/MyDrive/MyModel/DB/Metabolic/Model1-metabolic_network_original/metabolic_network_complete_checkpoint.pt\n"
          ]
        }
      ],
      "source": [
        "# ================= EVALUATION =================\n",
        "decoder.load_state_dict(torch.load(f'{base_path}decoder_best.pth', weights_only=False))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Test on positives only\n",
        "    pos_score = decoder(test_pos_g, H)\n",
        "    pos_labels = test_pos_g.edata['type']\n",
        "\n",
        "    test_auc = compute_auc(pos_score, pos_labels)\n",
        "\n",
        "predictions = torch.argmax(pos_score, dim=1).numpy()\n",
        "ground_truth = pos_labels.numpy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 2 (METABOLIC NETWORK) - FINAL RESULTS \")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best Epoch: {E}\")\n",
        "print(f\"\\nTest Performance (Positives Only):\")\n",
        "print(f\"  ROC-AUC: {test_auc[0]:.6f}\")\n",
        "print(f\"  PR-AUC:  {test_auc[1]:.6f}\")\n",
        "\n",
        "accuracy = accuracy_score(ground_truth, predictions)\n",
        "print(f\"  Top-1 Accuracy: {accuracy:.6f}\")\n",
        "\n",
        "# Top-3 accuracy\n",
        "top3_preds = torch.topk(pos_score, k=3, dim=1)[1].cpu().numpy()\n",
        "top3_acc = np.mean([gt in pred for gt, pred in zip(ground_truth, top3_preds)])\n",
        "print(f\"  Top-3 Accuracy: {top3_acc:.6f}\")\n",
        "\n",
        "# Per-class metrics\n",
        "print(\"\\nPer-Class Performance:\")\n",
        "print(classification_report(\n",
        "    ground_truth,\n",
        "    predictions,\n",
        "    target_names=[f\"Type_{i}\" for i in range(1, 87)],\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# ================= SAVE FINAL OUTPUTS FOR FUTURE EVALUATION =================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING TRAINING OUTPUTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save embeddings and complete state\n",
        "final_checkpoint = {\n",
        "    'best_epoch': E,\n",
        "    'drug_embeddings': H.cpu(),  # Best embeddings from training\n",
        "    'drug_to_idx': drug_to_idx,\n",
        "    'config': EXPERIMENT_CONFIG,\n",
        "    'final_metrics': {\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'test_roc_auc': test_auc[0],\n",
        "        'test_pr_auc': test_auc[1]\n",
        "    },\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'decoder_state_dict': decoder.state_dict()\n",
        "}\n",
        "\n",
        "checkpoint_path = f'{base_path}metabolic_network_complete_checkpoint.pt'\n",
        "torch.save(final_checkpoint, checkpoint_path)\n",
        "\n",
        "print(f\"✓ Complete checkpoint saved to: {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFr8j4CNQuUl"
      },
      "source": [
        "# IMPROVED METABOLIC NETWORK EVALUATION (Using Checkpoint Directly)\n",
        "\n",
        "*The textual description of the interaction type can be found in the output files test_predictions_top3.csv and test_predictions_with_names.csv*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2CL94rCME6y",
        "outputId": "0a47529f-7b94-4220-f18c-3688b2e3ecfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LOADING CHECKPOINT DIRECTLY\n",
            "================================================================================\n",
            "\n",
            "Loading checkpoint from: /content/drive/MyDrive/MyModel/DB/Metabolic/Model1-metabolic_network_original/metabolic_network_complete_checkpoint.pt\n",
            "✓ Checkpoint loaded successfully!\n",
            "  - Best epoch: 499\n",
            "  - Best validation loss: 0.5535\n",
            "  - Drug embeddings shape: torch.Size([1709, 128])\n",
            "  - Number of drugs in mapping: 1709\n",
            "\n",
            "Loading drug names from Google Drive...\n",
            "✓ Loaded 1709 drug names\n",
            "\n",
            "Loading test data...\n",
            "✓ Test set: 19200 samples\n",
            "\n",
            "Creating decoder and loading trained weights...\n",
            "✓ Decoder loaded from checkpoint\n",
            "\n",
            "Running evaluation...\n",
            "\n",
            "Creating detailed results table...\n",
            "\n",
            "Loading interaction type descriptions...\n",
            "✓ Loaded 86 interaction type descriptions\n",
            "Adding interaction translations with drug names...\n",
            "✓ Translations added successfully!\n",
            "✓ Detailed results saved to: /content/drive/MyDrive/MyModel/DB/Metabolic/Model1-metabolic_network_original/test_predictions_with_names.csv\n",
            "✓ Top-3 predictions saved to: /content/drive/MyDrive/MyModel/DB/Metabolic/Model1-metabolic_network_original/test_predictions_top3.csv\n",
            "\n",
            "================================================================================\n",
            "METABOLIC NETWORK - EVALUATION RESULTS (FROM CHECKPOINT)\n",
            "================================================================================\n",
            "\n",
            "Experiment: metabolic_network_original\n",
            "Timestamp: 2025-11-17 19:07:47\n",
            "Checkpoint Epoch: 499\n",
            "\n",
            "================================================================================\n",
            "AGGREGATE PERFORMANCE METRICS\n",
            "================================================================================\n",
            "Top-1 Accuracy:  0.857344\n",
            "Top-3 Accuracy:  0.981094\n",
            "Precision (macro): 0.849919\n",
            "Recall (macro):    0.801026\n",
            "F1-Score (macro):  0.810756\n",
            "ROC-AUC (macro):   0.994405\n",
            "PR-AUC (macro):    0.872433\n",
            "\n",
            "================================================================================\n",
            "CONFIDENCE-BASED ANALYSIS\n",
            "================================================================================\n",
            "High Confidence Correct (>0.7): 14,384 cases (74.92%)\n",
            "\n",
            "Sample High Confidence Correct Predictions:\n",
            "  Drug1_Name      Drug2_Name Predicted_Type_Name True_Type_Name  Prediction_Score Match\n",
            "   Valsartan      Dicoumarol             Type_47        Type_47          0.835144   Yes\n",
            "  Isradipine     Famciclovir             Type_47        Type_47          0.981288   Yes\n",
            "Moxifloxacin Diphenhydramine             Type_20        Type_20          0.998851   Yes\n",
            "     Estrone      Olsalazine             Type_49        Type_49          0.964823   Yes\n",
            "  Everolimus    Luliconazole             Type_73        Type_73          0.992203   Yes\n",
            "   Pargyline     Unoprostone             Type_60        Type_60          1.000000   Yes\n",
            "Proparacaine     Fospropofol             Type_49        Type_49          1.000000   Yes\n",
            "  Ranolazine      Bifonazole             Type_73        Type_73          0.769179   Yes\n",
            "  Conivaptan     Trabectedin             Type_73        Type_73          0.985651   Yes\n",
            "  Olanzapine   Diphenoxylate             Type_49        Type_49          0.991126   Yes\n",
            "\n",
            "High Confidence Errors (>0.7): 1,086 cases (5.66%)\n",
            "\n",
            "Sample High Confidence Errors:\n",
            "                  Drug1_Name   Drug2_Name Predicted_Type_Name True_Type_Name  Prediction_Score Match\n",
            "                  Rifampicin   Ifosfamide              Type_4        Type_77          0.722718    No\n",
            "Pentaerythritol tetranitrate    Vorapaxar              Type_6        Type_49          0.844899    No\n",
            "             Dexmedetomidine    Primidone              Type_4        Type_49          0.734909    No\n",
            "                  Cimetidine    Dasatinib             Type_73        Type_67          0.715702    No\n",
            "                  Bisoprolol    Primidone             Type_60         Type_4          0.787968    No\n",
            "                Lumefantrine   Lumacaftor             Type_75         Type_4          0.706389    No\n",
            "                   Clozapine Paliperidone             Type_20        Type_49          0.745430    No\n",
            "                  Epinastine    Silodosin             Type_73         Type_5          0.761993    No\n",
            "                 Delavirdine   Ranitidine             Type_47        Type_67          0.856878    No\n",
            "                  Aprepitant Drospirenone             Type_73        Type_75          0.979305    No\n",
            "\n",
            "Low Confidence (<0.5): 1,022 cases (5.32%)\n",
            "\n",
            "Sample Low Confidence Predictions:\n",
            "        Drug1_Name              Drug2_Name Predicted_Type_Name True_Type_Name  Prediction_Score Match\n",
            "        Irinotecan             Regorafenib             Type_47        Type_77          0.480390    No\n",
            "       Phentermine                 Doxepin             Type_76        Type_76          0.422330   Yes\n",
            "      Thioridazine              Moxonidine             Type_49        Type_49          0.389527   Yes\n",
            "       Vincristine          Troleandomycin             Type_73        Type_73          0.499385   Yes\n",
            "       Meprobamate              Lumacaftor             Type_75        Type_75          0.461786   Yes\n",
            "        Olanzapine             Cefpodoxime             Type_75        Type_67          0.383879    No\n",
            "        Guanfacine             Moclobemide             Type_49        Type_47          0.484003    No\n",
            "        Dyphylline              Probenecid             Type_73        Type_73          0.494645   Yes\n",
            "        Ergotamine            Vildagliptin             Type_49        Type_73          0.415388    No\n",
            "Diethylstilbestrol Eslicarbazepine acetate             Type_47        Type_75          0.473358    No\n",
            "\n",
            "Moderate Confidence (0.5-0.7): 2,708 cases (14.10%)\n",
            "\n",
            "================================================================================\n",
            "STATISTICS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Total Test Samples: 19,200\n",
            "Number of Interaction Types: 86\n",
            "\n",
            "Correct Predictions: 16,461 (85.73%)\n",
            "Incorrect Predictions: 2,739 (14.27%)\n",
            "\n",
            "Top-3 Statistics:\n",
            "  Correct in Top-3: 18,837 (98.11%)\n",
            "  Not in Top-3: 363 (1.89%)\n",
            "\n",
            "Prediction Score Statistics:\n",
            "  Mean:   0.8576\n",
            "  Median: 0.9357\n",
            "  Std:    0.1707\n",
            "  Min:    0.2449\n",
            "  Max:    1.0000\n",
            "\n",
            "By Correctness:\n",
            "  Correct predictions:   Mean = 0.8912, Std = 0.1451\n",
            "  Incorrect predictions: Mean = 0.6555, Std = 0.1729\n",
            "\n",
            "\n",
            "✓ Evaluation summary saved to: /content/drive/MyDrive/MyModel/DB/Metabolic/Model1-metabolic_network_original/evaluation_summary_improved.json\n",
            "================================================================================\n",
            "EVALUATION COMPLETE (Using Checkpoint)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ================= IMPROVED METABOLIC NETWORK EVALUATION (Using Checkpoint Directly) =================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# ======================= SETUP =======================\n",
        "base_path = '/content/drive/MyDrive/MyModel/DB/Metabolic/Model1-metabolic_network_original/'\n",
        "checkpoint_path = f'{base_path}metabolic_network_complete_checkpoint.pt'\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING CHECKPOINT DIRECTLY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ======================= LOAD CHECKPOINT =======================\n",
        "print(f\"\\nLoading checkpoint from: {checkpoint_path}\")\n",
        "checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
        "\n",
        "# Extract all information from checkpoint\n",
        "H = checkpoint['drug_embeddings']  # Pre-computed embeddings [1709, 128]\n",
        "drug_to_idx = checkpoint['drug_to_idx']\n",
        "EXPERIMENT_CONFIG = checkpoint['config']\n",
        "best_epoch = checkpoint['best_epoch']\n",
        "best_val_loss = checkpoint['final_metrics']['best_val_loss']\n",
        "\n",
        "print(f\"✓ Checkpoint loaded successfully!\")\n",
        "print(f\"  - Best epoch: {best_epoch}\")\n",
        "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"  - Drug embeddings shape: {H.shape}\")\n",
        "print(f\"  - Number of drugs in mapping: {len(drug_to_idx)}\")\n",
        "\n",
        "# ======================= LOAD DRUG NAMES =======================\n",
        "print(\"\\nLoading drug names from Google Drive...\")\n",
        "\n",
        "drug_names_path = '/content/drive/MyDrive/MyModel/ddi_DrugBank_DrugName_map.csv'\n",
        "drug_names_df = pd.read_csv(drug_names_path)\n",
        "\n",
        "# Create drug ID to name mapping\n",
        "if 'Drug ID' in drug_names_df.columns and 'Name' in drug_names_df.columns:\n",
        "    drug_id_to_name = dict(zip(drug_names_df['Drug ID'], drug_names_df['Name']))\n",
        "elif 'Drug_ID' in drug_names_df.columns and 'Name' in drug_names_df.columns:\n",
        "    drug_id_to_name = dict(zip(drug_names_df['Drug_ID'], drug_names_df['Name']))\n",
        "else:\n",
        "    print(f\"Available columns: {drug_names_df.columns.tolist()}\")\n",
        "    drug_id_to_name = dict(zip(drug_names_df.iloc[:, 0], drug_names_df.iloc[:, 1]))\n",
        "\n",
        "print(f\"✓ Loaded {len(drug_id_to_name)} drug names\")\n",
        "\n",
        "# ======================= LOAD TEST DATA =======================\n",
        "print(\"\\nLoading test data...\")\n",
        "\n",
        "# Load test data CSV to get drug IDs\n",
        "test_pos = pd.read_csv('/content/drive/MyDrive/MyModel/Metabolic-Seed42/test_fixed.csv')\n",
        "\n",
        "print(f\"✓ Test set: {len(test_pos)} samples\")\n",
        "\n",
        "# ======================= CREATE DECODER AND LOAD STATE =======================\n",
        "print(\"\\nCreating decoder and loading trained weights...\")\n",
        "\n",
        "# Create decoder\n",
        "decoder = MLPPredictor(EXPERIMENT_CONFIG['hidden_units'], num_classes=86)\n",
        "\n",
        "# Load decoder state from checkpoint (not from separate file)\n",
        "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
        "decoder.eval()\n",
        "\n",
        "print(f\"✓ Decoder loaded from checkpoint\")\n",
        "\n",
        "# ======================= EVALUATION =======================\n",
        "print(\"\\nRunning evaluation...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    decoder.eval()\n",
        "\n",
        "    # Get predictions (test_pos_g should be available from previous cells)\n",
        "    pos_score = decoder(test_pos_g, H)  # Shape: [num_edges, 86]\n",
        "    pos_labels = test_pos_g.edata['type']  # Ground truth types [0-85]\n",
        "\n",
        "    # Get probabilities and predictions\n",
        "    probs = F.softmax(pos_score, dim=1)\n",
        "    pred_probs, pred_classes = torch.max(probs, dim=1)\n",
        "\n",
        "    # Compute metrics\n",
        "    predictions = pred_classes.numpy()\n",
        "    ground_truth = pos_labels.numpy()\n",
        "    confidence_scores = pred_probs.numpy()\n",
        "    all_probs = probs.numpy()\n",
        "\n",
        "# ======================= COMPUTE METRICS =======================\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(ground_truth, predictions)\n",
        "\n",
        "# Top-3 accuracy\n",
        "top3_preds = torch.topk(pos_score, k=3, dim=1)[1].cpu().numpy()\n",
        "top3_acc = np.mean([gt in pred for gt, pred in zip(ground_truth, top3_preds)])\n",
        "\n",
        "# Multi-class ROC-AUC and PR-AUC\n",
        "labels_onehot = np.eye(86)[ground_truth]\n",
        "roc_auc = roc_auc_score(labels_onehot, all_probs, multi_class='ovr', average='macro')\n",
        "pr_auc = average_precision_score(labels_onehot, all_probs, average='macro')\n",
        "\n",
        "# TensorFlow metrics for consistency\n",
        "m1 = tf.keras.metrics.CategoricalAccuracy()\n",
        "m1.update_state(labels_onehot, all_probs)\n",
        "\n",
        "# Precision, Recall, F1 (macro average)\n",
        "precision = precision_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "recall = recall_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "f1 = f1_score(ground_truth, predictions, average='macro', zero_division=0)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(ground_truth, predictions)\n",
        "\n",
        "# ======================= CREATE DETAILED RESULTS TABLE =======================\n",
        "print(\"\\nCreating detailed results table...\")\n",
        "\n",
        "# Get drug pairs from test data\n",
        "drug1_ids = test_pos['Drug1_ID'].values\n",
        "drug2_ids = test_pos['Drug2_ID'].values\n",
        "\n",
        "# Create detailed results\n",
        "detailed_results = []\n",
        "top3_results = []\n",
        "\n",
        "for i in range(len(drug1_ids)):\n",
        "    drug1_id = drug1_ids[i]\n",
        "    drug2_id = drug2_ids[i]\n",
        "    true_type = ground_truth[i]\n",
        "    pred_type = predictions[i]\n",
        "    score = confidence_scores[i]\n",
        "\n",
        "    # Get top-3 predictions for this sample\n",
        "    top3_indices = np.argsort(all_probs[i])[-3:][::-1]\n",
        "    top3_scores = [all_probs[i][idx] for idx in top3_indices]\n",
        "\n",
        "    # Main detailed results table\n",
        "    detailed_results.append({\n",
        "        'Drug1_ID': drug1_id,\n",
        "        'Drug1_Name': drug_id_to_name.get(drug1_id, 'Unknown'),\n",
        "        'Drug2_ID': drug2_id,\n",
        "        'Drug2_Name': drug_id_to_name.get(drug2_id, 'Unknown'),\n",
        "        'Prediction_Score': float(score),\n",
        "        'Predicted_Type_Index': int(pred_type + 1),  # 1-86\n",
        "        'Predicted_Type_Name': f\"Type_{pred_type + 1}\",\n",
        "        'Interaction_Translation': 'N/A',  # Placeholder for translation\n",
        "        'True_Type_Index': int(true_type + 1),  # 1-86\n",
        "        'True_Type_Name': f\"Type_{true_type + 1}\",\n",
        "        'Match': 'Yes' if pred_type == true_type else 'No'\n",
        "    })\n",
        "\n",
        "    # Top-3 predictions table\n",
        "    top3_results.append({\n",
        "        'Drug1_ID': drug1_id,\n",
        "        'Drug1_Name': drug_id_to_name.get(drug1_id, 'Unknown'),\n",
        "        'Drug2_ID': drug2_id,\n",
        "        'Drug2_Name': drug_id_to_name.get(drug2_id, 'Unknown'),\n",
        "        'Predicted_Type_1': int(top3_indices[0] + 1),  # Final prediction\n",
        "        'Type_1_Score': float(top3_scores[0]),\n",
        "        'Predicted_Type_2': int(top3_indices[1] + 1),\n",
        "        'Type_2_Score': float(top3_scores[1]),\n",
        "        'Predicted_Type_3': int(top3_indices[2] + 1),\n",
        "        'Type_3_Score': float(top3_scores[2]),\n",
        "        'True_Type_Index': int(true_type + 1),\n",
        "        'True_Type_Name': f\"Type_{true_type + 1}\",\n",
        "        'Match_Top1': 'Yes' if top3_indices[0] == true_type else 'No',\n",
        "        'In_Top3': 'Yes' if true_type in top3_indices else 'No'\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(detailed_results)\n",
        "top3_df = pd.DataFrame(top3_results)\n",
        "\n",
        "# ======================= ADD INTERACTION TRANSLATIONS =======================\n",
        "print(\"\\nLoading interaction type descriptions...\")\n",
        "\n",
        "# Load the interaction type mapping\n",
        "interaction_map_path = '/content/drive/MyDrive/MyModel/ddi_DrugBank_label_map.csv'\n",
        "interaction_map_df = pd.read_csv(interaction_map_path)\n",
        "\n",
        "# Create a dictionary mapping type ID to description\n",
        "if 'ID' in interaction_map_df.columns and 'InteractionType' in interaction_map_df.columns:\n",
        "    type_to_description = dict(zip(interaction_map_df['ID'], interaction_map_df['InteractionType']))\n",
        "else:\n",
        "    type_to_description = dict(zip(interaction_map_df.iloc[:, 0], interaction_map_df.iloc[:, 1]))\n",
        "\n",
        "print(f\"✓ Loaded {len(type_to_description)} interaction type descriptions\")\n",
        "\n",
        "print(\"Adding interaction translations with drug names...\")\n",
        "\n",
        "def translate_interaction(row):\n",
        "    \"\"\"Replace #Drug1 and #Drug2 with actual drug names in the interaction description\"\"\"\n",
        "    type_idx = row['Predicted_Type_Index']\n",
        "\n",
        "    # Get the description template\n",
        "    description = type_to_description.get(type_idx, 'N/A')\n",
        "\n",
        "    if description != 'N/A':\n",
        "        # Replace #Drug1 and #Drug2 with actual drug names\n",
        "        description = description.replace('#Drug1', row['Drug1_Name'])\n",
        "        description = description.replace('#Drug2', row['Drug2_Name'])\n",
        "\n",
        "    return description\n",
        "\n",
        "# Apply translation to main results\n",
        "results_df['Interaction_Translation'] = results_df.apply(translate_interaction, axis=1)\n",
        "\n",
        "# Also add translations for top-3 predictions\n",
        "def translate_top3(row, type_col):\n",
        "    \"\"\"Translate interaction for top-3 predictions\"\"\"\n",
        "    type_idx = row[type_col]\n",
        "    description = type_to_description.get(type_idx, 'N/A')\n",
        "\n",
        "    if description != 'N/A':\n",
        "        description = description.replace('#Drug1', row['Drug1_Name'])\n",
        "        description = description.replace('#Drug2', row['Drug2_Name'])\n",
        "\n",
        "    return description\n",
        "\n",
        "top3_df['Type_1_Translation'] = top3_df.apply(lambda row: translate_top3(row, 'Predicted_Type_1'), axis=1)\n",
        "top3_df['Type_2_Translation'] = top3_df.apply(lambda row: translate_top3(row, 'Predicted_Type_2'), axis=1)\n",
        "top3_df['Type_3_Translation'] = top3_df.apply(lambda row: translate_top3(row, 'Predicted_Type_3'), axis=1)\n",
        "top3_df['True_Translation'] = top3_df.apply(lambda row: translate_top3(row, 'True_Type_Index'), axis=1)\n",
        "\n",
        "print(\"✓ Translations added successfully!\")\n",
        "\n",
        "# Save detailed results\n",
        "results_df.to_csv(f'{base_path}test_predictions_with_names.csv', index=False)\n",
        "print(f\"✓ Detailed results saved to: {base_path}test_predictions_with_names.csv\")\n",
        "\n",
        "# Save top-3 predictions\n",
        "top3_df.to_csv(f'{base_path}test_predictions_top3.csv', index=False)\n",
        "print(f\"✓ Top-3 predictions saved to: {base_path}test_predictions_top3.csv\")\n",
        "\n",
        "# ======================= PRINT RESULTS =======================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METABOLIC NETWORK - EVALUATION RESULTS (FROM CHECKPOINT)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nExperiment: {EXPERIMENT_CONFIG['experiment_name']}\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Checkpoint Epoch: {best_epoch}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"AGGREGATE PERFORMANCE METRICS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(f'Top-1 Accuracy:  {accuracy:.6f}')\n",
        "print(f'Top-3 Accuracy:  {top3_acc:.6f}')\n",
        "print(f'Precision (macro): {precision:.6f}')\n",
        "print(f'Recall (macro):    {recall:.6f}')\n",
        "print(f'F1-Score (macro):  {f1:.6f}')\n",
        "print(f'ROC-AUC (macro):   {roc_auc:.6f}')\n",
        "print(f'PR-AUC (macro):    {pr_auc:.6f}')\n",
        "\n",
        "# ======================= ANALYZE PREDICTIONS BY CONFIDENCE =======================\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"CONFIDENCE-BASED ANALYSIS\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "sample_cols = ['Drug1_Name', 'Drug2_Name', 'Predicted_Type_Name', 'True_Type_Name',\n",
        "               'Prediction_Score', 'Match']\n",
        "\n",
        "# High confidence correct predictions\n",
        "high_conf_correct = results_df[\n",
        "    (results_df['Match'] == 'Yes') &\n",
        "    (results_df['Prediction_Score'] > 0.7)\n",
        "]\n",
        "print(f\"High Confidence Correct (>0.7): {len(high_conf_correct):,} cases ({len(high_conf_correct)/len(results_df)*100:.2f}%)\")\n",
        "\n",
        "if len(high_conf_correct) > 0:\n",
        "    print(\"\\nSample High Confidence Correct Predictions:\")\n",
        "    print(high_conf_correct[sample_cols].head(10).to_string(index=False))\n",
        "\n",
        "# High confidence errors\n",
        "high_conf_errors = results_df[\n",
        "    (results_df['Match'] == 'No') &\n",
        "    (results_df['Prediction_Score'] > 0.7)\n",
        "]\n",
        "print(f\"\\nHigh Confidence Errors (>0.7): {len(high_conf_errors):,} cases ({len(high_conf_errors)/len(results_df)*100:.2f}%)\")\n",
        "\n",
        "if len(high_conf_errors) > 0:\n",
        "    print(\"\\nSample High Confidence Errors:\")\n",
        "    print(high_conf_errors[sample_cols].head(10).to_string(index=False))\n",
        "    high_conf_errors.to_csv(f'{base_path}high_confidence_errors_with_names.csv', index=False)\n",
        "\n",
        "# Low confidence predictions\n",
        "low_conf = results_df[results_df['Prediction_Score'] < 0.5]\n",
        "print(f\"\\nLow Confidence (<0.5): {len(low_conf):,} cases ({len(low_conf)/len(results_df)*100:.2f}%)\")\n",
        "\n",
        "if len(low_conf) > 0:\n",
        "    print(\"\\nSample Low Confidence Predictions:\")\n",
        "    print(low_conf[sample_cols].head(10).to_string(index=False))\n",
        "    low_conf.to_csv(f'{base_path}low_confidence_with_names.csv', index=False)\n",
        "\n",
        "# Uncertain predictions (moderate confidence)\n",
        "uncertain = results_df[\n",
        "    (results_df['Prediction_Score'] >= 0.5) &\n",
        "    (results_df['Prediction_Score'] <= 0.7)\n",
        "]\n",
        "print(f\"\\nModerate Confidence (0.5-0.7): {len(uncertain):,} cases ({len(uncertain)/len(results_df)*100:.2f}%)\")\n",
        "\n",
        "# ======================= STATISTICS SUMMARY =======================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STATISTICS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nTotal Test Samples: {len(results_df):,}\")\n",
        "print(f\"Number of Interaction Types: 86\")\n",
        "\n",
        "print(f\"\\nCorrect Predictions: {results_df[results_df['Match']=='Yes'].shape[0]:,} ({accuracy*100:.2f}%)\")\n",
        "print(f\"Incorrect Predictions: {results_df[results_df['Match']=='No'].shape[0]:,} ({(1-accuracy)*100:.2f}%)\")\n",
        "\n",
        "# Top-3 statistics\n",
        "top3_correct = top3_df[top3_df['In_Top3']=='Yes'].shape[0]\n",
        "print(f\"\\nTop-3 Statistics:\")\n",
        "print(f\"  Correct in Top-3: {top3_correct:,} ({top3_acc*100:.2f}%)\")\n",
        "print(f\"  Not in Top-3: {len(top3_df) - top3_correct:,} ({(1-top3_acc)*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nPrediction Score Statistics:\")\n",
        "print(f\"  Mean:   {results_df['Prediction_Score'].mean():.4f}\")\n",
        "print(f\"  Median: {results_df['Prediction_Score'].median():.4f}\")\n",
        "print(f\"  Std:    {results_df['Prediction_Score'].std():.4f}\")\n",
        "print(f\"  Min:    {results_df['Prediction_Score'].min():.4f}\")\n",
        "print(f\"  Max:    {results_df['Prediction_Score'].max():.4f}\")\n",
        "\n",
        "# Score statistics by correctness\n",
        "correct_scores = results_df[results_df['Match']=='Yes']['Prediction_Score']\n",
        "incorrect_scores = results_df[results_df['Match']=='No']['Prediction_Score']\n",
        "\n",
        "print(f\"\\nBy Correctness:\")\n",
        "print(f\"  Correct predictions:   Mean = {correct_scores.mean():.4f}, Std = {correct_scores.std():.4f}\")\n",
        "print(f\"  Incorrect predictions: Mean = {incorrect_scores.mean():.4f}, Std = {incorrect_scores.std():.4f}\")\n",
        "\n",
        "# ======================= SAVE SUMMARY =======================\n",
        "\n",
        "summary_report = {\n",
        "    'experiment_name': EXPERIMENT_CONFIG['experiment_name'],\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'checkpoint_info': {\n",
        "        'best_epoch': int(best_epoch),\n",
        "        'best_val_loss': float(best_val_loss),\n",
        "        'checkpoint_path': checkpoint_path\n",
        "    },\n",
        "    'metrics': {\n",
        "        'top1_accuracy': float(accuracy),\n",
        "        'top3_accuracy': float(top3_acc),\n",
        "        'precision_macro': float(precision),\n",
        "        'recall_macro': float(recall),\n",
        "        'f1_score_macro': float(f1),\n",
        "        'roc_auc_macro': float(roc_auc),\n",
        "        'pr_auc_macro': float(pr_auc)\n",
        "    },\n",
        "    'statistics': {\n",
        "        'total_samples': len(results_df),\n",
        "        'num_types': 86,\n",
        "        'correct_predictions': int(np.sum(predictions == ground_truth)),\n",
        "        'incorrect_predictions': int(np.sum(predictions != ground_truth)),\n",
        "        'high_confidence_correct': len(high_conf_correct),\n",
        "        'high_confidence_errors': len(high_conf_errors),\n",
        "        'low_confidence': len(low_conf),\n",
        "        'moderate_confidence': len(uncertain)\n",
        "    },\n",
        "    'score_statistics': {\n",
        "        'overall_mean': float(results_df['Prediction_Score'].mean()),\n",
        "        'overall_std': float(results_df['Prediction_Score'].std()),\n",
        "        'correct_mean': float(correct_scores.mean()),\n",
        "        'correct_std': float(correct_scores.std()),\n",
        "        'incorrect_mean': float(incorrect_scores.mean()),\n",
        "        'incorrect_std': float(incorrect_scores.std())\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{base_path}evaluation_summary_improved.json', 'w') as f:\n",
        "    json.dump(summary_report, f, indent=4)\n",
        "\n",
        "print(f\"\\n\\n✓ Evaluation summary saved to: {base_path}evaluation_summary_improved.json\")\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATION COMPLETE (Using Checkpoint)\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RujEsK_wOJuY",
        "outputId": "9595a714-9f31-4c21-eb00-907990c64655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 2 (METABOLIC NETWORK) - FINAL RESULTS (LINE 3: INTRINSIC)\n",
            "================================================================================\n",
            "Best Epoch: 499\n",
            "\n",
            "Test Performance (Positives Only):\n",
            "  ROC-AUC: 0.993705\n",
            "  PR-AUC:  0.886490\n",
            "  Top-1 Accuracy: 0.858802\n",
            "  Top-3 Accuracy: 0.980938\n",
            "\n",
            "Per-Class Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Type_1       1.00      0.50      0.67         2\n",
            "      Type_2       0.77      0.85      0.81        27\n",
            "      Type_3       0.98      1.00      0.99        58\n",
            "      Type_4       0.79      0.78      0.78       504\n",
            "      Type_5       0.90      0.81      0.85        32\n",
            "      Type_6       0.89      0.90      0.89       296\n",
            "      Type_7       0.50      1.00      0.67         1\n",
            "      Type_8       0.85      0.89      0.87        19\n",
            "      Type_9       0.90      0.95      0.92       230\n",
            "     Type_10       0.82      0.89      0.85        61\n",
            "     Type_11       0.60      0.22      0.32        27\n",
            "     Type_12       0.94      0.97      0.95        30\n",
            "     Type_13       1.00      0.75      0.86         4\n",
            "     Type_14       0.83      1.00      0.91        29\n",
            "     Type_15       0.82      0.88      0.85        16\n",
            "     Type_16       0.91      0.95      0.93       514\n",
            "     Type_17       0.75      1.00      0.86         6\n",
            "     Type_18       0.75      0.46      0.57        13\n",
            "     Type_19       0.86      0.80      0.83        15\n",
            "     Type_20       0.77      0.85      0.81       596\n",
            "     Type_21       0.98      0.98      0.98        42\n",
            "     Type_22       1.00      0.88      0.93         8\n",
            "     Type_23       0.60      1.00      0.75         3\n",
            "     Type_24       1.00      1.00      1.00        16\n",
            "     Type_25       0.84      0.82      0.83        62\n",
            "     Type_26       0.00      0.00      0.00         1\n",
            "     Type_27       0.98      1.00      0.99        99\n",
            "     Type_28       0.00      0.00      0.00         1\n",
            "     Type_29       1.00      1.00      1.00        36\n",
            "     Type_30       0.88      0.75      0.81        60\n",
            "     Type_31       1.00      1.00      1.00         1\n",
            "     Type_32       0.93      0.94      0.94       101\n",
            "     Type_33       0.65      0.47      0.55        51\n",
            "     Type_34       0.91      0.91      0.91        35\n",
            "     Type_35       0.40      0.40      0.40         5\n",
            "     Type_36       0.86      0.60      0.71        10\n",
            "     Type_37       0.94      0.94      0.94       258\n",
            "     Type_38       1.00      0.50      0.67         2\n",
            "     Type_39       1.00      1.00      1.00        14\n",
            "     Type_40       1.00      1.00      1.00        30\n",
            "     Type_41       0.00      0.00      0.00         2\n",
            "     Type_42       0.50      1.00      0.67         1\n",
            "     Type_43       0.50      1.00      0.67         1\n",
            "     Type_44       1.00      1.00      1.00         2\n",
            "     Type_45       1.00      1.00      1.00         2\n",
            "     Type_46       1.00      1.00      1.00         3\n",
            "     Type_47       0.80      0.86      0.83      3405\n",
            "     Type_48       0.75      0.67      0.71         9\n",
            "     Type_49       0.91      0.93      0.92      6099\n",
            "     Type_50       1.00      1.00      1.00         1\n",
            "     Type_51       1.00      1.00      1.00         9\n",
            "     Type_52       1.00      1.00      1.00         1\n",
            "     Type_53       0.74      0.76      0.75        33\n",
            "     Type_54       0.90      0.81      0.86       135\n",
            "     Type_55       1.00      1.00      1.00         9\n",
            "     Type_56       1.00      0.40      0.57         5\n",
            "     Type_57       0.95      0.90      0.92        61\n",
            "     Type_58       0.96      0.97      0.97       106\n",
            "     Type_59       1.00      1.00      1.00         5\n",
            "     Type_60       0.85      0.86      0.85       831\n",
            "     Type_61       0.98      0.98      0.98        55\n",
            "     Type_62       1.00      1.00      1.00         1\n",
            "     Type_63       1.00      1.00      1.00         3\n",
            "     Type_64       0.82      0.72      0.77        76\n",
            "     Type_65       1.00      1.00      1.00         3\n",
            "     Type_66       1.00      1.00      1.00        15\n",
            "     Type_67       0.91      0.80      0.85       122\n",
            "     Type_68       1.00      0.56      0.72        34\n",
            "     Type_69       0.90      1.00      0.95        19\n",
            "     Type_70       0.89      0.86      0.88       808\n",
            "     Type_71       0.77      0.84      0.81        57\n",
            "     Type_72       0.98      0.99      0.99       191\n",
            "     Type_73       0.80      0.73      0.76      2406\n",
            "     Type_74       0.98      1.00      0.99        57\n",
            "     Type_75       0.78      0.69      0.73       961\n",
            "     Type_76       0.87      0.90      0.88        58\n",
            "     Type_77       0.84      0.60      0.70        62\n",
            "     Type_78       0.67      1.00      0.80         2\n",
            "     Type_79       1.00      1.00      1.00         4\n",
            "     Type_80       0.83      0.36      0.50        14\n",
            "     Type_81       0.75      1.00      0.86         9\n",
            "     Type_82       0.83      0.77      0.80        31\n",
            "     Type_83       0.98      0.98      0.98       130\n",
            "     Type_84       1.00      1.00      1.00         4\n",
            "     Type_85       0.82      0.82      0.82        39\n",
            "     Type_86       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.86     19200\n",
            "   macro avg       0.85      0.83      0.83     19200\n",
            "weighted avg       0.86      0.86      0.86     19200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ================= CELL 13: UPDATED EVALUATION =================\n",
        "decoder.load_state_dict(torch.load(f'{base_path}decoder_best.pth', weights_only=False))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Test on positives only\n",
        "    pos_score = decoder(test_pos_g, H)\n",
        "    pos_labels = test_pos_g.edata['type']\n",
        "\n",
        "    test_auc = compute_auc(pos_score, pos_labels)\n",
        "\n",
        "predictions = torch.argmax(pos_score, dim=1).numpy()\n",
        "ground_truth = pos_labels.numpy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 2 (METABOLIC NETWORK) - FINAL RESULTS (LINE 3: INTRINSIC)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best Epoch: {E}\")\n",
        "print(f\"\\nTest Performance (Positives Only):\")\n",
        "print(f\"  ROC-AUC: {test_auc[0]:.6f}\")\n",
        "print(f\"  PR-AUC:  {test_auc[1]:.6f}\")\n",
        "\n",
        "accuracy = accuracy_score(ground_truth, predictions)\n",
        "print(f\"  Top-1 Accuracy: {accuracy:.6f}\")\n",
        "\n",
        "# Top-3 accuracy\n",
        "top3_preds = torch.topk(pos_score, k=3, dim=1)[1].cpu().numpy()\n",
        "top3_acc = np.mean([gt in pred for gt, pred in zip(ground_truth, top3_preds)])\n",
        "print(f\"  Top-3 Accuracy: {top3_acc:.6f}\")\n",
        "\n",
        "# Per-class metrics\n",
        "print(\"\\nPer-Class Performance:\")\n",
        "print(classification_report(\n",
        "    ground_truth,\n",
        "    predictions,\n",
        "    target_names=[f\"Type_{i}\" for i in range(1, 87)],\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# ================= SAVE FINAL OUTPUTS FOR FUTURE EVALUATION =================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING TRAINING OUTPUTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save embeddings and complete state\n",
        "final_checkpoint = {\n",
        "    'best_epoch': E,\n",
        "    'drug_embeddings': H.cpu(),  # Best embeddings from training\n",
        "    'drug_to_idx': drug_to_idx,\n",
        "    'config': EXPERIMENT_CONFIG,\n",
        "    'final_metrics': {\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'test_roc_auc': test_auc[0],\n",
        "        'test_pr_auc': test_auc[1]\n",
        "    },\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'decoder_state_dict': decoder.state_dict()\n",
        "}\n",
        "\n",
        "checkpoint_path = f'{base_path}metabolic_network_complete_checkpoint.pt'\n",
        "torch.save(final_checkpoint, checkpoint_path)\n",
        "\n",
        "print(f\"✓ Complete checkpoint saved to: {checkpoint_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

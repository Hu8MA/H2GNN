{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_cell",
        "outputId": "c013b966-125c-44da-daf5-509a87fb30e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torch-geometric scikit-learn rdkit pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mount_cell",
        "outputId": "4ee82cd3-9b74-4511-8f94-d52d49b521c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports_cell",
        "outputId": "290c678e-5fce-4ccf-fe70-159be1441145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv   \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "\n",
        "# RDKit for SMILES fingerprints\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MACCSkeys\n",
        "from rdkit import DataStructs\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Learning_rate = 0.005          *before we use 0.001 and 0.005 but there was no improvement until we used 0.01*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config_cell",
        "outputId": "4105b7d7-afd5-4a32-c2d0-c98aef92b3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "CONFIGURATION\n",
            "================================================================================\n",
            "Model: GCN + MLP\n",
            "Aggregator: MEAN\n",
            "Fingerprint: MORGAN (512-bit)\n",
            "Hidden Dim: 128\n",
            "Dropout: 0.5\n",
            "Learning Rate: 0.005\n",
            "Weight Decay: 0.04\n",
            "Epochs: 500\n",
            "Batch Size: 128\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "Fingerprint_type = 'morgan'\n",
        "N_bit = 512\n",
        "Hidden_dimensions = 128\n",
        "Included_dimensions = 128\n",
        "Dropout = 0.5\n",
        "Batch_size = 128\n",
        "Learning_rate = 0.005        # before we use 0.001 and 0.005 but there was no improvement until we used 0.01\n",
        "Decay_weight = 0.04\n",
        "Number_Eproch = 500\n",
        "Seeds = 42\n",
        "Aggregator = 'mean'\n",
        "torch.manual_seed(Seeds)\n",
        "np.random.seed(Seeds)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(Seeds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model: GCN + MLP\")\n",
        "print(f\"Aggregator: {Aggregator.upper()}\")\n",
        "print(f\"Fingerprint: {Fingerprint_type.upper()} ({N_bit}-bit)\")\n",
        "print(f\"Hidden Dim: {Hidden_dimensions}\")\n",
        "print(f\"Dropout: {Dropout}\")\n",
        "print(f\"Learning Rate: {Learning_rate}\")\n",
        "print(f\"Weight Decay: {Decay_weight}\")\n",
        "print(f\"Epochs: {Number_Eproch}\")\n",
        "print(f\"Batch Size: {Batch_size}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_data_cell",
        "outputId": "3e06f6af-043c-4bed-99de-b46197631203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from Google Drive...\n",
            "\n",
            "Train data preview:\n",
            "  Drug1_ID Drug2_ID  Label\n",
            "0  DB01097  DB05219     47\n",
            "1  DB00547  DB00784     49\n",
            "2  DB00623  DB01365     61\n",
            "3  DB00328  DB09027     73\n",
            "4  DB00742  DB00955     57\n",
            "\n",
            "Dataset sizes:\n",
            "  Training: 153,489 pairs\n",
            "  Validation: 19,188 pairs\n",
            "  Test: 19,200 pairs\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/MLHygnn/BaseLine/GCN/data/'\n",
        "DATA_PATH_SMILES = '/content/drive/MyDrive/MLHygnn/BaseLine/'\n",
        "\n",
        "print(\"Loading data from Google Drive...\")\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv(DATA_PATH + 'train_positive.csv')\n",
        "val_df = pd.read_csv(DATA_PATH + 'val_positive.csv')\n",
        "test_df = pd.read_csv(DATA_PATH + 'test_positive.csv')\n",
        "\n",
        "print(f\"\\nTrain data preview:\")\n",
        "print(train_df.head())\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Training: {len(train_df):,} pairs\")\n",
        "print(f\"  Validation: {len(val_df):,} pairs\")\n",
        "print(f\"  Test: {len(test_df):,} pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_smiles"
      },
      "source": [
        "## 6. Load Drug SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_smiles_cell",
        "outputId": "66dfb544-fe36-445e-b715-0c4eaac70502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Drug SMILES loaded: 1709 drugs\n",
            "  DrugBank_ID                                             SMILES\n",
            "0     DB00006  CC[C@H](C)[C@H](NC(=O)[C@H](CCC(O)=O)NC(=O)[C@...\n",
            "1     DB00014  CC(C)C[C@H](NC(=O)[C@@H](COC(C)(C)C)NC(=O)[C@H...\n",
            "2     DB00027  CC(C)C[C@@H](NC(=O)CNC(=O)[C@@H](NC=O)C(C)C)C(...\n",
            "3     DB00035  NC(=O)CC[C@@H]1NC(=O)[C@H](CC2=CC=CC=C2)NC(=O)...\n",
            "4     DB00080  CCCCCCCCCC(=O)N[C@@H](CC1=CNC2=C1C=CC=C2)C(=O)...\n"
          ]
        }
      ],
      "source": [
        "# Load drug SMILES\n",
        "drug_smiles_df = pd.read_csv(DATA_PATH_SMILES + 'Drugs_with_Smiles.csv')\n",
        "\n",
        "print(f\"\\nDrug SMILES loaded: {len(drug_smiles_df)} drugs\")\n",
        "print(drug_smiles_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fingerprints"
      },
      "source": [
        "## 7. Extract Molecular Fingerprints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We use Morgan Fingerprint (2,512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fingerprints_cell",
        "outputId": "857727cb-da77-4e29-b94e-d06e5d312666"
      },
      "outputs": [],
      "source": [
        "# Get unique drugs\n",
        "all_drugs = pd.concat([\n",
        "    train_df['Drug1_ID'], train_df['Drug2_ID'],\n",
        "    val_df['Drug1_ID'], val_df['Drug2_ID'],\n",
        "    test_df['Drug1_ID'], test_df['Drug2_ID']\n",
        "]).unique()\n",
        "\n",
        "print(f\"\\nðŸ”¬ Extracting {Fingerprint_type} fingerprints...\")\n",
        "print(f\"Total unique drugs: {len(all_drugs)}\")\n",
        "\n",
        "def smiles_to_fingerprint(smiles, fp_type='morgan', n_bits=512):\n",
        "    \"\"\"Convert SMILES to molecular fingerprint\"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "\n",
        "        if fp_type == 'morgan':\n",
        "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
        "        elif fp_type == 'maccs':\n",
        "            fp = MACCSkeys.GenMACCSKeys(mol)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown fingerprint type: {fp_type}\")\n",
        "\n",
        "        arr = np.zeros((0,), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "        return arr\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Extract fingerprints\n",
        "drug_to_fp = {}\n",
        "drug_to_idx = {drug: idx for idx, drug in enumerate(all_drugs)}\n",
        "\n",
        "for drug_id in tqdm(all_drugs, desc=\"Extracting fingerprints\"):\n",
        "    smiles_row = drug_smiles_df[drug_smiles_df['DrugBank_ID'] == drug_id]\n",
        "    if len(smiles_row) > 0:\n",
        "        smiles = smiles_row.iloc[0]['SMILES']\n",
        "        fp = smiles_to_fingerprint(smiles, fp_type=Fingerprint_type, n_bits=N_bit)\n",
        "        if fp is not None:\n",
        "            drug_to_fp[drug_id] = fp\n",
        "        else:\n",
        "            # Default fingerprint for failed cases\n",
        "            drug_to_fp[drug_id] = np.zeros(N_bit, dtype=np.int8)\n",
        "    else:\n",
        "        drug_to_fp[drug_id] = np.zeros(N_bit, dtype=np.int8)\n",
        "\n",
        "# Create feature matrix\n",
        "num_drugs = len(all_drugs)\n",
        "drug_features = np.zeros((num_drugs, N_bit), dtype=np.float32)\n",
        "\n",
        "for drug_id, idx in drug_to_idx.items():\n",
        "    if drug_id in drug_to_fp:\n",
        "        drug_features[idx] = drug_to_fp[drug_id]\n",
        "\n",
        "drug_features = torch.FloatTensor(drug_features).to(device)\n",
        "\n",
        "print(f\"\\nFingerprints extracted!\")\n",
        "print(f\"Feature matrix shape: {drug_features.shape}\")\n",
        "print(f\"Device: {drug_features.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_data"
      },
      "source": [
        "## 8. Prepare Graph Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prepare_data_cell",
        "outputId": "6cf64366-18c1-46af-ec56-0ac6b2517757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted labels from 1-86 to 0-85\n",
            "\n",
            "Data prepared!\n",
            "Training pairs: 153,489\n",
            "Validation pairs: 19,188\n",
            "Test pairs: 19,200\n",
            "Edge index shape: torch.Size([2, 306978])\n",
            "Number of classes: 86\n"
          ]
        }
      ],
      "source": [
        "def prepare_pairs(df, drug_to_idx):\n",
        "    \"\"\"Convert dataframe to pair indices and labels\"\"\"\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        drug1 = row['Drug1_ID']\n",
        "        drug2 = row['Drug2_ID']\n",
        "        label = row['Label']\n",
        "\n",
        "        if drug1 in drug_to_idx and drug2 in drug_to_idx:\n",
        "            idx1 = drug_to_idx[drug1]\n",
        "            idx2 = drug_to_idx[drug2]\n",
        "            pairs.append([idx1, idx2])\n",
        "            labels.append(label)\n",
        "\n",
        "    return torch.LongTensor(pairs), torch.LongTensor(labels)\n",
        "\n",
        "# Prepare training, validation, and test data\n",
        "train_pairs, train_types = prepare_pairs(train_df, drug_to_idx)\n",
        "val_pairs, val_types = prepare_pairs(val_df, drug_to_idx)\n",
        "test_pairs, test_types = prepare_pairs(test_df, drug_to_idx)\n",
        "# If labels are 1-86, convert to 0-85\n",
        "if train_types.min() == 1:\n",
        "    train_types = train_types - 1\n",
        "    val_types = val_types - 1\n",
        "    test_types = test_types - 1\n",
        "    print(f\"Converted labels from 1-86 to 0-85\")\n",
        "else:\n",
        "    print(\"Labels already 0-based\")\n",
        "# Move to device\n",
        "train_pairs = train_pairs.to(device)\n",
        "train_types = train_types.to(device)\n",
        "val_pairs = val_pairs.to(device)\n",
        "val_types = val_types.to(device)\n",
        "test_pairs = test_pairs.to(device)\n",
        "test_types = test_types.to(device)\n",
        "\n",
        "# Build edge index from training data (for GCN graph structure)\n",
        "edge_index = torch.cat([train_pairs.T, train_pairs.T[[1, 0]]], dim=1)  # Bidirectional edges\n",
        "\n",
        "print(f\"\\nData prepared!\")\n",
        "print(f\"Training pairs: {len(train_pairs):,}\")\n",
        "print(f\"Validation pairs: {len(val_pairs):,}\")\n",
        "print(f\"Test pairs: {len(test_pairs):,}\")\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "print(f\"Number of classes: {train_types.max().item() + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## 9. Define GCN + MLP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "model_cell",
        "outputId": "a4327aed-2bb9-4bf4-b0d8-231dc7e4bbf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "MODEL ARCHITECTURE\n",
            "================================================================================\n",
            "GCN_MLP(\n",
            "  (gcn1): GCNConv(512, 128)\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=128, out_features=86, bias=True)\n",
            "  )\n",
            ")\n",
            "================================================================================\n",
            "\n",
            "Total parameters: 240,982\n",
            "Trainable parameters: 240,982\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "class GCN_MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.5):\n",
        "        super(GCN_MLP, self).__init__()\n",
        "\n",
        "        # Single GCN layer (matching 1-layer HGNN)\n",
        "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
        "\n",
        "        # MLP for pair classification\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),  # 512 â†’ 256\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),       # 256 â†’ 128\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)           # 128 â†’ 86\n",
        "        )\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, drug_pairs):\n",
        "\n",
        "        # GCN Single layer\n",
        "        x = self.gcn1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Get embeddings for drug pairs\n",
        "        h_i = x[drug_pairs[:, 0]]  # First drug in pair\n",
        "        h_j = x[drug_pairs[:, 1]]  # Second drug in pair\n",
        "\n",
        "        h_pair = torch.cat([\n",
        "            h_i,                    # Drug 1 embedding\n",
        "            h_j,                    # Drug 2 embedding\n",
        "            h_i * h_j,              # Element-wise product\n",
        "            torch.abs(h_i - h_j)    # Absolute difference\n",
        "        ], dim=-1)\n",
        "\n",
        "        # MLP classifier\n",
        "        logits = self.mlp(h_pair)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Initialize model\n",
        "model = GCN_MLP(\n",
        "    input_dim=N_bit,\n",
        "    hidden_dim=Hidden_dimensions,\n",
        "    num_classes=86,\n",
        "    dropout=Dropout\n",
        ").to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\"*80)\n",
        "print(model)\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_setup"
      },
      "source": [
        "## 10. Training Setup\n",
        "\n",
        "### We used class weight in both the training and verification phases because the model couldn't train when we only applied class weight during the verification phase. Therefore, we had to apply it in both phases to enable it to learn and train. This contrasts with our model, which was able to train without class weight ðŸ’ªðŸ¥‡."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "training_setup_cell",
        "outputId": "697a3618-57e1-43c9-f887-e6095c0effdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS WEIGHTS\n",
            "================================================================================\n",
            "Class weights statistics (alpha=0.5):\n",
            "  Min weight: 0.0387\n",
            "  Max weight: 4.2745\n",
            "  Mean weight: 1.0000\n",
            "  Sample counts - Min: 4, Max: 48746\n",
            "================================================================================\n",
            "\n",
            "Training setup complete!\n",
            "Optimizer: Adam (lr=0.005, weight_decay=0.04)\n",
            "Loss: CrossEntropyLoss with class weights\n"
          ]
        }
      ],
      "source": [
        "# Calculate class weights (EXACTLY like our HGNN)\n",
        "type_counts = torch.bincount(train_types, minlength=86).float()\n",
        "alpha = 0.5\n",
        "class_weights = 1.0 / torch.pow(type_counts.clamp(min=1.0), alpha)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASS WEIGHTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Class weights statistics (alpha={alpha}):\")\n",
        "print(f\"  Min weight: {class_weights.min().item():.4f}\")\n",
        "print(f\"  Max weight: {class_weights.max().item():.4f}\")\n",
        "print(f\"  Mean weight: {class_weights.mean().item():.4f}\")\n",
        "print(f\"  Sample counts - Min: {type_counts.min().int().item()}, Max: {type_counts.max().int().item()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=Learning_rate, weight_decay=Decay_weight)\n",
        "\n",
        "print(f\"\\nTraining setup complete!\")\n",
        "print(f\"Optimizer: Adam (lr={Learning_rate}, weight_decay={Decay_weight})\")\n",
        "print(f\"Loss: CrossEntropyLoss with class weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_eval"
      },
      "source": [
        "## 11. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "train_eval_cell"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, criterion, pairs, labels, edge_index, x, batch_size=128):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Shuffle training data\n",
        "    perm = torch.randperm(len(pairs))\n",
        "\n",
        "    for i in range(0, len(pairs), batch_size):\n",
        "        batch_idx = perm[i:i+batch_size]\n",
        "        batch_pairs = pairs[batch_idx]\n",
        "        batch_labels = labels[batch_idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x, edge_index, batch_pairs)\n",
        "        loss = criterion(logits, batch_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item() * len(batch_pairs)\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "        total += len(batch_pairs)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, pairs, labels, edge_index, x, criterion, class_weights, batch_size=128):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(0, len(pairs), batch_size):\n",
        "        batch_pairs = pairs[i:i+batch_size]\n",
        "        batch_labels = labels[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x, edge_index, batch_pairs)\n",
        "        loss = criterion(logits, batch_labels)\n",
        "\n",
        "        total_loss += loss.item() * len(batch_pairs)\n",
        "\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(pairs)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'f1_micro': f1_score(all_labels, all_preds, average='micro', zero_division=0),\n",
        "        'f1_macro': f1_score(all_labels, all_preds, average='macro', zero_division=0),\n",
        "        'precision': precision_score(all_labels, all_preds, average='micro', zero_division=0),\n",
        "        'recall': recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    }\n",
        "\n",
        "    return metrics, avg_loss\n",
        "\n",
        "# RAM usage calculator\n",
        "def calculate_ram_usage():\n",
        "    \"\"\"Calculate RAM usage in GB\"\"\"\n",
        "    import psutil\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 / 1024 / 1024\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpCX6o6Tn1X_"
      },
      "source": [
        "## 12. Train Model experment-0\n",
        "N_bit = 512,\n",
        "Hidden_dimensions = 128,\n",
        "Included_dimensions = 128,\n",
        "Dropout = 0.5,\n",
        "Batch_size = 128,\n",
        "Learning_rate = 0.005,     \n",
        "Decay_weight = 0.04,\n",
        "Number_Eproch = 500,\n",
        "Seeds = 42,\n",
        "alph=0.5,\n",
        "\n",
        "\n",
        "## belowe the train and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxOCEZ90nywH",
        "outputId": "f888bcbf-52fc-4c9c-876d-24460f649b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FULL TRAINING GCN+MLP (500 EPOCHS) - GPU ACCELERATED\n",
            "================================================================================\n",
            "RAM usage before training: 1.66 GB\n",
            "Device: cuda\n",
            "Training data: 153,489 pairs\n",
            "\n",
            "New best: Epoch 0 - Val Loss: 3.8220 (Time: 9.9s)\n",
            "Epoch 0: loss: 3.9123, val_loss: 3.8220 (best: 3.8220, patience: 0)\n",
            "New best: Epoch 3 - Val Loss: 3.8095 (Time: 10.0s)\n",
            "New best: Epoch 8 - Val Loss: 3.8042 (Time: 9.9s)\n",
            "Epoch 10: loss: 3.9060, val_loss: 3.9008 (best: 3.8042, patience: 2)\n",
            "New best: Epoch 13 - Val Loss: 3.7735 (Time: 10.0s)\n",
            "Epoch 20: loss: 3.9150, val_loss: 3.9197 (best: 3.7735, patience: 7)\n",
            "New best: Epoch 22 - Val Loss: 3.7620 (Time: 10.1s)\n",
            "Epoch 30: loss: 3.9027, val_loss: 3.7902 (best: 3.7620, patience: 8)\n",
            "Epoch 40: loss: 3.9123, val_loss: 3.8284 (best: 3.7620, patience: 18)\n",
            "Epoch 50: loss: 3.9076, val_loss: 3.7993 (best: 3.7620, patience: 28)\n",
            "Epoch 60: loss: 3.9070, val_loss: 3.7917 (best: 3.7620, patience: 38)\n",
            "\n",
            "Early stopping at epoch 62\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED\n",
            "================================================================================\n",
            "Best epoch: 22\n",
            "Best validation loss: 3.7620\n",
            "Total time: 10.49 minutes\n",
            "Average time per epoch: 10.0 seconds\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"FULL TRAINING GCN+MLP ({Number_Eproch} EPOCHS) - GPU ACCELERATED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Training data: {len(train_pairs):,} pairs\\n\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "patience =40\n",
        "patience_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(Number_Eproch):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, optimizer, criterion,\n",
        "        train_pairs, train_types,\n",
        "        edge_index, drug_features,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    # Validation phase\n",
        "    val_metrics, val_loss = evaluate(\n",
        "        model, val_pairs, val_types,\n",
        "        edge_index, drug_features,\n",
        "        criterion, class_weights,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), '/content/best_gcn_mlp_model.pth')\n",
        "        print(f\"New best: Epoch {epoch} - Val Loss: {val_loss:.4f} (Time: {epoch_time:.1f}s)\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: loss: {train_loss:.4f}, val_loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience_counter})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total time: {total_time/60:.2f} minutes\")\n",
        "print(f\"Average time per epoch: {total_time/max(epoch+1, 1):.1f} seconds\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed86P4pEnods"
      },
      "source": [
        "## 12. Train Model experment-1\n",
        "N_bit = 512,\n",
        "Hidden_dimensions = 128,\n",
        "Included_dimensions = 128,\n",
        "Dropout = 0.6,\n",
        "Batch_size = 128,\n",
        "Learning_rate = 0.05,     \n",
        "Decay_weight = 0.01,\n",
        "Number_Eproch = 500,\n",
        "Seeds = 42,\n",
        "alph=0.4,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77tEuazekE4v",
        "outputId": "c49e95fb-ae19-4a8f-c9b2-95beb1a4badb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FULL TRAINING GCN+MLP (500 EPOCHS) - GPU ACCELERATED\n",
            "================================================================================\n",
            "RAM usage before training: 1.65 GB\n",
            "Device: cuda\n",
            "Training data: 153,489 pairs\n",
            "\n",
            "New best: Epoch 0 - Val Loss: 3.6385 (Time: 12.1s)\n",
            "Epoch 0: loss: 3.7330, val_loss: 3.6385 (best: 3.6385, patience: 0)\n",
            "New best: Epoch 3 - Val Loss: 3.6048 (Time: 10.0s)\n",
            "Epoch 10: loss: 3.7646, val_loss: 3.6791 (best: 3.6048, patience: 7)\n",
            "Epoch 20: loss: 3.7380, val_loss: 3.6163 (best: 3.6048, patience: 17)\n",
            "Epoch 30: loss: 3.7869, val_loss: 3.6572 (best: 3.6048, patience: 27)\n",
            "Epoch 40: loss: 3.8045, val_loss: 3.6455 (best: 3.6048, patience: 37)\n",
            "\n",
            "Early stopping at epoch 43\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED\n",
            "================================================================================\n",
            "Best epoch: 3\n",
            "Best validation loss: 3.6048\n",
            "Total time: 7.38 minutes\n",
            "Average time per epoch: 10.1 seconds\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"FULL TRAINING GCN+MLP ({Number_Eproch} EPOCHS) - GPU ACCELERATED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Training data: {len(train_pairs):,} pairs\\n\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "patience =40\n",
        "patience_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(Number_Eproch):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, optimizer, criterion,\n",
        "        train_pairs, train_types,\n",
        "        edge_index, drug_features,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    # Validation phase\n",
        "    val_metrics, val_loss = evaluate(\n",
        "        model, val_pairs, val_types,\n",
        "        edge_index, drug_features,\n",
        "        criterion, class_weights,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), '/content/best_gcn_mlp_model.pth')\n",
        "        print(f\"New best: Epoch {epoch} - Val Loss: {val_loss:.4f} (Time: {epoch_time:.1f}s)\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: loss: {train_loss:.4f}, val_loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience_counter})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total time: {total_time/60:.2f} minutes\")\n",
        "print(f\"Average time per epoch: {total_time/max(epoch+1, 1):.1f} seconds\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train"
      },
      "source": [
        "## 12. Train Model experment-2 \n",
        "\n",
        "N_bit = 512,\n",
        "Hidden_dimensions = 128,\n",
        "Included_dimensions = 128,\n",
        "Dropout = 0.5,\n",
        "Batch_size = 128,\n",
        "Learning_rate = 0.01 ,      \n",
        "Decay_weight = 0.001,\n",
        "Number_Eproch = 500,\n",
        "Seeds = 42,\n",
        "alph=0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "train_cell",
        "outputId": "86e75e98-d561-4d9b-8c38-42991e752818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FULL TRAINING GCN+MLP (500 EPOCHS) - GPU ACCELERATED\n",
            "================================================================================\n",
            "RAM usage before training: 1.21 GB\n",
            "Device: cuda\n",
            "Training data: 153,489 pairs\n",
            "\n",
            "New best: Epoch 0 - Val Loss: 3.0629 (Time: 10.7s)\n",
            "Epoch 0: loss: 3.2326, val_loss: 3.0629 (best: 3.0629, patience: 0)\n",
            "New best: Epoch 6 - Val Loss: 3.0594 (Time: 9.9s)\n",
            "New best: Epoch 7 - Val Loss: 3.0308 (Time: 10.0s)\n",
            "New best: Epoch 9 - Val Loss: 3.0290 (Time: 10.0s)\n",
            "Epoch 10: loss: 3.1736, val_loss: 3.0915 (best: 3.0290, patience: 1)\n",
            "Epoch 20: loss: 3.3268, val_loss: 3.2967 (best: 3.0290, patience: 11)\n",
            "Epoch 30: loss: 3.3244, val_loss: 3.2929 (best: 3.0290, patience: 21)\n",
            "Epoch 40: loss: 3.3256, val_loss: 3.2925 (best: 3.0290, patience: 31)\n",
            "Epoch 50: loss: 3.3258, val_loss: 3.2932 (best: 3.0290, patience: 41)\n",
            "Epoch 60: loss: 3.3255, val_loss: 3.2984 (best: 3.0290, patience: 51)\n",
            "Epoch 70: loss: 3.3248, val_loss: 3.2909 (best: 3.0290, patience: 61)\n",
            "Epoch 80: loss: 3.3262, val_loss: 3.2899 (best: 3.0290, patience: 71)\n",
            "Epoch 90: loss: 3.3262, val_loss: 3.2941 (best: 3.0290, patience: 81)\n",
            "Epoch 100: loss: 3.3244, val_loss: 3.3037 (best: 3.0290, patience: 91)\n",
            "\n",
            "Early stopping at epoch 109\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED\n",
            "================================================================================\n",
            "Best epoch: 9\n",
            "Best validation loss: 3.0290\n",
            "Total time: 18.33 minutes\n",
            "Average time per epoch: 10.0 seconds\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"FULL TRAINING GCN+MLP ({Number_Eproch} EPOCHS) - GPU ACCELERATED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Training data: {len(train_pairs):,} pairs\\n\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "patience = 100\n",
        "patience_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(Number_Eproch):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, optimizer, criterion,\n",
        "        train_pairs, train_types,\n",
        "        edge_index, drug_features,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    # Validation phase\n",
        "    val_metrics, val_loss = evaluate(\n",
        "        model, val_pairs, val_types,\n",
        "        edge_index, drug_features,\n",
        "        criterion, class_weights,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), '/content/best_gcn_mlp_model.pth')\n",
        "        print(f\"New best: Epoch {epoch} - Val Loss: {val_loss:.4f} (Time: {epoch_time:.1f}s)\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: loss: {train_loss:.4f}, val_loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience_counter})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total time: {total_time/60:.2f} minutes\")\n",
        "print(f\"Average time per epoch: {total_time/max(epoch+1, 1):.1f} seconds\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJhOasxWwEa7"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), DATA_PATH + 'best_gcn_mlp_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test"
      },
      "source": [
        "## 13. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_cell"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('/content/best_gcn_mlp_model.pth'))\n",
        "\n",
        "# Evaluate on test set\n",
        "test_metrics, test_loss = evaluate(\n",
        "    model, test_pairs, test_types,\n",
        "    edge_index, drug_features,\n",
        "    criterion, class_weights,\n",
        "    batch_size=Batch_size\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL TEST SET RESULTS (GCN + MLP)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Test Loss:  {test_loss:.4f}\")\n",
        "print(f\"Accuracy:   {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"F1 (Micro): {test_metrics['f1_micro']:.4f}\")\n",
        "print(f\"F1 (Macro): {test_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Precision:  {test_metrics['precision']:.4f}\")\n",
        "print(f\"Recall:     {test_metrics['recall']:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save"
      },
      "source": [
        "## 15. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_cell"
      },
      "outputs": [],
      "source": [
        "SAVE_PATH =  DATA_PATH+\"result/\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), SAVE_PATH + 'gcn_mlp_morgan512_model.pth')\n",
        "\n",
        "# Save results\n",
        "import json\n",
        "results = {\n",
        "    'model': 'GCN+MLP',\n",
        "    'features': 'Morgan 512',\n",
        "    'aggregator': Aggregator,\n",
        "    'best_epoch': best_epoch,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'test_metrics': test_metrics,\n",
        "    'test_loss': test_loss,\n",
        "    'training_time_minutes': total_time / 60,\n",
        "    'hyperparameters': {\n",
        "        'hidden_dim': Hidden_dimensions,\n",
        "        'dropout': Dropout,\n",
        "        'learning_rate': Learning_rate,\n",
        "        'weight_decay': Decay_weight,\n",
        "        'batch_size': Batch_size,\n",
        "        'epochs': Number_Eproch\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(SAVE_PATH + 'gcn_mlp_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nResults saved to: {SAVE_PATH}\")\n",
        "print(\"   - gcn_mlp_morgan512_model.pth\")\n",
        "print(\"   - gcn_mlp_results.json\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m49xntGwxd3D",
        "outputId": "d66c0f2d-1c95-4910-d95f-346130bcb98d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torch-geometric scikit-learn rdkit pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAbl62vZxd3E",
        "outputId": "ce718163-c739-446d-f366-9ab0dfb71e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq9GRt9pxd3E",
        "outputId": "1741e995-39e3-486b-9757-db4c641bfbe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# RDKit for SMILES fingerprints\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MACCSkeys\n",
        "from rdkit import DataStructs\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4aTWkZpxd3E",
        "outputId": "5897cce5-55f3-4d6b-82b3-3d0b3dbce2a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from Google Drive...\n",
            "\n",
            "Train data preview:\n",
            "  Drug1_ID Drug2_ID  Label\n",
            "0  DB01097  DB05219     47\n",
            "1  DB00547  DB00784     49\n",
            "2  DB00623  DB01365     61\n",
            "3  DB00328  DB09027     73\n",
            "4  DB00742  DB00955     57\n",
            "\n",
            "Columns: ['Drug1_ID', 'Drug2_ID', 'Label']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/GAT/data/'\n",
        "DATA_PATH_SMILES = '/content/drive/MyDrive/GAT/'\n",
        "print(\"Loading data from Google Drive...\")\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv(DATA_PATH + 'train_positive.csv')\n",
        "val_df = pd.read_csv(DATA_PATH + 'val_positive.csv')\n",
        "test_df = pd.read_csv(DATA_PATH + 'test_positive.csv')\n",
        "\n",
        "print(f\"\\nTrain data preview:\")\n",
        "print(train_df.head())\n",
        "print(f\"\\nColumns: {train_df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jihR049xd3F",
        "outputId": "ca98a954-d5a5-47f3-a380-e9cf6f5eadf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Drug SMILES loaded: 1709 drugs\n",
            "  DrugBank_ID                                             SMILES\n",
            "0     DB00006  CC[C@H](C)[C@H](NC(=O)[C@H](CCC(O)=O)NC(=O)[C@...\n",
            "1     DB00014  CC(C)C[C@H](NC(=O)[C@@H](COC(C)(C)C)NC(=O)[C@H...\n",
            "2     DB00027  CC(C)C[C@@H](NC(=O)CNC(=O)[C@@H](NC=O)C(C)C)C(...\n",
            "3     DB00035  NC(=O)CC[C@@H]1NC(=O)[C@H](CC2=CC=CC=C2)NC(=O)...\n",
            "4     DB00080  CCCCCCCCCC(=O)N[C@@H](CC1=CNC2=C1C=CC=C2)C(=O)...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drug_smiles_df = pd.read_csv(DATA_PATH_SMILES + 'Drugs_with_Smiles.csv')\n",
        "\n",
        "print(f\"\\nDrug SMILES loaded: {len(drug_smiles_df)} drugs\")\n",
        "print(drug_smiles_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We use Morgan Fingerprint (2, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umjzeVDrxd3F",
        "outputId": "12bbb33e-dc1d-4bd5-d574-1f1955fc2603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üî¨ Extracting morgan fingerprints...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'drug_smiles_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müî¨ Extracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINGERPRINT_TYPE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fingerprints...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Extract fingerprints for all drugs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m drug_id_to_smiles = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mdrug_smiles_df\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mDrugBank_ID\u001b[39m\u001b[33m'\u001b[39m], drug_smiles_df[\u001b[33m'\u001b[39m\u001b[33mSMILES\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Get all unique drugs from train/val/test\u001b[39;00m\n\u001b[32m     43\u001b[39m all_drug_ids = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[31mNameError\u001b[39m: name 'drug_smiles_df' is not defined"
          ]
        }
      ],
      "source": [
        "# fingerprint type\n",
        "FINGERPRINT_TYPE = 'morgan'\n",
        "N_BITS = 512\n",
        "\n",
        "\n",
        "def extract_fingerprint(smiles, fp_type='morgan', radius=2, n_bits=512):\n",
        "\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "        if mol is None:\n",
        "            print(f\"Warning: Invalid SMILES: {smiles}\")\n",
        "            # Return zero vector for invalid SMILES\n",
        "            return np.zeros(n_bits if fp_type == 'morgan' else 166)\n",
        "\n",
        "        if fp_type == 'morgan':\n",
        "            # Morgan fingerprint (ECFP-like)\n",
        "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=n_bits)\n",
        "            arr = np.zeros((n_bits,))\n",
        "        elif fp_type == 'maccs':\n",
        "            # MACCS keys (166-bit)\n",
        "            fp = MACCSkeys.GenMACCSKeys(mol)\n",
        "            arr = np.zeros((166,))\n",
        "        else:\n",
        "            raise ValueError(\"fp_type must be 'morgan' or 'maccs'\")\n",
        "\n",
        "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "        return arr\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing SMILES {smiles}: {e}\")\n",
        "        return np.zeros(n_bits if fp_type == 'morgan' else 166)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nüî¨ Extracting {FINGERPRINT_TYPE} fingerprints...\")\n",
        "\n",
        "# Extract fingerprints for all drugs\n",
        "drug_id_to_smiles = dict(zip(drug_smiles_df['DrugBank_ID'], drug_smiles_df['SMILES']))\n",
        "\n",
        "# Get all unique drugs from train/val/test\n",
        "all_drug_ids = set()\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    all_drug_ids.update(df['Drug1_ID'].values)\n",
        "    all_drug_ids.update(df['Drug2_ID'].values)\n",
        "\n",
        "all_drug_ids = sorted(list(all_drug_ids))\n",
        "drug_to_idx = {drug_id: idx for idx, drug_id in enumerate(all_drug_ids)}\n",
        "idx_to_drug = {idx: drug_id for drug_id, idx in drug_to_idx.items()}\n",
        "\n",
        "print(f\"Total unique drugs: {len(all_drug_ids)}\")\n",
        "\n",
        "# Extract fingerprints\n",
        "fingerprints = []\n",
        "for drug_id in tqdm(all_drug_ids, desc=\"Extracting fingerprints\"):\n",
        "    smiles = drug_id_to_smiles.get(drug_id, None)\n",
        "    if smiles is None:\n",
        "        print(f\"Warning: No SMILES found for drug {drug_id}\")\n",
        "        fp = np.zeros(N_BITS if FINGERPRINT_TYPE == 'morgan' else 166)\n",
        "    else:\n",
        "        fp = extract_fingerprint(smiles, fp_type=FINGERPRINT_TYPE, n_bits=N_BITS)\n",
        "    fingerprints.append(fp)\n",
        "\n",
        "# Convert to tensor\n",
        "drug_features = torch.FloatTensor(np.array(fingerprints))\n",
        "\n",
        "print(f\"\\nDrug features extracted:\")\n",
        "print(f\"   - Shape: {drug_features.shape}\")\n",
        "print(f\"   - Type: {FINGERPRINT_TYPE} fingerprints\")\n",
        "print(f\"   - This is a FAIR baseline (not using our chemical embeddings!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfkVuSSBxd3F",
        "outputId": "476dd4db-3be7-4714-b578-9efff58daacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Converting labels to 0-based indexing for PyTorch...\n",
            "Original label range: 1 to 86\n",
            "Original label range: 1 to 86\n",
            "‚úÖ Converted labels from 1-86 to 0-85\n",
            "Final label range: 0 to 85\n",
            "Converted label range: 0 to 85\n",
            "Number of classes: 86 (0-85)\n",
            "Final num_types: 86\n",
            "\n",
            "Dataset Statistics:\n",
            "   - Drugs: 1709\n",
            "   - Feature dimension: 512\n",
            "   - Interaction types: 86 (0-85)\n",
            "   - Train pairs: 153489\n",
            "   - Val pairs: 19188\n",
            "   - Test pairs: 19200\n"
          ]
        }
      ],
      "source": [
        "# Convert drug pairs to indices and get types\n",
        "def df_to_tensors(df, drug_to_idx):\n",
        "    pairs = []\n",
        "    types = []\n",
        "    for _, row in df.iterrows():\n",
        "        drug1_idx = drug_to_idx[row['Drug1_ID']]\n",
        "        drug2_idx = drug_to_idx[row['Drug2_ID']]\n",
        "        interaction_type = int(row['Label'])\n",
        "\n",
        "        pairs.append([drug1_idx, drug2_idx])\n",
        "        types.append(interaction_type)\n",
        "\n",
        "    return torch.tensor(pairs, dtype=torch.long), torch.tensor(types, dtype=torch.long)\n",
        "\n",
        "train_pairs, train_types = df_to_tensors(train_df, drug_to_idx)\n",
        "val_pairs, val_types = df_to_tensors(val_df, drug_to_idx)\n",
        "test_pairs, test_types = df_to_tensors(test_df, drug_to_idx)\n",
        "\n",
        "# ADD THIS CONVERSION:\n",
        "print(\"\\nConverting labels to 0-based indexing for PyTorch...\")\n",
        "print(f\"Original label range: {train_types.min().item()} to {train_types.max().item()}\")\n",
        "\n",
        "print(f\"Original label range: {train_types.min()} to {train_types.max()}\")\n",
        "\n",
        "# If labels are 1-86, convert to 0-85\n",
        "if train_types.min() == 1:\n",
        "    train_types = train_types - 1\n",
        "    val_types = val_types - 1\n",
        "    test_types = test_types - 1\n",
        "    print(f\"‚úÖ Converted labels from 1-86 to 0-85\")\n",
        "else:\n",
        "    print(\"‚úÖ Labels already 0-based\")\n",
        "\n",
        "print(f\"Final label range: {train_types.min()} to {train_types.max()}\")\n",
        "\n",
        "print(f\"Converted label range: {train_types.min().item()} to {train_types.max().item()}\")\n",
        "print(f\"Number of classes: {len(torch.unique(train_types))} (0-{train_types.max().item()})\")\n",
        "\n",
        "# Update num_types to match\n",
        "num_types = len(torch.unique(train_types))\n",
        "print(f\"Final num_types: {num_types}\")\n",
        "\n",
        "# Get dimensions\n",
        "num_drugs = len(drug_to_idx)\n",
        "num_types = len(torch.unique(train_types))\n",
        "feature_dim = drug_features.shape[1]\n",
        "\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"   - Drugs: {num_drugs}\")\n",
        "print(f\"   - Feature dimension: {feature_dim}\")\n",
        "num_types = len(torch.unique(train_types))  # Should be 86\n",
        "print(f\"   - Interaction types: {num_types} (0-{num_types-1})\")\n",
        "print(f\"   - Train pairs: {len(train_pairs)}\")\n",
        "print(f\"   - Val pairs: {len(val_pairs)}\")\n",
        "print(f\"   - Test pairs: {len(test_pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqZ9V0mJxd3G",
        "outputId": "f26a0a5d-baaa-4c02-b3ad-eee8b6c81caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Graph Statistics:\n",
            "   - Nodes: 1709\n",
            "   - Edges: 306978\n",
            "   - NO TYPE INFORMATION in graph!\n"
          ]
        }
      ],
      "source": [
        "def build_graph(train_pairs, num_drugs):\n",
        "    edges = set()\n",
        "\n",
        "    for pair in train_pairs:\n",
        "        drug_i = int(pair[0])\n",
        "        drug_j = int(pair[1])\n",
        "\n",
        "        # Add both directions (undirected graph)\n",
        "        edges.add((drug_i, drug_j))\n",
        "        edges.add((drug_j, drug_i))\n",
        "\n",
        "    # Convert to tensor\n",
        "    edge_list = list(edges)\n",
        "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    print(f\"\\nGraph Statistics:\")\n",
        "    print(f\"   - Nodes: {num_drugs}\")\n",
        "    print(f\"   - Edges: {edge_index.shape[1]}\")\n",
        "    print(f\"   - NO TYPE INFORMATION in graph!\")\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "# Build the graph\n",
        "edge_index = build_graph(train_pairs, num_drugs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CktOHW9yxd3G",
        "outputId": "81e86ea7-da03-49d8-cbc5-a6fe37687d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model classes defined\n"
          ]
        }
      ],
      "source": [
        "class GAT_Encoder(nn.Module):\n",
        "    \"\"\"SINGLE LAYER GAT Encoder - to match our HGNN\"\"\"\n",
        "    def __init__(self, in_features, hidden_dim, out_dim, num_heads=4, dropout=0.3):\n",
        "        super(GAT_Encoder, self).__init__()\n",
        "\n",
        "        # SINGLE LAYER only - matching our HGNN\n",
        "        self.gat1 = GATConv(in_features, out_dim, heads=num_heads, dropout=dropout, concat=True)\n",
        "\n",
        "        # Projection to get the right output dimension\n",
        "        self.projection = nn.Linear(out_dim * num_heads, out_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Single GAT layer\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.projection(x)  # Project to desired dimension\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLP_Decoder(nn.Module):\n",
        "    \"\"\"Simplified MLP Decoder - closer to our MLPPredictor\"\"\"\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_types, dropout=0.3):\n",
        "        super(MLP_Decoder, self).__init__()\n",
        "\n",
        "        # Make sure num_types is 87\n",
        "        print(f\"MLP Decoder initialized with {num_types} classes\")\n",
        "\n",
        "        input_dim = embedding_dim * 2  # Concatenated pair\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_types)  # Should output 87 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, drug_i_emb, drug_j_emb):\n",
        "        pair_emb = torch.cat([drug_i_emb, drug_j_emb], dim=1)\n",
        "        logits = self.mlp(pair_emb)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class GAT_MLP_Model(nn.Module):\n",
        "    \"\"\"Complete GAT + MLP Model\"\"\"\n",
        "    def __init__(self, in_features, hidden_dim, embedding_dim, num_types, num_heads=4, dropout=0.3):\n",
        "        super(GAT_MLP_Model, self).__init__()\n",
        "\n",
        "        self.encoder = GAT_Encoder(in_features, hidden_dim, embedding_dim, num_heads, dropout)\n",
        "        self.decoder = MLP_Decoder(embedding_dim, hidden_dim, num_types, dropout)\n",
        "\n",
        "    def forward(self, x, edge_index, drug_i_idx, drug_j_idx):\n",
        "        # Encode all drugs\n",
        "        drug_embeddings = self.encoder(x, edge_index)\n",
        "\n",
        "        # Get pair embeddings\n",
        "        drug_i_emb = drug_embeddings[drug_i_idx]\n",
        "        drug_j_emb = drug_embeddings[drug_j_idx]\n",
        "\n",
        "        # Predict type\n",
        "        logits = self.decoder(drug_i_emb, drug_j_emb)\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"Model classes defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_HVVXt6xd3H"
      },
      "outputs": [],
      "source": [
        "# Matching our HGNN config\n",
        "HIDDEN_DIM = 128          # Same as our 'hidden_units': 128\n",
        "EMBEDDING_DIM = 128       # Same as hidden for consistency\n",
        "NUM_HEADS =2              # Standard for GAT\n",
        "DROPOUT = 0.5             # Same as our 'dropout': 0.5\n",
        "BATCH_SIZE = 128          # Standard batch size\n",
        "LEARNING_RATE = 0.005     # Same as our 'learning_rate': 0.005\n",
        "WEIGHT_DECAY = 0.001       # Same as our 'weight_decay': 0.0\n",
        "NUM_EPOCHS = 200          # Same as our epochs\n",
        "SEED = 42                 # Same as our 'training_seed': 42\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "print(\"Hyperparameters (matched to HGNN metabolic):\")\n",
        "print(f\"   - Hidden dim: {HIDDEN_DIM}\")\n",
        "print(f\"   - Embedding dim: {EMBEDDING_DIM}\")\n",
        "print(f\"   - Dropout: {DROPOUT}\")\n",
        "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   - Weight decay: {WEIGHT_DECAY}\")\n",
        "print(f\"   - Seed: {SEED}\")\n",
        "\n",
        "model = GAT_MLP_Model(\n",
        "    in_features=512,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_types=num_types,  # Should be 87 now\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "# Move data to device\n",
        "drug_features = drug_features.to(device)\n",
        "edge_index = edge_index.to(device)\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"\\nModel initialized on {device}\")\n",
        "print(f\"   - Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmS7E83cVlOA",
        "outputId": "50c6eed0-c06a-4bc3-9b0f-2bbd3418abee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing one forward pass...\n",
            "Logits shape: torch.Size([2, 86])\n",
            "Predictions: tensor([61, 61], device='cuda:0')\n",
            "True labels: tensor([46, 48], device='cuda:0')\n",
            "Loss: 4.4905\n",
            "‚úÖ Forward pass successful!\n",
            "\n",
            "üìä Data sizes:\n",
            "Train pairs: torch.Size([153489, 2])\n",
            "Train types: torch.Size([153489])\n",
            "Drug features: torch.Size([1709, 512])\n",
            "Edge index: torch.Size([2, 306978])\n"
          ]
        }
      ],
      "source": [
        "# Debug: Test one forward pass\n",
        "print(\"Testing one forward pass...\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_i = train_pairs[:2, 0].to(device)\n",
        "    test_j = train_pairs[:2, 1].to(device)\n",
        "    test_types_debug = train_types[:2].to(device)\n",
        "\n",
        "    logits = model(drug_features, edge_index, test_i, test_j)\n",
        "    print(f\"Logits shape: {logits.shape}\")  # Should be [2, 86]\n",
        "    print(f\"Predictions: {torch.argmax(logits, dim=1)}\")\n",
        "    print(f\"True labels: {test_types_debug}\")\n",
        "    print(f\"Loss: {criterion(logits, test_types_debug):.4f}\")\n",
        "print(\"‚úÖ Forward pass successful!\")\n",
        "\n",
        "# Check data sizes\n",
        "print(f\"\\nüìä Data sizes:\")\n",
        "print(f\"Train pairs: {train_pairs.shape}\")\n",
        "print(f\"Train types: {train_types.shape}\")\n",
        "print(f\"Drug features: {drug_features.shape}\")\n",
        "print(f\"Edge index: {edge_index.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jiuqd1Fkk86J"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def calculate_ram_usage():\n",
        "    \"\"\"Calculate RAM usage in GB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss / (1024 ** 3)  # Convert to GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLgzaAHrpMoy",
        "outputId": "400cea00-98df-4d72-c9e8-88e0e53a31cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training functions ready with class weights\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, optimizer, criterion, pairs, types, class_weights):\n",
        "    \"\"\"Train for one epoch with class weights\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Shuffle data\n",
        "    indices = torch.randperm(len(pairs))\n",
        "    pairs = pairs[indices]\n",
        "    types = types[indices]\n",
        "\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Batch training\n",
        "    num_batches = (len(pairs) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * BATCH_SIZE\n",
        "        end_idx = min((i + 1) * BATCH_SIZE, len(pairs))\n",
        "\n",
        "        batch_pairs = pairs[start_idx:end_idx]\n",
        "        batch_types = types[start_idx:end_idx]\n",
        "\n",
        "        batch_i = batch_pairs[:, 0].to(device)\n",
        "        batch_j = batch_pairs[:, 1].to(device)\n",
        "        batch_types = batch_types.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        logits = model(drug_features, edge_index, batch_i, batch_j)\n",
        "\n",
        "        # Apply class weights to loss\n",
        "        loss = F.cross_entropy(logits, batch_types, weight=class_weights)\n",
        "\n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Track predictions\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_types.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def evaluate(model, pairs, types, criterion, class_weights):\n",
        "    \"\"\"Evaluate model with class weights\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        num_batches = (len(pairs) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * BATCH_SIZE\n",
        "            end_idx = min((i + 1) * BATCH_SIZE, len(pairs))\n",
        "\n",
        "            batch_pairs = pairs[start_idx:end_idx]\n",
        "            batch_types = types[start_idx:end_idx]\n",
        "\n",
        "            batch_i = batch_pairs[:, 0].to(device)\n",
        "            batch_j = batch_pairs[:, 1].to(device)\n",
        "            batch_types = batch_types.to(device)\n",
        "\n",
        "            logits = model(drug_features, edge_index, batch_i, batch_j)\n",
        "\n",
        "            # Apply class weights to loss\n",
        "            loss = F.cross_entropy(logits, batch_types, weight=class_weights)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch_types.cpu().numpy())\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    precision = precision_score(all_labels, all_preds, average='micro')\n",
        "    recall = recall_score(all_labels, all_preds, average='micro')\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_micro': f1_micro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "    return metrics, avg_loss\n",
        "\n",
        "print(\"Training functions ready with class weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZydGWiHdpN2B",
        "outputId": "4b6a1447-29bb-4921-a694-7333dde9cdcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FULL TRAINING GAT+MLP (200 EPOCHS) - GPU ACCELERATED\n",
            "================================================================================\n",
            "\n",
            "Class weights statistics (alpha=0.3):\n",
            "  Min weight: 0.1566\n",
            "  Max weight: 2.6340\n",
            "  Mean weight: 1.0000\n",
            "  Sample counts - Min: 4, Max: 48746\n",
            "\n",
            "RAM usage before training: 1.63 GB\n",
            "Device: cuda (GPU)\n",
            "Training data: 153,489 pairs\n",
            "New best: Epoch 0 - Val Loss: 1.7177 (Time: 27.0s)\n",
            "Epoch 0: loss: 2.4016, val_loss: 1.7177 (best: 1.7177, patience: 0)\n",
            "New best: Epoch 1 - Val Loss: 1.5461 (Time: 26.7s)\n",
            "New best: Epoch 2 - Val Loss: 1.5451 (Time: 26.9s)\n",
            "New best: Epoch 3 - Val Loss: 1.4399 (Time: 26.5s)\n",
            "New best: Epoch 6 - Val Loss: 1.3788 (Time: 26.5s)\n",
            "New best: Epoch 8 - Val Loss: 1.3438 (Time: 26.5s)\n",
            "Epoch 10: loss: 1.7732, val_loss: 1.3907 (best: 1.3438, patience: 2)\n",
            "New best: Epoch 15 - Val Loss: 1.3098 (Time: 26.7s)\n",
            "Epoch 20: loss: 1.7475, val_loss: 1.3489 (best: 1.3098, patience: 5)\n",
            "New best: Epoch 21 - Val Loss: 1.3028 (Time: 26.5s)\n",
            "Epoch 30: loss: 1.7565, val_loss: 1.3337 (best: 1.3028, patience: 9)\n",
            "New best: Epoch 31 - Val Loss: 1.2979 (Time: 26.5s)\n",
            "New best: Epoch 36 - Val Loss: 1.2904 (Time: 26.6s)\n",
            "Epoch 40: loss: 1.7309, val_loss: 1.2946 (best: 1.2904, patience: 4)\n",
            "Epoch 50: loss: 1.7275, val_loss: 1.3262 (best: 1.2904, patience: 14)\n",
            "New best: Epoch 54 - Val Loss: 1.2830 (Time: 27.2s)\n",
            "Epoch 60: loss: 1.7337, val_loss: 1.4446 (best: 1.2830, patience: 6)\n",
            "Epoch 70: loss: 1.7293, val_loss: 1.3136 (best: 1.2830, patience: 16)\n",
            "New best: Epoch 79 - Val Loss: 1.2339 (Time: 26.6s)\n",
            "Epoch 80: loss: 1.7315, val_loss: 1.3525 (best: 1.2339, patience: 1)\n",
            "Epoch 90: loss: 1.7334, val_loss: 1.3154 (best: 1.2339, patience: 11)\n",
            "Epoch 100: loss: 1.7314, val_loss: 1.2984 (best: 1.2339, patience: 21)\n",
            "Epoch 110: loss: 1.7492, val_loss: 1.3532 (best: 1.2339, patience: 31)\n",
            "Epoch 120: loss: 1.7460, val_loss: 1.3093 (best: 1.2339, patience: 41)\n",
            "Epoch 130: loss: 1.7323, val_loss: 1.3562 (best: 1.2339, patience: 51)\n",
            "Epoch 140: loss: 1.7263, val_loss: 1.3616 (best: 1.2339, patience: 61)\n",
            "Epoch 150: loss: 1.7272, val_loss: 1.3520 (best: 1.2339, patience: 71)\n",
            "Epoch 160: loss: 1.7314, val_loss: 1.3114 (best: 1.2339, patience: 81)\n",
            "Epoch 170: loss: 1.7303, val_loss: 1.3251 (best: 1.2339, patience: 91)\n",
            "Early stopping at epoch 179\n",
            "\n",
            "Best epoch: 79, Best val loss: 1.2339\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL TRAINING GAT+MLP (200 EPOCHS) - GPU ACCELERATED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate class weights (EXACTLY like HGNN)\n",
        "type_counts = torch.bincount(train_types, minlength=86).float()\n",
        "alpha = 0.3\n",
        "class_weights = 1.0 / torch.pow(type_counts.clamp(min=1.0), alpha)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "print(f\"\\nClass weights statistics (alpha={alpha}):\")\n",
        "print(f\"  Min weight: {class_weights.min():.4f}\")\n",
        "print(f\"  Max weight: {class_weights.max():.4f}\")\n",
        "print(f\"  Mean weight: {class_weights.mean():.4f}\")\n",
        "print(f\"  Sample counts - Min: {type_counts.min():.0f}, Max: {type_counts.max():.0f}\")\n",
        "\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"\\nRAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"Device: {device} (GPU)\")\n",
        "print(f\"Training data: {len(train_pairs):,} pairs\")\n",
        "\n",
        "best_val_loss = 1e10\n",
        "patience = 100\n",
        "patience_counter = 0\n",
        "best_epoch = 0\n",
        "start_time = time.time()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, optimizer, criterion, train_pairs, train_types, class_weights)\n",
        "\n",
        "    # Validation\n",
        "    val_metrics, val_loss = evaluate(model, val_pairs, val_types, criterion, class_weights)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # Early stopping based on VALIDATION LOSS\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), '/content/best_gat_mlp_model.pth')\n",
        "        print(f\"New best: Epoch {epoch} - Val Loss: {val_loss:.4f} (Time: {epoch_time:.1f}s)\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: loss: {train_loss:.4f}, val_loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience_counter})\")\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nBest epoch: {best_epoch}, Best val loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYkNcd3VMeWd",
        "outputId": "da549363-f169-4ca4-dd1c-542525c7e222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Best model saved successfully!\n",
            "üìÅ Location: /content/drive/MyDrive/GAT/data/best_gat_mlp_model.pth\n",
            "üìä Best epoch: 79\n",
            "üìä Best val loss: 1.2339\n",
            "‚úÖ Checkpoint saved successfully!\n",
            "üìÅ Location: /content/drive/MyDrive/GAT/data/checkpoints/checkpoint_epoch_200.pth\n",
            "üìä Epoch: 200\n",
            "\n",
            "üí° You can now continue training from epoch 200 to 500\n"
          ]
        }
      ],
      "source": [
        "# Save the best model to our DATA_PATH\n",
        "best_model_save_path = DATA_PATH + 'best_gat_mlp_model.pth'\n",
        "\n",
        "# Save complete model checkpoint\n",
        "torch.save({\n",
        "    'epoch': best_epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'val_loss': best_val_loss,\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'hyperparameters': {\n",
        "        'HIDDEN_DIM': HIDDEN_DIM,\n",
        "        'EMBEDDING_DIM': EMBEDDING_DIM,\n",
        "        'NUM_HEADS': NUM_HEADS,\n",
        "        'DROPOUT': DROPOUT,\n",
        "        'LEARNING_RATE': LEARNING_RATE,\n",
        "        'WEIGHT_DECAY': WEIGHT_DECAY,\n",
        "        'BATCH_SIZE': BATCH_SIZE,\n",
        "    }\n",
        "}, best_model_save_path)\n",
        "\n",
        "print(\"‚úÖ Best model saved successfully!\")\n",
        "print(f\"üìÅ Location: {best_model_save_path}\")\n",
        "print(f\"üìä Best epoch: {best_epoch}\")\n",
        "print(f\"üìä Best val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Run this cell AFTER training finishes to create checkpoint at current epoch\n",
        "# ============================================================================\n",
        "\n",
        "# Create checkpoint directory\n",
        "CHECKPOINT_DIR = DATA_PATH + 'checkpoints/'\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Save checkpoint at current epoch (e.g., epoch 200)\n",
        "current_epoch = NUM_EPOCHS  # This will be 200 if you trained for 200 epochs\n",
        "checkpoint_save_path = CHECKPOINT_DIR + f'checkpoint_epoch_{current_epoch}.pth'\n",
        "\n",
        "torch.save({\n",
        "    'epoch': current_epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'class_weights': class_weights,\n",
        "    'hyperparameters': {\n",
        "        'HIDDEN_DIM': HIDDEN_DIM,\n",
        "        'EMBEDDING_DIM': EMBEDDING_DIM,\n",
        "        'NUM_HEADS': NUM_HEADS,\n",
        "        'DROPOUT': DROPOUT,\n",
        "        'LEARNING_RATE': LEARNING_RATE,\n",
        "        'WEIGHT_DECAY': WEIGHT_DECAY,\n",
        "        'BATCH_SIZE': BATCH_SIZE,\n",
        "        'NUM_EPOCHS': NUM_EPOCHS,\n",
        "        'SEED': SEED,\n",
        "    }\n",
        "}, checkpoint_save_path)\n",
        "\n",
        "print(\"‚úÖ Checkpoint saved successfully!\")\n",
        "print(f\"üìÅ Location: {checkpoint_save_path}\")\n",
        "print(f\"üìä Epoch: {current_epoch}\")\n",
        "print(f\"\\nüí° You can now continue training from epoch {current_epoch} to 500\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVjoizgDxd3I"
      },
      "source": [
        "## 9. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-dOosGlxd3I",
        "outputId": "b841f380-e0fe-4307-94a0-f719cd2511dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üìä FINAL TEST SET RESULTS (GAT + MLP)\n",
            "============================================================\n",
            "Test Loss:  1.2492\n",
            "Accuracy:   0.6444\n",
            "F1 (Micro): 0.6444\n",
            "F1 (Macro): 0.3840\n",
            "Precision:  0.6444\n",
            "Recall:     0.6444\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('/content/best_gat_mlp_model.pth'))\n",
        "\n",
        "# Evaluate on test set\n",
        "test_metrics, test_loss = evaluate(model, test_pairs, test_types, criterion, class_weights)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä FINAL TEST SET RESULTS (GAT + MLP)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Test Loss:  {test_loss:.4f}\")\n",
        "print(f\"Accuracy:   {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"F1 (Micro): {test_metrics['f1_micro']:.4f}\")\n",
        "print(f\"F1 (Macro): {test_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Precision:  {test_metrics['precision']:.4f}\")\n",
        "print(f\"Recall:     {test_metrics['recall']:.4f}\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torch-geometric -q\n",
        "!pip install rdkit -q\n",
        "print(\"âœ… Installation complete\")"
      ],
      "metadata": {
        "id": "-iZ-yVGE9A3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    accuracy_score, f1_score, precision_score, recall_score\n",
        ")\n",
        "import time"
      ],
      "metadata": {
        "id": "fnkXmFpR9D4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxavuI289GhY",
        "outputId": "d3cfb636-b30d-457a-bf68-597c192ffa6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/MLHygnn/BaseLine/GCN_Chemical/'\n",
        "SMILES_FILE = '/content/drive/MyDrive/MLHygnn/BaseLine/Drugs_with_Smiles.csv'\n",
        "\n",
        "config = {\n",
        "    'learning_rate': 0.005,\n",
        "    'hidden_units': 128,\n",
        "    'dropout': 0.5,\n",
        "    'num_epochs': 500,\n",
        "    'patience': 200,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\" Using device: {device}\")"
      ],
      "metadata": {
        "id": "rIx3d0OB9JlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LOAD ALL DATA SPLITS (SAME AS YOUR MODEL)\n",
        "# ============================================================================\n",
        "print(\"Loading data splits...\")\n",
        "\n",
        "train_pos = pd.read_csv(f'{BASE_PATH}data/train_postive.csv')\n",
        "train_neg = pd.read_csv(f'{BASE_PATH}data/train_negatives.csv')\n",
        "test_pos = pd.read_csv(f'{BASE_PATH}data/test_postive.csv')\n",
        "test_neg = pd.read_csv(f'{BASE_PATH}data/test_negatives.csv')\n",
        "val_pos = pd.read_csv(f'{BASE_PATH}data/val_postive.csv')\n",
        "val_neg = pd.read_csv(f'{BASE_PATH}data/val_negatives.csv')\n",
        "\n",
        "smiles_df = pd.read_csv(SMILES_FILE)\n",
        "smiles_dict = dict(zip(smiles_df['DrugBank_ID'], smiles_df['SMILES']))\n",
        "\n",
        "print(f\" Data loaded:\")\n",
        "print(f\"   Train positive: {len(train_pos)}, negative: {len(train_neg)}\")\n",
        "print(f\"   Val positive: {len(val_pos)}, negative: {len(val_neg)}\")\n",
        "print(f\"   Test positive: {len(test_pos)}, negative: {len(test_neg)}\")"
      ],
      "metadata": {
        "id": "0i7WnveECOPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# CREATE DRUG-TO-INDEX MAPPING AND NODE FEATURES\n",
        "# ============================================================================\n",
        "all_drugs = list(smiles_dict.keys())\n",
        "drug_to_idx = {drug: idx for idx, drug in enumerate(all_drugs)}\n",
        "num_drugs = len(all_drugs)\n",
        "\n",
        "print(f\"Total drugs: {num_drugs}\")\n",
        "\n",
        "# FAIR COMPARISON: Use one-hot identity features like your HyGNN\n",
        "x = torch.eye(num_drugs, dtype=torch.float).to(device)\n",
        "print(f\" Node features (one-hot): {x.shape}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE GRAPH EDGES FOR ALL SPLITS\n",
        "# ============================================================================\n",
        "def create_edge_index_and_labels(pos_df, neg_df, drug_mapping, directed=False):\n",
        "    \"\"\"Create edge index and labels from positive and negative samples\"\"\"\n",
        "    edges = []\n",
        "    labels = []\n",
        "\n",
        "    # Positive edges\n",
        "    for _, row in pos_df.iterrows():\n",
        "        src = drug_mapping[row['Drug1_ID']]\n",
        "        dst = drug_mapping[row['Drug2_ID']]\n",
        "        edges.append([src, dst])\n",
        "        labels.append(1)\n",
        "        if directed:  # Add reverse for undirected graph\n",
        "            edges.append([dst, src])\n",
        "            labels.append(1)\n",
        "\n",
        "    # Negative edges\n",
        "    for _, row in neg_df.iterrows():\n",
        "        src = drug_mapping[row['Drug1_ID']]\n",
        "        dst = drug_mapping[row['Drug2_ID']]\n",
        "        edges.append([src, dst])\n",
        "        labels.append(0)\n",
        "        if directed:\n",
        "            edges.append([dst, src])\n",
        "            labels.append(0)\n",
        "\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "    return edge_index, labels\n",
        "\n",
        "print(\"Building graphs...\")\n",
        "\n",
        "# Training graph (undirected - same as your approach)\n",
        "train_edge_index, train_labels = create_edge_index_and_labels(\n",
        "    train_pos, train_neg, drug_to_idx, directed=True\n",
        ")\n",
        "\n",
        "# Validation graph (directed for evaluation)\n",
        "val_edge_index, val_labels = create_edge_index_and_labels(\n",
        "    val_pos, val_neg, drug_to_idx, directed=False\n",
        ")\n",
        "\n",
        "# Test graph (directed for evaluation)\n",
        "test_edge_index, test_labels = create_edge_index_and_labels(\n",
        "    test_pos, test_neg, drug_to_idx, directed=False\n",
        ")\n",
        "\n",
        "print(f\"âœ… Graph construction complete:\")\n",
        "print(f\"   Training edges: {train_edge_index.shape[1]} (undirected)\")\n",
        "print(f\"   Validation edges: {val_edge_index.shape[1]} (directed)\")\n",
        "print(f\"   Test edges: {test_edge_index.shape[1]} (directed)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze-nzippCKJf",
        "outputId": "7c291864-ef54-495d-fe5c-84c07750fca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Total drugs: 1709\n",
            "âœ… Node features (one-hot): torch.Size([1709, 1709])\n",
            "Building graphs...\n",
            "âœ… Graph construction complete:\n",
            "   Training edges: 614004 (undirected)\n",
            "   Validation edges: 38374 (directed)\n",
            "   Test edges: 38378 (directed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================================================\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class LinkPredictor(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.W2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, z, edge_index):\n",
        "        src = z[edge_index[0]]\n",
        "        dst = z[edge_index[1]]\n",
        "        h = torch.cat([src, dst], dim=1)\n",
        "        h = F.relu(self.W1(h))\n",
        "        return self.W2(h).squeeze()\n",
        "\n",
        "model = GCN(num_drugs, config['hidden_units'], config['dropout']).to(device)\n",
        "predictor = LinkPredictor(config['hidden_units']).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(predictor.parameters()),\n",
        "    lr=config['learning_rate']\n",
        ")\n",
        "\n",
        "print(\"âœ… Model created\")\n"
      ],
      "metadata": {
        "id": "coDHDOr4Calw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import os\n",
        "def calculate_ram_usage():\n",
        "    \"\"\"Calculate current RAM usage in GB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    ram_gb = process.memory_info().rss / (1024 ** 3)  # Convert to GB\n",
        "    return ram_gb"
      ],
      "metadata": {
        "id": "eInhmkIFVCon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# TRAINING WITH VALIDATION AND EARLY STOPPING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING GCN WITH VALIDATION (FAIR COMPARISON)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def compute_loss(pred, labels):\n",
        "    return F.binary_cross_entropy_with_logits(pred, labels)\n",
        "\n",
        "def compute_metrics(pred, labels):\n",
        "    pred_proba = torch.sigmoid(pred).cpu().numpy()\n",
        "    pred_labels = (pred_proba > 0.5).astype(int)\n",
        "    true_labels = labels.cpu().numpy()\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    roc_auc = roc_auc_score(true_labels, pred_proba)\n",
        "    pr_auc = average_precision_score(true_labels, pred_proba)\n",
        "    f1 = f1_score(true_labels, pred_labels)\n",
        "    precision = precision_score(true_labels, pred_labels)\n",
        "    recall = recall_score(true_labels, pred_labels)\n",
        "\n",
        "    return accuracy, precision, recall, f1, roc_auc, pr_auc\n",
        "\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"ðŸ“Š RAM usage before training: {ram_before:.2f} GB\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "best_epoch = 0\n",
        "start_time = time.time()\n",
        "\n",
        "# Store training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(config['num_epochs']):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    predictor.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    z = model(x, train_edge_index)\n",
        "    pred = predictor(z, train_edge_index)\n",
        "    train_loss = compute_loss(pred, train_labels)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    predictor.eval()\n",
        "    with torch.no_grad():\n",
        "        z_val = model(x, train_edge_index)  # Use same training graph for embeddings\n",
        "        pred_val = predictor(z_val, val_edge_index)\n",
        "        val_loss = compute_loss(pred_val, val_labels)\n",
        "\n",
        "    train_losses.append(train_loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_epoch = epoch\n",
        "\n",
        "        # Save best model\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'predictor_state_dict': predictor.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'val_loss': val_loss.item()\n",
        "        }, f'{BASE_PATH}gcn_best_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= config['patience']:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        val_accuracy, val_precision, val_recall, val_f1, val_roc_auc, val_pr_auc = compute_metrics(pred_val, val_labels)\n",
        "        print(f\"Epoch {epoch:3d}: Train Loss = {train_loss.item():.4f}, Val Loss = {val_loss.item():.4f} , Best-val-loss: {best_val_loss:.4f}\")\n",
        "       #print(f\" Val Acc = {val_accuracy:.4f}, Val ROC-AUC = {val_roc_auc:.4f}\")\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "# Get RAM usage after training\n",
        "ram_after = calculate_ram_usage()\n",
        "ram_used = ram_after - ram_before\n",
        "\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"RAM usage after training:  {ram_after:.2f} GB\")\n",
        "print(f\"RAM used during training:  {ram_used:.2f} GB\")\n",
        "print(f\"âœ… Training complete: {train_time/60:.2f} minutes\")\n",
        "print(f\"Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bgYs_jfCYJN",
        "outputId": "811aba8d-4863-49ea-a768-1d4930bfa2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING GCN WITH VALIDATION (FAIR COMPARISON)\n",
            "================================================================================\n",
            "ðŸ“Š RAM usage before training: 0.91 GB\n",
            "Epoch   0: Train Loss = 0.6935, Val Loss = 0.6925 , Best-val-loss: 0.6925\n",
            "Epoch  20: Train Loss = 0.4832, Val Loss = 0.5136 , Best-val-loss: 0.5114\n",
            "Epoch  40: Train Loss = 0.4505, Val Loss = 0.4868 , Best-val-loss: 0.4868\n",
            "Epoch  60: Train Loss = 0.4378, Val Loss = 0.4830 , Best-val-loss: 0.4825\n",
            "Epoch  80: Train Loss = 0.4315, Val Loss = 0.4777 , Best-val-loss: 0.4777\n",
            "Epoch 100: Train Loss = 0.4268, Val Loss = 0.4730 , Best-val-loss: 0.4730\n",
            "Epoch 120: Train Loss = 0.4102, Val Loss = 0.4551 , Best-val-loss: 0.4551\n",
            "Epoch 140: Train Loss = 0.3692, Val Loss = 0.4017 , Best-val-loss: 0.4017\n",
            "Epoch 160: Train Loss = 0.3400, Val Loss = 0.3816 , Best-val-loss: 0.3770\n",
            "Epoch 180: Train Loss = 0.3294, Val Loss = 0.3668 , Best-val-loss: 0.3668\n",
            "Epoch 200: Train Loss = 0.3242, Val Loss = 0.3616 , Best-val-loss: 0.3616\n",
            "Epoch 220: Train Loss = 0.3207, Val Loss = 0.3598 , Best-val-loss: 0.3572\n",
            "Epoch 240: Train Loss = 0.3170, Val Loss = 0.3563 , Best-val-loss: 0.3537\n",
            "Epoch 260: Train Loss = 0.3125, Val Loss = 0.3523 , Best-val-loss: 0.3507\n",
            "Epoch 280: Train Loss = 0.3066, Val Loss = 0.3429 , Best-val-loss: 0.3429\n",
            "Epoch 300: Train Loss = 0.2996, Val Loss = 0.3400 , Best-val-loss: 0.3350\n",
            "Epoch 320: Train Loss = 0.2895, Val Loss = 0.3288 , Best-val-loss: 0.3266\n",
            "Epoch 340: Train Loss = 0.2821, Val Loss = 0.3169 , Best-val-loss: 0.3169\n",
            "Epoch 360: Train Loss = 0.2770, Val Loss = 0.3114 , Best-val-loss: 0.3114\n",
            "Epoch 380: Train Loss = 0.2717, Val Loss = 0.3087 , Best-val-loss: 0.3083\n",
            "Epoch 400: Train Loss = 0.2715, Val Loss = 0.3036 , Best-val-loss: 0.3036\n",
            "Epoch 420: Train Loss = 0.2655, Val Loss = 0.3022 , Best-val-loss: 0.3013\n",
            "Epoch 440: Train Loss = 0.2629, Val Loss = 0.3014 , Best-val-loss: 0.2982\n",
            "Epoch 460: Train Loss = 0.2607, Val Loss = 0.2936 , Best-val-loss: 0.2936\n",
            "Epoch 480: Train Loss = 0.2544, Val Loss = 0.2882 , Best-val-loss: 0.2882\n",
            "RAM usage before training: 0.91 GB\n",
            "RAM usage after training:  1.05 GB\n",
            "RAM used during training:  0.14 GB\n",
            "âœ… Training complete: 60.63 minutes\n",
            "Best epoch: 499, Best validation loss: 0.2812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FINAL TESTING WITH BEST MODEL\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL TESTING WITH BEST MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load(f'{BASE_PATH}gcn_best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "predictor.load_state_dict(checkpoint['predictor_state_dict'])\n",
        "\n",
        "model.eval()\n",
        "predictor.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    z_test = model(x, train_edge_index)\n",
        "    pred_test = predictor(z_test, test_edge_index)\n",
        "    pred_proba = torch.sigmoid(pred_test).cpu().numpy()\n",
        "    pred_labels = (pred_proba > 0.5).astype(int)\n",
        "    true_labels = test_labels.cpu().numpy()\n",
        "\n",
        "# Calculate final metrics\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "roc_auc = roc_auc_score(true_labels, pred_proba)\n",
        "pr_auc = average_precision_score(true_labels, pred_proba)\n",
        "f1 = f1_score(true_labels, pred_labels)\n",
        "precision = precision_score(true_labels, pred_labels)\n",
        "recall = recall_score(true_labels, pred_labels)\n",
        "\n",
        "# ============================================================================\n",
        "# PRINT COMPREHENSIVE RESULTS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GCN - FINAL RESULTS (FAIR COMPARISON)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best Epoch:        {best_epoch}\")\n",
        "print(f\"Accuracy:          {accuracy:.4f}\")\n",
        "print(f\"Precision:         {precision:.4f}\")\n",
        "print(f\"Recall:            {recall:.4f}\")\n",
        "print(f\"F1-Score:          {f1:.4f}\")\n",
        "print(f\"ROC-AUC:           {roc_auc:.4f}\")\n",
        "print(f\"PR-AUC:            {pr_auc:.4f}\")\n",
        "print(f\"Training Time:     {train_time/60:.2f} minutes\")\n",
        "print(f\"Total Epochs:      {epoch}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON WITH YOUR MODEL'S RESULTS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON WITH YOUR HyGNN MODEL\")\n",
        "print(\"=\"*80)\n",
        "print(\"Your HyGNN Results:\")\n",
        "print(\"  Accuracy:  0.9313, Precision: 0.9196, Recall: 0.9469\")\n",
        "print(\"  F1-Score:  0.9330, ROC-AUC: 0.9842, PR-AUC: 0.9841\")\n",
        "print(f\"\\\\nGCN Results:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}, ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}\")\n",
        "\n",
        "# Save detailed results for comparison\n",
        "results_df = pd.DataFrame({\n",
        "    'Drug1_ID': test_pos['Drug1_ID'].tolist() + test_neg['Drug1_ID'].tolist(),\n",
        "    'Drug2_ID': test_pos['Drug2_ID'].tolist() + test_neg['Drug2_ID'].tolist(),\n",
        "    'True_Label': true_labels,\n",
        "    'Predicted_Label': pred_labels,\n",
        "    'Prediction_Score': pred_proba\n",
        "})\n",
        "\n",
        "results_df.to_csv(f'{BASE_PATH}gcn_test_predictions.csv', index=False)\n",
        "print(f\"\\\\nDetailed predictions saved to: {BASE_PATH}gcn_test_predictions.csv\")"
      ],
      "metadata": {
        "id": "ZJpl3OUfB_jU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac32fa0d-96ae-46a8-81b0-871bb9e46a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL TESTING WITH BEST MODEL\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "GCN - FINAL RESULTS (FAIR COMPARISON)\n",
            "================================================================================\n",
            "Best Epoch:        499\n",
            "Accuracy:          0.8765\n",
            "Precision:         0.8785\n",
            "Recall:            0.8740\n",
            "F1-Score:          0.8762\n",
            "ROC-AUC:           0.9499\n",
            "PR-AUC:            0.9516\n",
            "Training Time:     60.63 minutes\n",
            "Total Epochs:      499\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPARISON WITH YOUR HyGNN MODEL\n",
            "================================================================================\n",
            "Your HyGNN Results:\n",
            "  Accuracy:  0.9313, Precision: 0.9196, Recall: 0.9469\n",
            "  F1-Score:  0.9330, ROC-AUC: 0.9842, PR-AUC: 0.9841\n",
            "\\nGCN Results:\n",
            "  Accuracy:  0.8765, Precision: 0.8785, Recall: 0.8740\n",
            "  F1-Score:  0.8762, ROC-AUC: 0.9499, PR-AUC: 0.9516\n",
            "\\nDetailed predictions saved to: /content/drive/MyDrive/MLHygnn/BaseLine/GCN_Chemical/gcn_test_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab/Notebook: System spec snapshot (no extra installs needed)\n",
        "import os, sys, json, platform, shutil, subprocess, re\n",
        "from datetime import datetime\n",
        "\n",
        "def run(cmd):\n",
        "    try:\n",
        "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)\n",
        "        return p.stdout.strip() or p.stderr.strip()\n",
        "    except Exception as e:\n",
        "        return f\"(error running {' '.join(cmd)}: {e})\"\n",
        "\n",
        "def grep(text, pattern, default=None, flags=re.I):\n",
        "    m = re.search(pattern, text or \"\", flags)\n",
        "    return m.group(1).strip() if m else default\n",
        "\n",
        "def parse_lscpu():\n",
        "    out = run([\"bash\",\"-lc\",\"lscpu\"])\n",
        "    info = {}\n",
        "    for line in out.splitlines():\n",
        "        if \":\" in line:\n",
        "            k,v = line.split(\":\",1)\n",
        "            info[k.strip()] = v.strip()\n",
        "    return out, {\n",
        "        \"model_name\": info.get(\"Model name\"),\n",
        "        \"architecture\": info.get(\"Architecture\"),\n",
        "        \"cpus\": info.get(\"CPU(s)\"),\n",
        "        \"threads_per_core\": info.get(\"Thread(s) per core\"),\n",
        "        \"cores_per_socket\": info.get(\"Core(s) per socket\"),\n",
        "        \"sockets\": info.get(\"Socket(s)\"),\n",
        "        \"base_mhz\": info.get(\"CPU MHz\"),\n",
        "        \"max_mhz\": info.get(\"CPU max MHz\"),\n",
        "        \"vendor\": info.get(\"Vendor ID\"),\n",
        "    }\n",
        "\n",
        "def parse_free():\n",
        "    out = run([\"bash\",\"-lc\",\"free -h\"])\n",
        "    total = grep(out, r\"Mem:\\s+(\\S+)\", None)\n",
        "    return out, {\"ram_total_human\": total}\n",
        "\n",
        "def parse_df():\n",
        "    out = run([\"bash\",\"-lc\",\"df -h /\"])\n",
        "    # header then root line\n",
        "    lines = out.splitlines()\n",
        "    root = lines[1].split() if len(lines) >= 2 else []\n",
        "    total = root[1] if len(root) >= 2 else None\n",
        "    avail = root[3] if len(root) >= 4 else None\n",
        "    return out, {\"disk_root_total_human\": total, \"disk_root_available_human\": avail}\n",
        "\n",
        "def nvidia_smi():\n",
        "    if shutil.which(\"nvidia-smi\"):\n",
        "        out = run([\"bash\",\"-lc\",\"nvidia-smi -L\"])\n",
        "        full = run([\"bash\",\"-lc\",\"nvidia-smi\"])\n",
        "        driver = grep(full, r\"Driver Version:\\s*([^\\s]+)\")\n",
        "        cuda = grep(full, r\"CUDA Version:\\s*([^\\s]+)\")\n",
        "        # Try VRAM for first GPU\n",
        "        vram = None\n",
        "        m = re.search(r\"(\\d+)\\s*MiB\\s+Total\", full)\n",
        "        if m: vram = f\"{m.group(1)} MiB\"\n",
        "        return {\"present\": True, \"list\": out, \"driver\": driver, \"cuda_version\": cuda, \"vram_total\": vram, \"raw_table\": full}\n",
        "    return {\"present\": False}\n",
        "\n",
        "def detect_env():\n",
        "    in_colab = False\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        in_colab = True\n",
        "    except Exception:\n",
        "        in_colab = bool(os.environ.get(\"COLAB_GPU\") or os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"))\n",
        "    return {\n",
        "        \"in_colab\": in_colab,\n",
        "        \"python\": sys.version.split()[0],\n",
        "        \"python_build\": \" \".join(platform.python_build()),\n",
        "        \"os\": platform.system(),\n",
        "        \"os_release\": platform.release(),\n",
        "        \"platform\": platform.platform(),\n",
        "        \"machine\": platform.machine(),\n",
        "        \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
        "    }\n",
        "\n",
        "# Collect\n",
        "lscpu_out, cpu = parse_lscpu()\n",
        "free_out, ram = parse_free()\n",
        "df_out, disk = parse_df()\n",
        "gpu = nvidia_smi()\n",
        "env = detect_env()\n",
        "\n",
        "summary = {\n",
        "    \"environment\": env,\n",
        "    \"cpu\": cpu,\n",
        "    \"ram\": ram,\n",
        "    \"disk\": disk,\n",
        "    \"gpu\": {k:v for k,v in gpu.items() if k != \"raw_table\"}  # keep raw separately\n",
        "}\n",
        "\n",
        "print(\"=== SUMMARY (JSON) ===\")\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "print(\"\\n=== RAW: lscpu ===\")\n",
        "print(lscpu_out)\n",
        "\n",
        "print(\"\\n=== RAW: free -h ===\")\n",
        "print(free_out)\n",
        "\n",
        "print(\"\\n=== RAW: df -h / ===\")\n",
        "print(df_out)\n",
        "\n",
        "print(\"\\n=== RAW: nvidia-smi ===\")\n",
        "print(gpu.get(\"raw_table\",\"(no GPU / nvidia-smi not found)\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghs9TuQQJUKY",
        "outputId": "d8140b33-6e2c-40e5-b1ad-7e35132a95b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SUMMARY (JSON) ===\n",
            "{\n",
            "  \"environment\": {\n",
            "    \"in_colab\": true,\n",
            "    \"python\": \"3.12.12\",\n",
            "    \"python_build\": \"main Oct 10 2025 08:52:57\",\n",
            "    \"os\": \"Linux\",\n",
            "    \"os_release\": \"6.6.105+\",\n",
            "    \"platform\": \"Linux-6.6.105+-x86_64-with-glibc2.35\",\n",
            "    \"machine\": \"x86_64\",\n",
            "    \"timestamp\": \"2025-10-21T15:04:59.512990Z\"\n",
            "  },\n",
            "  \"cpu\": {\n",
            "    \"model_name\": \"AMD EPYC 7B12\",\n",
            "    \"architecture\": \"x86_64\",\n",
            "    \"cpus\": \"2\",\n",
            "    \"threads_per_core\": \"2\",\n",
            "    \"cores_per_socket\": \"1\",\n",
            "    \"sockets\": \"1\",\n",
            "    \"base_mhz\": null,\n",
            "    \"max_mhz\": null,\n",
            "    \"vendor\": \"AuthenticAMD\"\n",
            "  },\n",
            "  \"ram\": {\n",
            "    \"ram_total_human\": \"12Gi\"\n",
            "  },\n",
            "  \"disk\": {\n",
            "    \"disk_root_total_human\": \"108G\",\n",
            "    \"disk_root_available_human\": \"69G\"\n",
            "  },\n",
            "  \"gpu\": {\n",
            "    \"present\": false\n",
            "  }\n",
            "}\n",
            "\n",
            "=== RAW: lscpu ===\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           48 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               AuthenticAMD\n",
            "Model name:                              AMD EPYC 7B12\n",
            "CPU family:                              23\n",
            "Model:                                   49\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                0\n",
            "BogoMIPS:                                4499.99\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                512 KiB (1 instance)\n",
            "L3 cache:                                16 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Not affected\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Not affected\n",
            "Vulnerability Mds:                       Not affected\n",
            "Vulnerability Meltdown:                  Not affected\n",
            "Vulnerability Mmio stale data:           Not affected\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Vulnerable\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Not affected\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Not affected\n",
            "\n",
            "=== RAW: free -h ===\n",
            "total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       1.7Gi       8.7Gi       2.0Mi       2.3Gi        10Gi\n",
            "Swap:             0B          0B          0B\n",
            "\n",
            "=== RAW: df -h / ===\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   40G   69G  37% /\n",
            "\n",
            "=== RAW: nvidia-smi ===\n",
            "(no GPU / nvidia-smi not found)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1935633988.py:77: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_cell",
        "outputId": "3c2dfef1-7030-44f7-d09d-0db6ed282d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torch-geometric scikit-learn rdkit pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mount_cell",
        "outputId": "1e4d32d9-70f8-4b5c-e252-0f915ef06e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports_cell",
        "outputId": "eda7bc88-83dc-4046-aee4-b31818bc669e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv  \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "\n",
        "# RDKit for SMILES fingerprints\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MACCSkeys\n",
        "from rdkit import DataStructs\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config_cell",
        "outputId": "a9f4bc4c-6ed5-4320-d20d-cba8c7d8e705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "CONFIGURATION\n",
            "================================================================================\n",
            "Model: GraphSAGE + MLP\n",
            "Aggregator: MEAN\n",
            "Fingerprint: MORGAN (512-bit)\n",
            "Hidden Dim: 128\n",
            "Dropout: 0.5\n",
            "Learning Rate: 0.01\n",
            "Weight Decay: 0.001\n",
            "Epochs: 500\n",
            "Batch Size: 128\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "Fingerprint_type = 'morgan'\n",
        "N_bit = 512\n",
        "Hidden_dimensions = 128\n",
        "Included_dimensions = 128\n",
        "Dropout = 0.5\n",
        "Batch_size = 128\n",
        "Learning_rate = 0.01\n",
        "Decay_weight = 0.001\n",
        "Number_Eproch = 500\n",
        "Seeds = 42\n",
        "Aggregator = 'mean'\n",
        "torch.manual_seed(Seeds)\n",
        "np.random.seed(Seeds)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(Seeds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model: GraphSAGE + MLP\")\n",
        "print(f\"Aggregator: {Aggregator.upper()}\")\n",
        "print(f\"Fingerprint: {Fingerprint_type.upper()} ({N_bit}-bit)\")\n",
        "print(f\"Hidden Dim: {Hidden_dimensions}\")\n",
        "print(f\"Dropout: {Dropout}\")\n",
        "print(f\"Learning Rate: {Learning_rate}\")\n",
        "print(f\"Weight Decay: {Decay_weight}\")\n",
        "print(f\"Epochs: {Number_Eproch}\")\n",
        "print(f\"Batch Size: {Batch_size}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "## 5. Load Data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_data_cell",
        "outputId": "5c7c0f13-e8e4-4701-b59a-6bcc4f6f5b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from Google Drive...\n",
            "\n",
            "Train data preview:\n",
            "  Drug1_ID Drug2_ID  Label\n",
            "0  DB01097  DB05219     47\n",
            "1  DB00547  DB00784     49\n",
            "2  DB00623  DB01365     61\n",
            "3  DB00328  DB09027     73\n",
            "4  DB00742  DB00955     57\n",
            "\n",
            "Dataset sizes:\n",
            "  Training: 153,489 pairs\n",
            "  Validation: 19,188 pairs\n",
            "  Test: 19,200 pairs\n"
          ]
        }
      ],
      "source": [
        "# Path to our data folder in Google Drive\n",
        "DATA_PATH = '/content/drive/MyDrive/MyModel/GCN2_MLP/data/'\n",
        "DATA_PATH_SMILES = '/content/drive/MyDrive/MyModel/'\n",
        "\n",
        "print(\"Loading data from Google Drive...\")\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv(DATA_PATH + 'train_positive.csv')\n",
        "val_df = pd.read_csv(DATA_PATH + 'val_positive.csv')\n",
        "test_df = pd.read_csv(DATA_PATH + 'test_positive.csv')\n",
        "\n",
        "print(f\"\\nTrain data preview:\")\n",
        "print(train_df.head())\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Training: {len(train_df):,} pairs\")\n",
        "print(f\"  Validation: {len(val_df):,} pairs\")\n",
        "print(f\"  Test: {len(test_df):,} pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_smiles"
      },
      "source": [
        "## 6. Load Drug SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_smiles_cell",
        "outputId": "bfa59f30-1b65-44b2-d865-9ac9fb05caeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Drug SMILES loaded: 1709 drugs\n",
            "  DrugBank_ID                                             SMILES\n",
            "0     DB00006  CC[C@H](C)[C@H](NC(=O)[C@H](CCC(O)=O)NC(=O)[C@...\n",
            "1     DB00014  CC(C)C[C@H](NC(=O)[C@@H](COC(C)(C)C)NC(=O)[C@H...\n",
            "2     DB00027  CC(C)C[C@@H](NC(=O)CNC(=O)[C@@H](NC=O)C(C)C)C(...\n",
            "3     DB00035  NC(=O)CC[C@@H]1NC(=O)[C@H](CC2=CC=CC=C2)NC(=O)...\n",
            "4     DB00080  CCCCCCCCCC(=O)N[C@@H](CC1=CNC2=C1C=CC=C2)C(=O)...\n"
          ]
        }
      ],
      "source": [
        "# Load drug SMILES\n",
        "drug_smiles_df = pd.read_csv(DATA_PATH_SMILES + 'Drugs_with_Smiles.csv')\n",
        "\n",
        "print(f\"\\nDrug SMILES loaded: {len(drug_smiles_df)} drugs\")\n",
        "print(drug_smiles_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fingerprints"
      },
      "source": [
        "## 7. Extract Molecular Fingerprints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We use Morgan Fingerprint (2,512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fingerprints_cell",
        "outputId": "36bc8ae9-db9d-4760-cd37-66bede01c5c2"
      },
      "outputs": [],
      "source": [
        "# Get unique drugs\n",
        "all_drugs = pd.concat([\n",
        "    train_df['Drug1_ID'], train_df['Drug2_ID'],\n",
        "    val_df['Drug1_ID'], val_df['Drug2_ID'],\n",
        "    test_df['Drug1_ID'], test_df['Drug2_ID']\n",
        "]).unique()\n",
        "\n",
        "print(f\"\\nðŸ”¬ Extracting {Fingerprint_type} fingerprints...\")\n",
        "print(f\"Total unique drugs: {len(all_drugs)}\")\n",
        "\n",
        "def smiles_to_fingerprint(smiles, fp_type='morgan', n_bits=512):\n",
        "    \"\"\"Convert SMILES to molecular fingerprint\"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "\n",
        "        if fp_type == 'morgan':\n",
        "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
        "        elif fp_type == 'maccs':\n",
        "            fp = MACCSkeys.GenMACCSKeys(mol)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown fingerprint type: {fp_type}\")\n",
        "\n",
        "        arr = np.zeros((0,), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "        return arr\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Extract fingerprints\n",
        "drug_to_fp = {}\n",
        "drug_to_idx = {drug: idx for idx, drug in enumerate(all_drugs)}\n",
        "\n",
        "for drug_id in tqdm(all_drugs, desc=\"Extracting fingerprints\"):\n",
        "    smiles_row = drug_smiles_df[drug_smiles_df['DrugBank_ID'] == drug_id]\n",
        "    if len(smiles_row) > 0:\n",
        "        smiles = smiles_row.iloc[0]['SMILES']\n",
        "        fp = smiles_to_fingerprint(smiles, fp_type=Fingerprint_type, n_bits=N_bit)\n",
        "        if fp is not None:\n",
        "            drug_to_fp[drug_id] = fp\n",
        "        else:\n",
        "            # Default fingerprint for failed cases\n",
        "            drug_to_fp[drug_id] = np.zeros(N_bit, dtype=np.int8)\n",
        "    else:\n",
        "        drug_to_fp[drug_id] = np.zeros(N_bit, dtype=np.int8)\n",
        "\n",
        "# Create feature matrix\n",
        "num_drugs = len(all_drugs)\n",
        "drug_features = np.zeros((num_drugs, N_bit), dtype=np.float32)\n",
        "\n",
        "for drug_id, idx in drug_to_idx.items():\n",
        "    if drug_id in drug_to_fp:\n",
        "        drug_features[idx] = drug_to_fp[drug_id]\n",
        "\n",
        "drug_features = torch.FloatTensor(drug_features).to(device)\n",
        "\n",
        "print(f\"\\nFingerprints extracted!\")\n",
        "print(f\"Feature matrix shape: {drug_features.shape}\")\n",
        "print(f\"Device: {drug_features.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_data"
      },
      "source": [
        "## 8. Prepare Graph Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prepare_data_cell",
        "outputId": "d542ebd7-da4a-4f51-9f56-8d08767d973a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted labels from 1-86 to 0-85\n",
            "\n",
            "Data prepared!\n",
            "Training pairs: 153,489\n",
            "Validation pairs: 19,188\n",
            "Test pairs: 19,200\n",
            "Edge index shape: torch.Size([2, 306978])\n",
            "Number of classes: 86\n"
          ]
        }
      ],
      "source": [
        "def prepare_pairs(df, drug_to_idx):\n",
        "    \"\"\"Convert dataframe to pair indices and labels\"\"\"\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        drug1 = row['Drug1_ID']\n",
        "        drug2 = row['Drug2_ID']\n",
        "        label = row['Label']\n",
        "\n",
        "        if drug1 in drug_to_idx and drug2 in drug_to_idx:\n",
        "            idx1 = drug_to_idx[drug1]\n",
        "            idx2 = drug_to_idx[drug2]\n",
        "            pairs.append([idx1, idx2])\n",
        "            labels.append(label)\n",
        "\n",
        "    return torch.LongTensor(pairs), torch.LongTensor(labels)\n",
        "\n",
        "# Prepare training, validation, and test data\n",
        "train_pairs, train_types = prepare_pairs(train_df, drug_to_idx)\n",
        "val_pairs, val_types = prepare_pairs(val_df, drug_to_idx)\n",
        "test_pairs, test_types = prepare_pairs(test_df, drug_to_idx)\n",
        "# If labels are 1-86, convert to 0-85\n",
        "if train_types.min() == 1:\n",
        "    train_types = train_types - 1\n",
        "    val_types = val_types - 1\n",
        "    test_types = test_types - 1\n",
        "    print(f\"Converted labels from 1-86 to 0-85\")\n",
        "else:\n",
        "    print(\"Labels already 0-based\")\n",
        "# Move to device\n",
        "train_pairs = train_pairs.to(device)\n",
        "train_types = train_types.to(device)\n",
        "val_pairs = val_pairs.to(device)\n",
        "val_types = val_types.to(device)\n",
        "test_pairs = test_pairs.to(device)\n",
        "test_types = test_types.to(device)\n",
        "\n",
        "# Build edge index from training data (for GCN graph structure)\n",
        "edge_index = torch.cat([train_pairs.T, train_pairs.T[[1, 0]]], dim=1)  # Bidirectional edges\n",
        "\n",
        "print(f\"\\nData prepared!\")\n",
        "print(f\"Training pairs: {len(train_pairs):,}\")\n",
        "print(f\"Validation pairs: {len(val_pairs):,}\")\n",
        "print(f\"Test pairs: {len(test_pairs):,}\")\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "print(f\"Number of classes: {train_types.max().item() + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "# 9. Define GCN + MLP Model\n",
        "\n",
        "### We use tow layer \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "model_cell",
        "outputId": "d116d69d-79d6-41fd-c84b-3d22e4a024b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "MODEL ARCHITECTURE (2-Layer GCN)\n",
            "================================================================================\n",
            "GCN_MLP_2Layer(\n",
            "  (gcn1): GCNConv(512, 128)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (gcn2): GCNConv(128, 128)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=128, out_features=86, bias=True)\n",
            "  )\n",
            ")\n",
            "================================================================================\n",
            "\n",
            "Total parameters: 258,006\n",
            "Trainable parameters: 258,006\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class GCN_MLP_2Layer(nn.Module):\n",
        "     def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.5):\n",
        "        super(GCN_MLP_2Layer, self).__init__()\n",
        "\n",
        "        # Two GCN layers with BatchNorm\n",
        "        self.gcn1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        # MLP for pair classification\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),  # 512 â†’ 256\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),       # 256 â†’ 128\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)           # 128 â†’ 86\n",
        "        )\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "     def forward(self, x, edge_index, drug_pairs):\n",
        "\n",
        "        # First GCN layer with BatchNorm\n",
        "        x = self.gcn1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Second GCN layer with BatchNorm\n",
        "        x = self.gcn2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Get embeddings for drug pairs\n",
        "        h_i = x[drug_pairs[:, 0]]  # First drug in pair\n",
        "        h_j = x[drug_pairs[:, 1]]  # Second drug in pair\n",
        "\n",
        "        # Combine pair features: [concat, hadamard, abs_diff]\n",
        "        h_pair = torch.cat([\n",
        "            h_i,                    # Drug 1 embedding\n",
        "            h_j,                    # Drug 2 embedding\n",
        "            h_i * h_j,              # Element-wise product\n",
        "            torch.abs(h_i - h_j)    # Absolute difference\n",
        "        ], dim=-1)\n",
        "\n",
        "        # MLP classifier\n",
        "        logits = self.mlp(h_pair)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "model = GCN_MLP_2Layer(\n",
        "    input_dim=N_bit,          # 512 for Morgan fingerprints\n",
        "    hidden_dim=Hidden_dimensions,     # 128\n",
        "    num_classes=86,\n",
        "    dropout=Dropout            # 0.5\n",
        ").to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL ARCHITECTURE (2-Layer GCN)\")\n",
        "print(\"=\"*80)\n",
        "print(model)\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_setup"
      },
      "source": [
        "# 10. Training Setup\n",
        "\n",
        "### We used class weight in both the training and verification phases because the model couldn't train when we only applied class weight during the verification phase. Therefore, we had to apply it in both phases to enable it to learn and train. This contrasts with our model, which was able to train without class weight ðŸ’ªðŸ¥‡."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "training_setup_cell",
        "outputId": "91709ff8-3ee3-4552-faa1-8a623774f498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " CLASS WEIGHTS\n",
            "================================================================================\n",
            "Class weights statistics (alpha=0.3):\n",
            "  Min weight: 0.1566\n",
            "  Max weight: 2.6340\n",
            "  Mean weight: 1.0000\n",
            "  Sample counts - Min: 4, Max: 48746\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Calculate class weights (EXACTLY like Our HGNN )\n",
        "type_counts = torch.bincount(train_types, minlength=86).float()\n",
        "alpha = 0.3\n",
        "class_weights = 1.0 / torch.pow(type_counts.clamp(min=1.0), alpha)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" CLASS WEIGHTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Class weights statistics (alpha={alpha}):\")\n",
        "print(f\"  Min weight: {class_weights.min().item():.4f}\")\n",
        "print(f\"  Max weight: {class_weights.max().item():.4f}\")\n",
        "print(f\"  Mean weight: {class_weights.mean().item():.4f}\")\n",
        "print(f\"  Sample counts - Min: {type_counts.min().int().item()}, Max: {type_counts.max().int().item()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=Learning_rate, weight_decay=Decay_weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_eval"
      },
      "source": [
        "## 11. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_eval_cell"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, criterion, pairs, labels, edge_index, x, batch_size=128):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    perm = torch.randperm(len(pairs)) # Shuffle during training  = prevent memorizing data order because GCN use batches\n",
        "    for i in range(0, len(pairs), batch_size):\n",
        "        batch_idx = perm[i:i+batch_size]\n",
        "        batch_pairs = pairs[batch_idx]\n",
        "        batch_labels = labels[batch_idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(x, edge_index, batch_pairs)\n",
        "        loss = criterion(logits, batch_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item() * len(batch_pairs)\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "        total += len(batch_pairs)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, pairs, labels, edge_index, x, criterion, class_weights, batch_size=128):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(0, len(pairs), batch_size):\n",
        "        batch_pairs = pairs[i:i+batch_size]\n",
        "        batch_labels = labels[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x, edge_index, batch_pairs)\n",
        "        loss = criterion(logits, batch_labels)\n",
        "\n",
        "        total_loss += loss.item() * len(batch_pairs)\n",
        "\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(pairs)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'f1_micro': f1_score(all_labels, all_preds, average='micro', zero_division=0),\n",
        "        'f1_macro': f1_score(all_labels, all_preds, average='macro', zero_division=0),\n",
        "        'precision': precision_score(all_labels, all_preds, average='micro', zero_division=0),\n",
        "        'recall': recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    }\n",
        "\n",
        "    return metrics, avg_loss\n",
        "\n",
        "# RAM usage calculator\n",
        "def calculate_ram_usage():\n",
        "    \"\"\"Calculate RAM usage in GB\"\"\"\n",
        "    import psutil\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 / 1024 / 1024\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train"
      },
      "source": [
        "## 12. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "train_cell",
        "outputId": "55c22d0b-4b28-45ea-8989-a1fcda7e4855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FULL TRAINING GCN+MLP (500 EPOCHS) - GPU ACCELERATED\n",
            "================================================================================\n",
            "RAM usage before training: 1.66 GB\n",
            "Device: cuda\n",
            "Training data: 153,489 pairs\n",
            "\n",
            "New best: Epoch 0 - Val Loss: 1.5139 (Time: 19.2s)\n",
            "Epoch 0: loss: 1.7538, val_loss: 1.5139 (best: 1.5139, patience: 0)\n",
            "New best: Epoch 3 - Val Loss: 1.4735 (Time: 19.0s)\n",
            "New best: Epoch 5 - Val Loss: 1.4681 (Time: 19.2s)\n",
            "New best: Epoch 8 - Val Loss: 1.4141 (Time: 19.1s)\n",
            "Epoch 10: loss: 1.2961, val_loss: 1.6583 (best: 1.4141, patience: 2)\n",
            "New best: Epoch 11 - Val Loss: 1.2043 (Time: 19.1s)\n",
            "New best: Epoch 12 - Val Loss: 1.0206 (Time: 19.1s)\n",
            "Epoch 20: loss: 1.2296, val_loss: 1.4612 (best: 1.0206, patience: 8)\n",
            "New best: Epoch 26 - Val Loss: 0.9457 (Time: 19.1s)\n",
            "Epoch 30: loss: 1.2029, val_loss: 1.3138 (best: 0.9457, patience: 4)\n",
            "New best: Epoch 35 - Val Loss: 0.9319 (Time: 19.1s)\n",
            "Epoch 40: loss: 1.1910, val_loss: 1.0631 (best: 0.9319, patience: 5)\n",
            "Epoch 50: loss: 1.1826, val_loss: 1.9611 (best: 0.9319, patience: 15)\n",
            "New best: Epoch 54 - Val Loss: 0.8935 (Time: 19.1s)\n",
            "Epoch 60: loss: 1.1698, val_loss: 1.5476 (best: 0.8935, patience: 6)\n",
            "Epoch 70: loss: 1.1667, val_loss: 1.3708 (best: 0.8935, patience: 16)\n",
            "Epoch 80: loss: 1.1568, val_loss: 1.3115 (best: 0.8935, patience: 26)\n",
            "Epoch 90: loss: 1.1560, val_loss: 1.3870 (best: 0.8935, patience: 36)\n",
            "Epoch 100: loss: 1.1606, val_loss: 1.0045 (best: 0.8935, patience: 46)\n",
            "Epoch 110: loss: 1.1535, val_loss: 1.6074 (best: 0.8935, patience: 56)\n",
            "Epoch 120: loss: 1.1593, val_loss: 1.1662 (best: 0.8935, patience: 66)\n",
            "Epoch 130: loss: 1.1535, val_loss: 2.3807 (best: 0.8935, patience: 76)\n",
            "New best: Epoch 137 - Val Loss: 0.8342 (Time: 19.0s)\n",
            "Epoch 140: loss: 1.1459, val_loss: 1.9769 (best: 0.8342, patience: 3)\n",
            "Epoch 150: loss: 1.1453, val_loss: 1.0015 (best: 0.8342, patience: 13)\n",
            "Epoch 160: loss: 1.1461, val_loss: 1.2215 (best: 0.8342, patience: 23)\n",
            "Epoch 170: loss: 1.1422, val_loss: 1.1449 (best: 0.8342, patience: 33)\n",
            "Epoch 180: loss: 1.1429, val_loss: 1.1372 (best: 0.8342, patience: 43)\n",
            "Epoch 190: loss: 1.1405, val_loss: 2.1459 (best: 0.8342, patience: 53)\n",
            "Epoch 200: loss: 1.1398, val_loss: 1.1722 (best: 0.8342, patience: 63)\n",
            "Epoch 210: loss: 1.1432, val_loss: 1.3301 (best: 0.8342, patience: 73)\n",
            "Epoch 220: loss: 1.1402, val_loss: 0.9780 (best: 0.8342, patience: 83)\n",
            "Epoch 230: loss: 1.1308, val_loss: 1.1524 (best: 0.8342, patience: 93)\n",
            "\n",
            "Early stopping at epoch 237\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED\n",
            "================================================================================\n",
            "Best epoch: 137\n",
            "Best validation loss: 0.8342\n",
            "Total time: 75.56 minutes\n",
            "Average time per epoch: 19.0 seconds\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"FULL TRAINING GCN+MLP ({Number_Eproch} EPOCHS) - GPU ACCELERATED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Training data: {len(train_pairs):,} pairs\\n\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "patience = 100\n",
        "patience_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(Number_Eproch):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, optimizer, criterion,\n",
        "        train_pairs, train_types,\n",
        "        edge_index, drug_features,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    # Validation phase\n",
        "    val_metrics, val_loss = evaluate(\n",
        "        model, val_pairs, val_types,\n",
        "        edge_index, drug_features,\n",
        "        criterion, class_weights,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), '/content/best_gcn_mlp_model.pth')\n",
        "        torch.save(DATA_PATH,'best_gcn_mlp_model.pth')\n",
        "\n",
        "        print(f\"New best: Epoch {epoch} - Val Loss: {val_loss:.4f} (Time: {epoch_time:.1f}s)\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: loss: {train_loss:.4f}, val_loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience_counter})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total time: {total_time/60:.2f} minutes\")\n",
        "print(f\"Average time per epoch: {total_time/max(epoch+1, 1):.1f} seconds\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCugNx1jbAg6"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), DATA_PATH + 'best_gcn2_mlp_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test"
      },
      "source": [
        "## 13. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_cell",
        "outputId": "3c4d02b0-dd5d-437e-c85e-aa2916bb9606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL TEST SET RESULTS (GCN + MLP)\n",
            "================================================================================\n",
            "Accuracy:   0.4592\n",
            "F1 (Micro): 0.4592\n",
            "F1 (Macro): 0.4201\n",
            "Precision:  0.4592\n",
            "Recall:     0.4592\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load(DATA_PATH + 'best_gcn2_mlp_model.pth'))\n",
        "model.eval()\n",
        "# Evaluate on test set\n",
        "test_metrics, test_loss = evaluate(\n",
        "    model, test_pairs, test_types,\n",
        "    edge_index, drug_features,\n",
        "    criterion, class_weights,\n",
        "    batch_size=Batch_size\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL TEST SET RESULTS (GCN + MLP)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy:   {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"F1 (Micro): {test_metrics['f1_micro']:.4f}\")\n",
        "print(f\"F1 (Macro): {test_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Precision:  {test_metrics['precision']:.4f}\")\n",
        "print(f\"Recall:     {test_metrics['recall']:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save"
      },
      "source": [
        "## 15. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "save_cell",
        "outputId": "efe03342-b7d0-4ebe-8e41-8aff7a9ed891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results saved to: /content/drive/MyDrive/MyModel/GCN2_MLP/data/result/\n",
            "   - gcn_mlp_morgan512_model.pth\n",
            "   - gcn_mlp_results.json\n"
          ]
        }
      ],
      "source": [
        "# Save results to Google Drive (optional)\n",
        "SAVE_PATH =  DATA_PATH+\"result/\" \n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "# Save model\n",
        "torch.save(model.state_dict(), SAVE_PATH + 'gcn2_mlp_morgan512_model.pth')\n",
        "\n",
        "# Save results\n",
        "import json\n",
        "results = {\n",
        "    'model': 'GCN+MLP',\n",
        "    'features': 'Morgan 512',\n",
        "    'best_epoch': best_epoch,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'test_metrics': test_metrics,\n",
        "    'test_loss': test_loss,\n",
        "    'training_time_minutes': total_time / 60,\n",
        "    'hyperparameters': {\n",
        "        'hidden_dim': Hidden_dimensions,\n",
        "        'dropout': Dropout,\n",
        "        'learning_rate': Learning_rate,\n",
        "        'weight_decay': Decay_weight,\n",
        "        'batch_size': Batch_size,\n",
        "        'epochs': Number_Eproch\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(SAVE_PATH + 'gcn_mlp_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nResults saved to: {SAVE_PATH}\")\n",
        "print(\"   - gcn_mlp_morgan512_model.pth\")\n",
        "print(\"   - gcn_mlp_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "analysis_cell",
        "outputId": "477788aa-23af-4fb2-8f50-f53b748c0f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DETAILED CLASSIFICATION REPORT\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.24      0.56      0.34        27\n",
            "           2       0.83      0.78      0.80        58\n",
            "           3       0.55      0.48      0.51       504\n",
            "           4       0.75      0.47      0.58        32\n",
            "           5       0.67      0.85      0.75       296\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.67      0.84      0.74        19\n",
            "           8       0.71      0.83      0.77       230\n",
            "           9       0.78      0.62      0.69        61\n",
            "          10       0.00      0.00      0.00        27\n",
            "          11       0.31      0.90      0.46        30\n",
            "          12       0.00      0.00      0.00         4\n",
            "          13       0.75      0.52      0.61        29\n",
            "          14       0.00      0.00      0.00        16\n",
            "          15       0.36      0.66      0.46       514\n",
            "          16       1.00      0.50      0.67         6\n",
            "          17       0.00      0.00      0.00        13\n",
            "          18       0.00      0.00      0.00        15\n",
            "          19       0.08      0.98      0.16       596\n",
            "          20       0.42      0.93      0.58        42\n",
            "          21       0.50      0.88      0.64         8\n",
            "          22       0.14      0.33      0.20         3\n",
            "          23       0.94      1.00      0.97        16\n",
            "          24       0.52      0.42      0.46        62\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.79      0.97      0.87        99\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       1.00      0.94      0.97        36\n",
            "          29       1.00      0.13      0.24        60\n",
            "          30       0.00      0.00      0.00         1\n",
            "          31       0.88      0.21      0.34       101\n",
            "          32       0.00      0.00      0.00        51\n",
            "          33       0.81      0.71      0.76        35\n",
            "          34       0.08      0.40      0.14         5\n",
            "          35       0.00      0.00      0.00        10\n",
            "          36       0.93      0.65      0.76       258\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.87      0.93      0.90        14\n",
            "          39       0.86      1.00      0.92        30\n",
            "          40       0.00      0.00      0.00         2\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.00      0.00      0.00         1\n",
            "          43       0.00      0.00      0.00         2\n",
            "          44       1.00      1.00      1.00         2\n",
            "          45       1.00      0.33      0.50         3\n",
            "          46       0.79      0.09      0.15      3405\n",
            "          47       1.00      0.56      0.71         9\n",
            "          48       0.88      0.57      0.69      6099\n",
            "          49       0.17      1.00      0.29         1\n",
            "          50       0.54      0.78      0.64         9\n",
            "          51       0.00      0.00      0.00         1\n",
            "          52       0.42      0.24      0.31        33\n",
            "          53       0.74      0.41      0.53       135\n",
            "          54       0.26      1.00      0.41         9\n",
            "          55       0.00      0.00      0.00         5\n",
            "          56       0.36      0.46      0.41        61\n",
            "          57       0.40      0.99      0.57       106\n",
            "          58       1.00      0.80      0.89         5\n",
            "          59       0.65      0.68      0.67       831\n",
            "          60       0.42      1.00      0.59        55\n",
            "          61       0.00      0.00      0.00         1\n",
            "          62       0.00      0.00      0.00         3\n",
            "          63       0.26      0.54      0.35        76\n",
            "          64       1.00      1.00      1.00         3\n",
            "          65       0.91      0.67      0.77        15\n",
            "          66       0.49      0.86      0.62       122\n",
            "          67       1.00      0.24      0.38        34\n",
            "          68       0.94      0.79      0.86        19\n",
            "          69       0.72      0.65      0.68       808\n",
            "          70       0.42      0.65      0.51        57\n",
            "          71       0.90      0.93      0.92       191\n",
            "          72       0.60      0.13      0.21      2406\n",
            "          73       0.93      0.96      0.95        57\n",
            "          74       0.40      0.51      0.45       961\n",
            "          75       0.72      0.48      0.58        58\n",
            "          76       0.74      0.27      0.40        62\n",
            "          77       0.00      0.00      0.00         2\n",
            "          78       0.00      0.00      0.00         4\n",
            "          79       0.00      0.00      0.00        14\n",
            "          80       0.10      0.56      0.18         9\n",
            "          81       0.50      0.39      0.44        31\n",
            "          82       0.97      0.85      0.91       130\n",
            "          83       0.09      0.75      0.16         4\n",
            "          84       0.93      0.33      0.49        39\n",
            "          85       1.00      0.50      0.67         4\n",
            "\n",
            "    accuracy                           0.46     19200\n",
            "   macro avg       0.47      0.47      0.42     19200\n",
            "weighted avg       0.71      0.46      0.48     19200\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Per-class performance analysis\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get predictions for test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_pairs), Batch_size):\n",
        "        batch_pairs = test_pairs[i:i+Batch_size]\n",
        "        batch_labels = test_types[i:i+Batch_size]\n",
        "\n",
        "        logits = model(drug_features, edge_index, batch_pairs)\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_cell"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torch-geometric scikit-learn rdkit pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mount_cell",
        "outputId": "3c4a5e8e-ffbd-4e45-81ba-7e6e828cd4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports_cell",
        "outputId": "032c323b-33ab-4a12-aede-f8cac7c1f901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "\n",
        "# RDKit for SMILES fingerprints\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MACCSkeys\n",
        "from rdkit import DataStructs\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config"
      },
      "source": [
        "## 4. Configuration (Matching GAT/GCN Setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config_cell",
        "outputId": "a6ac0bcc-4e0e-492d-91a0-118fc651ff58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "CONFIGURATION\n",
            "================================================================================\n",
            "Model: GraphSAGE + MLP\n",
            "Aggregator: MEAN\n",
            "Fingerprint: MORGAN (512-bit)\n",
            "Hidden Dim: 128\n",
            "Dropout: 0.5\n",
            "Learning Rate: 0.01\n",
            "Weight Decay: 0.001\n",
            "Epochs: 500\n",
            "Batch Size: 128\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "Fingerprint_type = 'morgan'\n",
        "N_bit = 512\n",
        "Hidden_dimensions = 128\n",
        "Included_dimensions = 128\n",
        "Dropout = 0.5\n",
        "Batch_size = 128\n",
        "Learning_rate = 0.01\n",
        "Decay_weight = 0.001\n",
        "Number_Eproch = 500\n",
        "Seeds = 42\n",
        "Aggregator = 'mean'\n",
        "torch.manual_seed(Seeds)\n",
        "np.random.seed(Seeds)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(Seeds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model: GraphSAGE + MLP\")\n",
        "print(f\"Aggregator: {Aggregator.upper()}\")\n",
        "print(f\"Fingerprint: {Fingerprint_type.upper()} ({N_bit}-bit)\")\n",
        "print(f\"Hidden Dim: {Hidden_dimensions}\")\n",
        "print(f\"Dropout: {Dropout}\")\n",
        "print(f\"Learning Rate: {Learning_rate}\")\n",
        "print(f\"Weight Decay: {Decay_weight}\")\n",
        "print(f\"Epochs: {Number_Eproch}\")\n",
        "print(f\"Batch Size: {Batch_size}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "## 5. Load Data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_data_cell",
        "outputId": "8a389982-2bff-47d9-95d7-dd8e3fb29aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train data preview:\n",
            "  Drug1_ID Drug2_ID  Label\n",
            "0  DB01097  DB05219     47\n",
            "1  DB00547  DB00784     49\n",
            "2  DB00623  DB01365     61\n",
            "3  DB00328  DB09027     73\n",
            "4  DB00742  DB00955     57\n",
            "\n",
            "Dataset sizes:\n",
            "  Training: 153,489 pairs\n",
            "  Validation: 19,188 pairs\n",
            "  Test: 19,200 pairs\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/GraphSAGE2_MLP/data/'\n",
        "DATA_PATH_SMILES = '/content/drive/MyDrive/GraphSAGE2_MLP/'\n",
        "\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv(DATA_PATH + 'train_positive.csv')\n",
        "val_df = pd.read_csv(DATA_PATH + 'val_positive.csv')\n",
        "test_df = pd.read_csv(DATA_PATH + 'test_positive.csv')\n",
        "\n",
        "print(f\"\\nTrain data preview:\")\n",
        "print(train_df.head())\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Training: {len(train_df):,} pairs\")\n",
        "print(f\"  Validation: {len(val_df):,} pairs\")\n",
        "print(f\"  Test: {len(test_df):,} pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_smiles"
      },
      "source": [
        "## 6. Load Drug SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_smiles_cell",
        "outputId": "60e4399f-b54a-4479-9965-6b85fe737252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Drug SMILES loaded: 1709 drugs\n",
            "  DrugBank_ID                                             SMILES\n",
            "0     DB00006  CC[C@H](C)[C@H](NC(=O)[C@H](CCC(O)=O)NC(=O)[C@...\n",
            "1     DB00014  CC(C)C[C@H](NC(=O)[C@@H](COC(C)(C)C)NC(=O)[C@H...\n",
            "2     DB00027  CC(C)C[C@@H](NC(=O)CNC(=O)[C@@H](NC=O)C(C)C)C(...\n",
            "3     DB00035  NC(=O)CC[C@@H]1NC(=O)[C@H](CC2=CC=CC=C2)NC(=O)...\n",
            "4     DB00080  CCCCCCCCCC(=O)N[C@@H](CC1=CNC2=C1C=CC=C2)C(=O)...\n"
          ]
        }
      ],
      "source": [
        "# Load drug SMILES\n",
        "drug_smiles_df = pd.read_csv(DATA_PATH_SMILES + 'Drugs_with_Smiles.csv')\n",
        "\n",
        "print(f\"\\nDrug SMILES loaded: {len(drug_smiles_df)} drugs\")\n",
        "print(drug_smiles_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fingerprints"
      },
      "source": [
        "## 7. Extract Molecular Fingerprints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We use Morgan Fingerprint (2,512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fingerprints_cell",
        "outputId": "c6b50809-797e-4c2d-f328-f62667d404d1"
      },
      "outputs": [],
      "source": [
        "# Get unique drugs\n",
        "all_drugs = pd.concat([\n",
        "    train_df['Drug1_ID'], train_df['Drug2_ID'],\n",
        "    val_df['Drug1_ID'], val_df['Drug2_ID'],\n",
        "    test_df['Drug1_ID'], test_df['Drug2_ID']\n",
        "]).unique()\n",
        "\n",
        "print(f\"\\nExtracting {Fingerprint_type} fingerprints...\")\n",
        "print(f\"Total unique drugs: {len(all_drugs)}\")\n",
        "\n",
        "def smiles_to_fingerprint(smiles, fp_type='morgan', n_bits=512):\n",
        "    \"\"\"Convert SMILES to molecular fingerprint\"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "\n",
        "        if fp_type == 'morgan':\n",
        "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits)\n",
        "        elif fp_type == 'maccs':\n",
        "            fp = MACCSkeys.GenMACCSKeys(mol)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown fingerprint type: {fp_type}\")\n",
        "\n",
        "        arr = np.zeros((0,), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "        return arr\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Extract fingerprints\n",
        "drug_to_fp = {}\n",
        "drug_to_idx = {drug: idx for idx, drug in enumerate(all_drugs)}\n",
        "\n",
        "for drug_id in tqdm(all_drugs, desc=\"Extracting fingerprints\"):\n",
        "    smiles_row = drug_smiles_df[drug_smiles_df['DrugBank_ID'] == drug_id]\n",
        "    if len(smiles_row) > 0:\n",
        "        smiles = smiles_row.iloc[0]['SMILES']\n",
        "        fp = smiles_to_fingerprint(smiles, fp_type=Fingerprint_type, n_bits=N_bit)\n",
        "        if fp is not None:\n",
        "            drug_to_fp[drug_id] = fp\n",
        "        else:\n",
        "            drug_to_fp[drug_id] = np.zeros(N_bit, dtype=np.int8)\n",
        "    else:\n",
        "        drug_to_fp[drug_id] = np.zeros(N_bit, dtype=np.int8)\n",
        "\n",
        "# Create feature matrix\n",
        "num_drugs = len(all_drugs)\n",
        "drug_features = np.zeros((num_drugs, N_bit), dtype=np.float32)\n",
        "\n",
        "for drug_id, idx in drug_to_idx.items():\n",
        "    if drug_id in drug_to_fp:\n",
        "        drug_features[idx] = drug_to_fp[drug_id]\n",
        "\n",
        "drug_features = torch.FloatTensor(drug_features).to(device)\n",
        "\n",
        "print(f\"\\nFingerprints extracted!\")\n",
        "print(f\"Feature matrix shape: {drug_features.shape}\")\n",
        "print(f\"Device: {drug_features.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare_data"
      },
      "source": [
        "## 8. Prepare Graph Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prepare_data_cell",
        "outputId": "fd2bbb1e-d9d7-4cdd-c359-eb70d7b77d89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted labels from 1-86 to 0-85\n",
            "\n",
            "Data prepared!\n",
            "Training pairs: 153,489\n",
            "Validation pairs: 19,188\n",
            "Test pairs: 19,200\n",
            "Edge index shape: torch.Size([2, 306978])\n",
            "Number of classes: 86\n"
          ]
        }
      ],
      "source": [
        "def prepare_pairs(df, drug_to_idx):\n",
        "    \"\"\"Convert dataframe to pair indices and labels\"\"\"\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        drug1 = row['Drug1_ID']\n",
        "        drug2 = row['Drug2_ID']\n",
        "        label = row['Label']\n",
        "\n",
        "        if drug1 in drug_to_idx and drug2 in drug_to_idx:\n",
        "            idx1 = drug_to_idx[drug1]\n",
        "            idx2 = drug_to_idx[drug2]\n",
        "            pairs.append([idx1, idx2])\n",
        "            labels.append(label)\n",
        "\n",
        "    return torch.LongTensor(pairs), torch.LongTensor(labels)\n",
        "\n",
        "# Prepare training, validation, and test data\n",
        "train_pairs, train_types = prepare_pairs(train_df, drug_to_idx)\n",
        "val_pairs, val_types = prepare_pairs(val_df, drug_to_idx)\n",
        "test_pairs, test_types = prepare_pairs(test_df, drug_to_idx)\n",
        "# If labels are 1-86, convert to 0-85\n",
        "if train_types.min() == 1:\n",
        "    train_types = train_types - 1\n",
        "    val_types = val_types - 1\n",
        "    test_types = test_types - 1\n",
        "    print(f\"Converted labels from 1-86 to 0-85\")\n",
        "else:\n",
        "    print(\"Labels already 0-based\")\n",
        "# Move to device\n",
        "train_pairs = train_pairs.to(device)\n",
        "train_types = train_types.to(device)\n",
        "val_pairs = val_pairs.to(device)\n",
        "val_types = val_types.to(device)\n",
        "test_pairs = test_pairs.to(device)\n",
        "test_types = test_types.to(device)\n",
        "\n",
        "# Build edge index from training data (for GraphSAGE graph structure)\n",
        "edge_index = torch.cat([train_pairs.T, train_pairs.T[[1, 0]]], dim=1)  # Bidirectional edges\n",
        "\n",
        "print(f\"\\nData prepared!\")\n",
        "print(f\"Training pairs: {len(train_pairs):,}\")\n",
        "print(f\"Validation pairs: {len(val_pairs):,}\")\n",
        "print(f\"Test pairs: {len(test_pairs):,}\")\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "print(f\"Number of classes: {train_types.max().item() + 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## 9. Define GraphSAGE + MLP Model\n",
        "\n",
        "### We use tow layer \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "model_cell",
        "outputId": "c4579c59-0585-440f-ac31-697a5d444fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "MODEL ARCHITECTURE (2-Layer GraphSAGE)\n",
            "================================================================================\n",
            "GraphSAGE_MLP_2Layer(\n",
            "  (sage1): SAGEConv(512, 128, aggr=mean)\n",
            "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (sage2): SAGEConv(128, 128, aggr=mean)\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=128, out_features=86, bias=True)\n",
            "  )\n",
            ")\n",
            "================================================================================\n",
            "\n",
            "Total parameters: 339,926\n",
            "Trainable parameters: 339,926\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class GraphSAGE_MLP_2Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.5, aggregator='mean'):\n",
        "        super(GraphSAGE_MLP_2Layer, self).__init__()\n",
        "\n",
        "        # Two GraphSAGE layers with BatchNorm\n",
        "        self.sage1 = SAGEConv(input_dim, hidden_dim, aggr=aggregator)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        self.sage2 = SAGEConv(hidden_dim, hidden_dim, aggr=aggregator)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),  # 512 â†’ 256\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),       # 256 â†’ 128\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)           # 128 â†’ 86\n",
        "        )\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, drug_pairs):\n",
        "\n",
        "        # First GraphSAGE layer with BatchNorm\n",
        "        x = self.sage1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Second GraphSAGE layer with BatchNorm\n",
        "        x = self.sage2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Get embeddings for drug pairs\n",
        "        h_i = x[drug_pairs[:, 0]]  # First drug in pair\n",
        "        h_j = x[drug_pairs[:, 1]]  # Second drug in pair\n",
        "\n",
        "        # Combine pair features: [concat, hadamard, abs_diff]\n",
        "        h_pair = torch.cat([\n",
        "            h_i,                    # Drug 1 embedding\n",
        "            h_j,                    # Drug 2 embedding\n",
        "            h_i * h_j,              # Element-wise product\n",
        "            torch.abs(h_i - h_j)    # Absolute difference\n",
        "        ], dim=-1)\n",
        "\n",
        "        # MLP classifier\n",
        "        logits = self.mlp(h_pair)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = GraphSAGE_MLP_2Layer(\n",
        "    input_dim=N_bit,          # 512 for Morgan fingerprints\n",
        "    hidden_dim=Hidden_dimensions,     # 128\n",
        "    num_classes=86,\n",
        "    dropout=Dropout,           # 0.5\n",
        "    aggregator=Aggregator      # 'mean'\n",
        ").to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL ARCHITECTURE (2-Layer GraphSAGE)\")\n",
        "print(\"=\"*80)\n",
        "print(model)\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_setup"
      },
      "source": [
        "## 10. Training Setup\n",
        "\n",
        "\n",
        "### We used class weight in both the training and verification phases because the model couldn't train when we only applied class weight during the verification phase. Therefore, we had to apply it in both phases to enable it to learn and train. This contrasts with our model, which was able to train without class weight ðŸ’ªðŸ¥‡."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "training_setup_cell",
        "outputId": "f4474e26-b861-45ba-9038-65fc6eebae6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS WEIGHTS\n",
            "================================================================================\n",
            "Class weights statistics (alpha=0.3):\n",
            "  Min weight: 0.1566\n",
            "  Max weight: 2.6340\n",
            "  Mean weight: 1.0000\n",
            "  Sample counts - Min: 4, Max: 48746\n",
            "================================================================================\n",
            "\n",
            "Training setup complete!\n",
            "Optimizer: Adam (lr=0.01, weight_decay=0.001)\n",
            "Loss: CrossEntropyLoss with class weights\n"
          ]
        }
      ],
      "source": [
        "# Calculate class weights (EXACTLY like our HGNN)\n",
        "type_counts = torch.bincount(train_types, minlength=86).float()\n",
        "alpha = 0.3\n",
        "class_weights = 1.0 / torch.pow(type_counts.clamp(min=1.0), alpha)\n",
        "class_weights = class_weights / class_weights.mean()\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASS WEIGHTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Class weights statistics (alpha={alpha}):\")\n",
        "print(f\"  Min weight: {class_weights.min().item():.4f}\")\n",
        "print(f\"  Max weight: {class_weights.max().item():.4f}\")\n",
        "print(f\"  Mean weight: {class_weights.mean().item():.4f}\")\n",
        "print(f\"  Sample counts - Min: {type_counts.min().int().item()}, Max: {type_counts.max().int().item()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=Learning_rate, weight_decay=Decay_weight)\n",
        "\n",
        "print(f\"\\nTraining setup complete!\")\n",
        "print(f\"Optimizer: Adam (lr={Learning_rate}, weight_decay={Decay_weight})\")\n",
        "print(f\"Loss: CrossEntropyLoss with class weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train_eval"
      },
      "source": [
        "## 11. Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "train_eval_cell",
        "outputId": "234b951b-8ed7-4d11-eee4-54078d59bb4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training functions defined!\n"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, optimizer, criterion, pairs, labels, edge_index, x, batch_size=128):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    perm = torch.randperm(len(pairs))\n",
        "    for i in range(0, len(pairs), batch_size):\n",
        "        batch_idx = perm[i:i+batch_size]\n",
        "        batch_pairs = pairs[batch_idx]\n",
        "        batch_labels = labels[batch_idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x, edge_index, batch_pairs)\n",
        "        loss = criterion(logits, batch_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item() * len(batch_pairs)\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "        total += len(batch_pairs)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, pairs, labels, edge_index, x, criterion, class_weights, batch_size=128):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(0, len(pairs), batch_size):\n",
        "        batch_pairs = pairs[i:i+batch_size]\n",
        "        batch_labels = labels[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x, edge_index, batch_pairs)\n",
        "        loss = criterion(logits, batch_labels)\n",
        "\n",
        "        total_loss += loss.item() * len(batch_pairs)\n",
        "\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(pairs)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'f1_micro': f1_score(all_labels, all_preds, average='micro', zero_division=0),\n",
        "        'f1_macro': f1_score(all_labels, all_preds, average='macro', zero_division=0),\n",
        "        'precision': precision_score(all_labels, all_preds, average='micro', zero_division=0),\n",
        "        'recall': recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    }\n",
        "\n",
        "    return metrics, avg_loss\n",
        "\n",
        "# RAM usage calculator\n",
        "def calculate_ram_usage():\n",
        "    \"\"\"Calculate RAM usage in GB\"\"\"\n",
        "    import psutil\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 / 1024 / 1024\n",
        "\n",
        "print(\"\\nTraining functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train"
      },
      "source": [
        "## 12. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "train_cell",
        "outputId": "35a54889-bffe-4a27-9573-66868b18e8f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FULL TRAINING GraphSAGE+MLP (500 EPOCHS) - GPU ACCELERATED\n",
            "================================================================================\n",
            "RAM usage before training: 1.64 GB\n",
            "Device: cuda\n",
            "Training data: 153,489 pairs\n",
            "\n",
            "New best: Epoch 0 - Val Loss: 1.5338 (Time: 18.0s)\n",
            "Epoch 0: loss: 2.3198, val_loss: 1.5338 (best: 1.5338, patience: 0)\n",
            "New best: Epoch 1 - Val Loss: 1.1726 (Time: 18.0s)\n",
            "New best: Epoch 2 - Val Loss: 1.0323 (Time: 17.9s)\n",
            "New best: Epoch 3 - Val Loss: 0.9874 (Time: 17.7s)\n",
            "New best: Epoch 4 - Val Loss: 0.9021 (Time: 17.8s)\n",
            "New best: Epoch 5 - Val Loss: 0.8715 (Time: 17.9s)\n",
            "New best: Epoch 6 - Val Loss: 0.8523 (Time: 17.8s)\n",
            "New best: Epoch 7 - Val Loss: 0.8281 (Time: 17.9s)\n",
            "New best: Epoch 8 - Val Loss: 0.7978 (Time: 17.8s)\n",
            "New best: Epoch 9 - Val Loss: 0.7869 (Time: 17.8s)\n",
            "Epoch 10: loss: 1.1716, val_loss: 0.7892 (best: 0.7869, patience: 1)\n",
            "New best: Epoch 11 - Val Loss: 0.7464 (Time: 17.8s)\n",
            "New best: Epoch 14 - Val Loss: 0.7394 (Time: 17.8s)\n",
            "New best: Epoch 15 - Val Loss: 0.7314 (Time: 17.8s)\n",
            "New best: Epoch 16 - Val Loss: 0.7279 (Time: 17.8s)\n",
            "New best: Epoch 19 - Val Loss: 0.7276 (Time: 17.8s)\n",
            "New best: Epoch 20 - Val Loss: 0.7037 (Time: 17.9s)\n",
            "Epoch 20: loss: 1.1145, val_loss: 0.7037 (best: 0.7037, patience: 0)\n",
            "New best: Epoch 28 - Val Loss: 0.6989 (Time: 17.8s)\n",
            "New best: Epoch 29 - Val Loss: 0.6897 (Time: 17.8s)\n",
            "Epoch 30: loss: 1.0950, val_loss: 0.7105 (best: 0.6897, patience: 1)\n",
            "New best: Epoch 33 - Val Loss: 0.6845 (Time: 17.8s)\n",
            "New best: Epoch 36 - Val Loss: 0.6802 (Time: 17.8s)\n",
            "Epoch 40: loss: 1.0851, val_loss: 0.6993 (best: 0.6802, patience: 4)\n",
            "New best: Epoch 45 - Val Loss: 0.6750 (Time: 17.8s)\n",
            "New best: Epoch 46 - Val Loss: 0.6747 (Time: 17.8s)\n",
            "Epoch 50: loss: 1.0749, val_loss: 0.6841 (best: 0.6747, patience: 4)\n",
            "New best: Epoch 51 - Val Loss: 0.6654 (Time: 17.8s)\n",
            "Epoch 60: loss: 1.0686, val_loss: 0.7141 (best: 0.6654, patience: 9)\n",
            "New best: Epoch 62 - Val Loss: 0.6642 (Time: 17.8s)\n",
            "Epoch 70: loss: 1.0703, val_loss: 0.6774 (best: 0.6642, patience: 8)\n",
            "New best: Epoch 75 - Val Loss: 0.6638 (Time: 17.9s)\n",
            "Epoch 80: loss: 1.0694, val_loss: 0.6762 (best: 0.6638, patience: 5)\n",
            "New best: Epoch 81 - Val Loss: 0.6615 (Time: 17.9s)\n",
            "New best: Epoch 82 - Val Loss: 0.6612 (Time: 17.8s)\n",
            "New best: Epoch 83 - Val Loss: 0.6502 (Time: 17.9s)\n",
            "Epoch 90: loss: 1.0676, val_loss: 0.6899 (best: 0.6502, patience: 7)\n",
            "Epoch 100: loss: 1.0598, val_loss: 0.6723 (best: 0.6502, patience: 17)\n",
            "Epoch 110: loss: 1.0542, val_loss: 0.6808 (best: 0.6502, patience: 27)\n",
            "Epoch 120: loss: 1.0550, val_loss: 0.6643 (best: 0.6502, patience: 37)\n",
            "Epoch 130: loss: 1.0530, val_loss: 0.6801 (best: 0.6502, patience: 47)\n",
            "New best: Epoch 134 - Val Loss: 0.6486 (Time: 17.9s)\n",
            "Epoch 140: loss: 1.0551, val_loss: 0.6696 (best: 0.6486, patience: 6)\n",
            "Epoch 150: loss: 1.0460, val_loss: 0.6634 (best: 0.6486, patience: 16)\n",
            "New best: Epoch 156 - Val Loss: 0.6432 (Time: 17.9s)\n",
            "Epoch 160: loss: 1.0535, val_loss: 0.6508 (best: 0.6432, patience: 4)\n",
            "Epoch 170: loss: 1.0472, val_loss: 0.6615 (best: 0.6432, patience: 14)\n",
            "Epoch 180: loss: 1.0551, val_loss: 0.6574 (best: 0.6432, patience: 24)\n",
            "New best: Epoch 188 - Val Loss: 0.6396 (Time: 17.8s)\n",
            "Epoch 190: loss: 1.0525, val_loss: 0.6606 (best: 0.6396, patience: 2)\n",
            "Epoch 200: loss: 1.0507, val_loss: 0.6650 (best: 0.6396, patience: 12)\n",
            "New best: Epoch 210 - Val Loss: 0.6393 (Time: 17.8s)\n",
            "Epoch 210: loss: 1.0495, val_loss: 0.6393 (best: 0.6393, patience: 0)\n",
            "Epoch 220: loss: 1.0523, val_loss: 0.6641 (best: 0.6393, patience: 10)\n",
            "New best: Epoch 223 - Val Loss: 0.6385 (Time: 17.8s)\n",
            "Epoch 230: loss: 1.0461, val_loss: 0.6451 (best: 0.6385, patience: 7)\n",
            "Epoch 240: loss: 1.0425, val_loss: 0.6446 (best: 0.6385, patience: 17)\n",
            "Epoch 250: loss: 1.0488, val_loss: 0.6659 (best: 0.6385, patience: 27)\n",
            "Epoch 260: loss: 1.0418, val_loss: 0.6545 (best: 0.6385, patience: 37)\n",
            "Epoch 270: loss: 1.0417, val_loss: 0.6613 (best: 0.6385, patience: 47)\n",
            "Epoch 280: loss: 1.0451, val_loss: 0.6514 (best: 0.6385, patience: 57)\n",
            "New best: Epoch 283 - Val Loss: 0.6302 (Time: 17.8s)\n",
            "Epoch 290: loss: 1.0378, val_loss: 0.6493 (best: 0.6302, patience: 7)\n",
            "New best: Epoch 300 - Val Loss: 0.6281 (Time: 17.8s)\n",
            "Epoch 300: loss: 1.0401, val_loss: 0.6281 (best: 0.6281, patience: 0)\n",
            "Epoch 310: loss: 1.0421, val_loss: 0.6387 (best: 0.6281, patience: 10)\n",
            "Epoch 320: loss: 1.0403, val_loss: 0.6415 (best: 0.6281, patience: 20)\n",
            "Epoch 330: loss: 1.0444, val_loss: 0.6510 (best: 0.6281, patience: 30)\n",
            "Epoch 340: loss: 1.0398, val_loss: 0.6512 (best: 0.6281, patience: 40)\n",
            "Epoch 350: loss: 1.0391, val_loss: 0.6497 (best: 0.6281, patience: 50)\n",
            "Epoch 360: loss: 1.0393, val_loss: 0.6623 (best: 0.6281, patience: 60)\n",
            "New best: Epoch 367 - Val Loss: 0.6271 (Time: 17.9s)\n",
            "Epoch 370: loss: 1.0342, val_loss: 0.6357 (best: 0.6271, patience: 3)\n",
            "Epoch 380: loss: 1.0392, val_loss: 0.6445 (best: 0.6271, patience: 13)\n",
            "Epoch 390: loss: 1.0338, val_loss: 0.6285 (best: 0.6271, patience: 23)\n",
            "Epoch 400: loss: 1.0355, val_loss: 0.6275 (best: 0.6271, patience: 33)\n",
            "Epoch 410: loss: 1.0417, val_loss: 0.6388 (best: 0.6271, patience: 43)\n",
            "Epoch 420: loss: 1.0361, val_loss: 0.6385 (best: 0.6271, patience: 53)\n",
            "Epoch 430: loss: 1.0437, val_loss: 0.6414 (best: 0.6271, patience: 63)\n",
            "New best: Epoch 440 - Val Loss: 0.6245 (Time: 17.9s)\n",
            "Epoch 440: loss: 1.0363, val_loss: 0.6245 (best: 0.6245, patience: 0)\n",
            "Epoch 450: loss: 1.0376, val_loss: 0.6401 (best: 0.6245, patience: 10)\n",
            "Epoch 460: loss: 1.0409, val_loss: 0.6796 (best: 0.6245, patience: 20)\n",
            "Epoch 470: loss: 1.0366, val_loss: 0.6301 (best: 0.6245, patience: 30)\n",
            "Epoch 480: loss: 1.0305, val_loss: 0.6406 (best: 0.6245, patience: 40)\n",
            "Epoch 490: loss: 1.0415, val_loss: 0.6520 (best: 0.6245, patience: 50)\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETED\n",
            "================================================================================\n",
            "Best epoch: 440\n",
            "Best validation loss: 0.6245\n",
            "Total time: 148.52 minutes\n",
            "Average time per epoch: 17.8 seconds\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"FULL TRAINING GraphSAGE+MLP ({Number_Eproch} EPOCHS) - GPU ACCELERATED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get RAM usage before training\n",
        "ram_before = calculate_ram_usage()\n",
        "print(f\"RAM usage before training: {ram_before:.2f} GB\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Training data: {len(train_pairs):,} pairs\\n\")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_epoch = 0\n",
        "patience = 100\n",
        "patience_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(Number_Eproch):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, optimizer, criterion,\n",
        "        train_pairs, train_types,\n",
        "        edge_index, drug_features,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    # Validation phase\n",
        "    val_metrics, val_loss = evaluate(\n",
        "        model, val_pairs, val_types,\n",
        "        edge_index, drug_features,\n",
        "        criterion, class_weights,\n",
        "        batch_size=Batch_size\n",
        "    )\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), '/content/best_graphsage_mlp_model.pth')\n",
        "        print(f\"New best: Epoch {epoch} - Val Loss: {val_loss:.4f} (Time: {epoch_time:.1f}s)\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Print every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: loss: {train_loss:.4f}, val_loss: {val_loss:.4f} (best: {best_val_loss:.4f}, patience: {patience_counter})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Total time: {total_time/60:.2f} minutes\")\n",
        "print(f\"Average time per epoch: {total_time/max(epoch+1, 1):.1f} seconds\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-3Qcj4Uiu2Y"
      },
      "source": [
        "#Save model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHJpYOg6bdIq"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), DATA_PATH + 'best_graphsage_mlp_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test"
      },
      "source": [
        "## 13. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_cell",
        "outputId": "001033a7-46f7-4d0d-9f79-0fd28e4080a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL TEST SET RESULTS (GraphSAGE + MLP)\n",
            "================================================================================\n",
            "Test Loss:  0.6363\n",
            "Accuracy:   0.8090\n",
            "F1 (Micro): 0.8090\n",
            "F1 (Macro): 0.5947\n",
            "Precision:  0.8090\n",
            "Recall:     0.8090\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('/content/best_graphsage_mlp_model.pth'))\n",
        "\n",
        "# Evaluate on test set\n",
        "test_metrics, test_loss = evaluate(\n",
        "    model, test_pairs, test_types,\n",
        "    edge_index, drug_features,\n",
        "    criterion, class_weights,\n",
        "    batch_size=Batch_size\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL TEST SET RESULTS (GraphSAGE + MLP)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Test Loss:  {test_loss:.4f}\")\n",
        "print(f\"Accuracy:   {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"F1 (Micro): {test_metrics['f1_micro']:.4f}\")\n",
        "print(f\"F1 (Macro): {test_metrics['f1_macro']:.4f}\")\n",
        "print(f\"Precision:  {test_metrics['precision']:.4f}\")\n",
        "print(f\"Recall:     {test_metrics['recall']:.4f}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save"
      },
      "source": [
        "## 15. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "save_cell",
        "outputId": "9fd99d67-bebf-41d7-c2c4-a40a78380e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results saved to: /content/drive/MyDrive/GraphSAGE2_MLP/data/result/\n",
            "   - graphsage_mlp_morgan512_model.pth\n",
            "   - graphsage_mlp_results.json\n"
          ]
        }
      ],
      "source": [
        "SAVE_PATH =  DATA_PATH+\"result/\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), SAVE_PATH + 'graphsage_mlp_morgan512_model.pth')\n",
        "\n",
        "# Save results\n",
        "import json\n",
        "results = {\n",
        "    'model': 'GraphSAGE+MLP',\n",
        "    'features': 'Morgan 512',\n",
        "    'aggregator': Aggregator,\n",
        "    'best_epoch': best_epoch,\n",
        "    'best_val_loss': best_val_loss,\n",
        "    'test_metrics': test_metrics,\n",
        "    'test_loss': test_loss,\n",
        "    'training_time_minutes': total_time / 60,\n",
        "    'hyperparameters': {\n",
        "        'hidden_dim': Hidden_dimensions,\n",
        "        'dropout': Dropout,\n",
        "        'learning_rate': Learning_rate,\n",
        "        'weight_decay': Decay_weight,\n",
        "        'batch_size': Batch_size,\n",
        "        'epochs': Number_Eproch\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(SAVE_PATH + 'graphsage_mlp_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nResults saved to: {SAVE_PATH}\")\n",
        "print(\"   - graphsage_mlp_morgan512_model.pth\")\n",
        "print(\"   - graphsage_mlp_results.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "analysis_cell",
        "outputId": "bf3f253a-6ffb-4e81-a108-ecced13e8d94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DETAILED CLASSIFICATION REPORT\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.28      0.70      0.40        27\n",
            "           2       0.98      1.00      0.99        58\n",
            "           3       0.61      0.89      0.72       504\n",
            "           4       0.95      0.62      0.75        32\n",
            "           5       0.71      0.91      0.80       296\n",
            "           6       0.50      1.00      0.67         1\n",
            "           7       0.57      0.84      0.68        19\n",
            "           8       0.77      0.92      0.84       230\n",
            "           9       0.73      1.00      0.84        61\n",
            "          10       0.00      0.00      0.00        27\n",
            "          11       0.51      0.63      0.57        30\n",
            "          12       0.17      0.25      0.20         4\n",
            "          13       0.69      1.00      0.82        29\n",
            "          14       0.33      1.00      0.50        16\n",
            "          15       0.90      0.99      0.94       514\n",
            "          16       1.00      0.67      0.80         6\n",
            "          17       0.26      0.38      0.31        13\n",
            "          18       0.47      0.93      0.62        15\n",
            "          19       0.70      0.90      0.79       596\n",
            "          20       0.73      0.90      0.81        42\n",
            "          21       0.53      1.00      0.70         8\n",
            "          22       0.23      1.00      0.38         3\n",
            "          23       0.80      1.00      0.89        16\n",
            "          24       0.65      0.71      0.68        62\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       0.93      0.97      0.95        99\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.95      0.97      0.96        36\n",
            "          29       0.57      0.93      0.71        60\n",
            "          30       0.00      0.00      0.00         1\n",
            "          31       0.94      0.98      0.96       101\n",
            "          32       0.34      0.31      0.33        51\n",
            "          33       0.61      0.97      0.75        35\n",
            "          34       0.22      0.40      0.29         5\n",
            "          35       0.83      1.00      0.91        10\n",
            "          36       0.88      0.89      0.88       258\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.74      1.00      0.85        14\n",
            "          39       0.97      1.00      0.98        30\n",
            "          40       0.00      0.00      0.00         2\n",
            "          41       0.00      0.00      0.00         1\n",
            "          42       0.50      1.00      0.67         1\n",
            "          43       0.00      0.00      0.00         2\n",
            "          44       1.00      1.00      1.00         2\n",
            "          45       0.19      1.00      0.32         3\n",
            "          46       0.79      0.80      0.80      3405\n",
            "          47       0.88      0.78      0.82         9\n",
            "          48       0.92      0.86      0.89      6099\n",
            "          49       0.00      0.00      0.00         1\n",
            "          50       0.89      0.89      0.89         9\n",
            "          51       0.00      0.00      0.00         1\n",
            "          52       0.60      0.64      0.62        33\n",
            "          53       0.71      0.82      0.76       135\n",
            "          54       0.75      1.00      0.86         9\n",
            "          55       0.00      0.00      0.00         5\n",
            "          56       0.52      0.95      0.67        61\n",
            "          57       0.68      0.99      0.80       106\n",
            "          58       1.00      1.00      1.00         5\n",
            "          59       0.83      0.82      0.83       831\n",
            "          60       0.83      1.00      0.91        55\n",
            "          61       0.00      0.00      0.00         1\n",
            "          62       0.00      0.00      0.00         3\n",
            "          63       0.43      0.74      0.55        76\n",
            "          64       0.50      1.00      0.67         3\n",
            "          65       1.00      0.80      0.89        15\n",
            "          66       0.58      0.92      0.71       122\n",
            "          67       0.74      0.41      0.53        34\n",
            "          68       0.68      1.00      0.81        19\n",
            "          69       0.86      0.77      0.82       808\n",
            "          70       0.48      0.72      0.57        57\n",
            "          71       0.98      0.97      0.97       191\n",
            "          72       0.78      0.66      0.71      2406\n",
            "          73       0.98      1.00      0.99        57\n",
            "          74       0.74      0.53      0.62       961\n",
            "          75       0.72      0.88      0.79        58\n",
            "          76       0.69      0.73      0.71        62\n",
            "          77       0.50      1.00      0.67         2\n",
            "          78       0.00      0.00      0.00         4\n",
            "          79       0.00      0.00      0.00        14\n",
            "          80       1.00      0.11      0.20         9\n",
            "          81       0.69      0.65      0.67        31\n",
            "          82       0.95      0.99      0.97       130\n",
            "          83       0.50      0.75      0.60         4\n",
            "          84       0.74      0.74      0.74        39\n",
            "          85       0.80      1.00      0.89         4\n",
            "\n",
            "    accuracy                           0.81     19200\n",
            "   macro avg       0.56      0.68      0.59     19200\n",
            "weighted avg       0.82      0.81      0.81     19200\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Per-class performance analysis\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get predictions for test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, len(test_pairs), Batch_size):\n",
        "        batch_pairs = test_pairs[i:i+Batch_size]\n",
        "        batch_labels = test_types[i:i+Batch_size]\n",
        "\n",
        "        logits = model(drug_features, edge_index, batch_pairs)\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
